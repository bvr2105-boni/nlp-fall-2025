{
  "evaluation_timestamp": "20251130_202055",
  "total_resumes": 113,
  "total_evaluations": 113,
  "top_jobs_per_resume": 1,
  "evaluation_mode": "top_1_jobs_per_resume",
  "description": "Each resume is matched with its top 1 matching jobs from database (same logic as Resume Matching page). Total evaluations = 1 × number of resumes.",
  "results": [
    {
      "resume_file": "2024-jeffchiarelli-resume.pdf",
      "job_rank": 1,
      "job_title": "Senior Marketing Manager (Mandaluyong)",
      "job_company": "Filinvest Development Corporation",
      "job_id": "4332447038",
      "skill_score": 0.17142857142857143,
      "semantic_score": 0.6534501727282841,
      "topic_score": 0.8423386216163635,
      "final_score": 0.7911125005142111,
      "resume_skills_count": 28,
      "job_skills_count": 13,
      "matching_skills_count": 6,
      "resume_text_length": 6978,
      "resume_skills": [
        "a/b testing",
        "campaign optimization",
        "communication",
        "competitive positioning",
        "conversion rate optimization",
        "data analysis",
        "data analytics",
        "digital marketing",
        "email marketing",
        "google ads",
        "google analytics",
        "google search console",
        "hubspot",
        "lambda",
        "lead generation",
        "leadership",
        "marketing automation",
        "merchandising",
        "outreach",
        "performance management",
        "ppc",
        "process improvement",
        "project management",
        "salesforce",
        "sem",
        "seo",
        "shopify",
        "strategic planning"
      ],
      "job_skills": [
        "collaboration",
        "communication",
        "cross-functional collaboration",
        "design thinking",
        "digital marketing",
        "interpretation",
        "lead generation",
        "leadership",
        "mentoring",
        "merchandising",
        "project management",
        "stakeholder management",
        "visual merchandising"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks direct experience in field activation and on‑site merchandising, core duties for the Senior Marketing Manager role.",
      "llm_recommendations": [
        "Obtain hands‑on experience with trade marketing or experiential events through short‑term projects or volunteering.",
        "Highlight any event‑centric marketing work (e.g., launch events, trade shows) and quantify its impact.",
        "Pursue a certification or training in experiential marketing, POP merchandising, or trade show operations.",
        "Include measurable results from any physical marketing initiatives (e.g., booth traffic, lead capture) in the résumé.",
        "Emphasize transferable project‑management and creative storytelling skills that can be adapted to physical activations."
      ],
      "linkedin_keywords": [
        "Senior Marketing Manager",
        "Field Activation",
        "Trade Marketing",
        "Experiential Marketing",
        "Brand Activation",
        "Lead Generation",
        "Sales Funnel",
        "Cross‑Functional Leadership",
        "Project Management",
        "Digital Campaigns."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Aisha_Kaur_20251129_061632.pdf",
      "job_rank": 1,
      "job_title": "ML Engineer (Outside IR35, £550 per day) with verification",
      "job_company": "Oliver Bernard",
      "job_id": "4318473151",
      "skill_score": 0.2608695652173913,
      "semantic_score": 0.7064733155337999,
      "topic_score": 0.8206532001495361,
      "final_score": 0.8252424079699285,
      "resume_skills_count": 22,
      "job_skills_count": 7,
      "matching_skills_count": 6,
      "resume_text_length": 1878,
      "resume_skills": [
        "a/b testing",
        "airflow",
        "aws",
        "azure",
        "ci/cd",
        "data ingestion",
        "data pipeline",
        "docker",
        "etl",
        "feature engineering",
        "feature engineering pipelines",
        "gcp",
        "git",
        "gitlab",
        "kubernetes",
        "machine learning",
        "microservices",
        "mlflow",
        "python",
        "rest apis",
        "spark",
        "sql"
      ],
      "job_skills": [
        "aws",
        "ci/cd",
        "docker",
        "gcp",
        "kubernetes",
        "machine learning",
        "problem-solving skills"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate possesses core skills in end‑to‑end ML deployment, REST API development, cloud platforms (AWS, GCP), and CI/CD/Kubernetes/DevOps practices, matching the essential requirements for the role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Aisha_Patel_20251129_055725.pdf",
      "job_rank": 1,
      "job_title": "Data Integration Engineer with verification",
      "job_company": "Boot Barn",
      "job_id": "4319462092",
      "skill_score": 0.17647058823529413,
      "semantic_score": 0.6430913805961609,
      "topic_score": 0.6117086410522461,
      "final_score": 0.6931529500905205,
      "resume_skills_count": 19,
      "job_skills_count": 21,
      "matching_skills_count": 6,
      "resume_text_length": 1912,
      "resume_skills": [
        "a/b testing",
        "agile",
        "azure",
        "ci/cd",
        "data analysis",
        "decision making",
        "flask",
        "git",
        "github",
        "github actions",
        "machine learning",
        "power bi",
        "python",
        "r",
        "scrum",
        "snowflake",
        "sql",
        "sql server",
        "tableau"
      ],
      "job_skills": [
        "azure",
        "azure data factory",
        "ci/cd",
        "communication",
        "data integration",
        "elt",
        "etl",
        "git",
        "mongodb",
        "problem solving",
        "python",
        "reporting",
        "scala",
        "spark",
        "sql",
        "sql server",
        "stakeholder management",
        "teamwork",
        "time management",
        "user stories",
        "validation rules"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The profile focuses on data science and ML model deployment rather than the core ETL/SSIS, Spark, Azure Data Factory, and Data Vault development responsibilities required for this Data Integration Engineer role.",
      "llm_recommendations": [
        "Acquire hands‑on experience with SSIS and Azure Data Factory through tutorials or a course project.",
        "Build a small data integration pipeline using Spark or Azure Databricks to demonstrate proficiency in batch/streaming workflows.",
        "Gain familiarity with Data Vault 2.0 concepts and create sample models (hub, link, satellite) to showcase modeling skill.",
        "Learn PowerShell or .NET scripting for job orchestration and demonstrate a simple automation script.",
        "Highlight any existing SQL Server or Snowflake ETL work and explicitly mention readiness to transition to SSIS/Spark environments."
      ],
      "linkedin_keywords": [
        "Data Integration Engineer",
        "ETL Developer",
        "SSIS Developer",
        "Spark Developer",
        "Azure Data Factory",
        "Data Vault 2.0",
        "CDC",
        "Azure DevOps",
        "PowerShell",
        "SQL Server",
        "Snowflake",
        "MongoDB",
        "Cloud Data Integration",
        "Python ETL."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Aisha_Thompson_20251129_062821.pdf",
      "job_rank": 1,
      "job_title": "Data Scientist, Watchlist",
      "job_company": "RemoteHunter",
      "job_id": "4319177929",
      "skill_score": 0.3333333333333333,
      "semantic_score": 0.596553159376415,
      "topic_score": 0.9529943466186523,
      "final_score": 0.8498491686650224,
      "resume_skills_count": 17,
      "job_skills_count": 11,
      "matching_skills_count": 7,
      "resume_text_length": 1677,
      "resume_skills": [
        "airflow",
        "aws",
        "azure",
        "azure data factory",
        "data visualization",
        "etl",
        "hadoop",
        "leadership",
        "machine learning",
        "pandas",
        "python",
        "r",
        "sagemaker",
        "spark",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "adaptability",
        "aws",
        "databricks",
        "feature engineering",
        "hadoop",
        "machine learning",
        "python",
        "r",
        "spark",
        "sql",
        "xgboost"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate demonstrates the core technical stack, data‑handling expertise, and business‑oriented model deployment experience required for the role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Alexandra_Reyes_20251129_065105.pdf",
      "job_rank": 1,
      "job_title": "DevSecOps",
      "job_company": "QualiZeal",
      "job_id": "4332406515",
      "skill_score": 0.34285714285714286,
      "semantic_score": 0.7498737573623657,
      "topic_score": 0.6414638161659241,
      "final_score": 0.8000109170164381,
      "resume_skills_count": 29,
      "job_skills_count": 18,
      "matching_skills_count": 12,
      "resume_text_length": 3046,
      "resume_skills": [
        "a/b testing",
        "airflow",
        "ansible",
        "aws",
        "bash",
        "ci/cd",
        "data ingestion",
        "docker",
        "gcp",
        "git",
        "github",
        "github actions",
        "gradient boosting",
        "jenkins",
        "kubernetes",
        "machine learning",
        "mlflow",
        "neural networks",
        "pandas",
        "power bi",
        "python",
        "random forest",
        "s3",
        "sagemaker",
        "software engineering",
        "spark",
        "sql",
        "tableau",
        "terraform"
      ],
      "job_skills": [
        "ansible",
        "aws",
        "azure",
        "bash",
        "bitbucket",
        "ci/cd",
        "cypress",
        "docker",
        "git",
        "github",
        "github actions",
        "gitlab",
        "jenkins",
        "junit",
        "kubernetes",
        "pytest",
        "python",
        "terraform"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate possesses the core DevOps competencies—Jenkins, Actions, Docker, Kubernetes, Terraform, AWS, scripting, and version control—along with proven experience in CI/CD pipeline design, container orchestration, infrastructure automation, and monitoring, enabling them to effectively fulfill the key responsibilities of a DevSecOps role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Alkhatib-Khaled-Resume.pdf",
      "job_rank": 1,
      "job_title": "Technical Lead - Java",
      "job_company": "Soho Square Solutions",
      "job_id": "4318099038",
      "skill_score": 0.20833333333333334,
      "semantic_score": 0.7024603244709632,
      "topic_score": 0.44877317547798157,
      "final_score": 0.664029927063124,
      "resume_skills_count": 15,
      "job_skills_count": 14,
      "matching_skills_count": 5,
      "resume_text_length": 3976,
      "resume_skills": [
        "angular",
        "aws",
        "bitbucket",
        "git",
        "github",
        "go",
        "javascript",
        "node.js",
        "postgresql",
        "react",
        "shopify",
        "siem",
        "splunk",
        "sql",
        "translation"
      ],
      "job_skills": [
        "angular",
        "aws",
        "ci/cd",
        "cloud platforms",
        "collaboration",
        "docker",
        "git",
        "java",
        "javascript",
        "jenkins",
        "mongodb",
        "sql",
        "sql server",
        "typescript"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate does not meet the critical experience thresholds for Java/Spring, advanced Angular, 8+ years of progressive development, leadership, and CI/CD pipeline expertise required for the Technical Lead role.",
      "llm_recommendations": [
        "Build a portfolio of Java/Spring Boot projects, including full‑stack solutions that demonstrate experience with orchestration layers and database integration.",
        "Gain at least 2–3 years of hands‑on Angular development, focusing on architecture, component design, and unit/integration testing.",
        "Earn advanced AWS certifications (e.g., AWS Solutions Architect – Associate or Professional) and acquire hands‑on experience with services like EKS, ECS, and CI/CD tools such as Jenkins or GitHub Actions.",
        "Seek opportunities to lead small teams or mentor junior developers to build leadership and project management skills.",
        "Develop test automation skills (JUnit, Selenium, Cucumber) and document test strategies for a small open‑source or internal project."
      ],
      "linkedin_keywords": [
        "Java",
        "Spring Boot",
        "Angular",
        "Full-Stack Developer",
        "Cloud Architect",
        "AWS",
        "DevOps",
        "CI/CD",
        "Build Automation",
        "Database Engineer",
        "Security Engineer."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Amber-Gaston-Resume-4.25.pdf",
      "job_rank": 1,
      "job_title": "Developer (Bubble Certified)",
      "job_company": "Tulong Tech",
      "job_id": "4319269984",
      "skill_score": 0.0,
      "semantic_score": 0.5687281915924515,
      "topic_score": 0.3993537724018097,
      "final_score": 0.4840409819971306,
      "resume_skills_count": 24,
      "job_skills_count": 3,
      "matching_skills_count": 0,
      "resume_text_length": 5675,
      "resume_skills": [
        "collaboration",
        "customer retention",
        "digital marketing",
        "email marketing",
        "excel",
        "figma",
        "google ads",
        "google analytics",
        "hubspot",
        "jira",
        "klaviyo",
        "magento",
        "mailchimp",
        "merchandising",
        "microsoft excel",
        "procurement",
        "project management",
        "salesforce",
        "seo",
        "shopify",
        "tableau",
        "trend analysis",
        "visual merchandising",
        "woocommerce"
      ],
      "job_skills": [
        "communication",
        "software development",
        "software engineering"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks the essential technical experience (Bubble platform, back‑end web development, and programming skills) required for the role.",
      "llm_recommendations": [
        "Complete a Bubble development course and build sample apps to demonstrate proficiency",
        "Gain foundation in JavaScript, HTML/CSS, and basic back‑end concepts (e.g., APIs, databases) through online tutorials or bootcamps",
        "Highlight any prior experience with web platforms (Shopify, WordPress) by showing custom code or integrations to illustrate coding competence",
        "Obtain a relevant certification or portfolio piece that showcases problem‑solving and user‑friendly design in web applications",
        "Emphasize remote work experience and communication skills in future applications to align with the role’s expectations"
      ],
      "linkedin_keywords": [
        "Bubble Developer",
        "Web Development",
        "JavaScript",
        "HTML",
        "CSS",
        "Back‑End Development",
        "API Integration",
        "Remote Web Developer",
        "Front‑End Development",
        "Programming Skills"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Amina_Diop_20251129_140929.pdf",
      "job_rank": 1,
      "job_title": "Sr. Analyst, Digital Initiatives",
      "job_company": "Toast",
      "job_id": "4331625169",
      "skill_score": 0.1875,
      "semantic_score": 0.6127252578735395,
      "topic_score": 0.8349452018737793,
      "final_score": 0.7756161242723483,
      "resume_skills_count": 24,
      "job_skills_count": 14,
      "matching_skills_count": 6,
      "resume_text_length": 2209,
      "resume_skills": [
        "agile",
        "airflow",
        "business intelligence",
        "data analytics",
        "data ingestion",
        "data pipeline",
        "data transformation",
        "etl",
        "git",
        "logistics",
        "looker",
        "machine learning",
        "numpy",
        "pandas",
        "python",
        "redshift",
        "reporting",
        "scrum",
        "snowflake",
        "spark",
        "sql",
        "tableau",
        "unit testing",
        "validation rules"
      ],
      "job_skills": [
        "business intelligence",
        "communication",
        "data analysis",
        "data analytics",
        "data visualization",
        "excel",
        "power bi",
        "presentation skills",
        "python",
        "r",
        "reporting",
        "sql",
        "stakeholder management",
        "tableau"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks the core 7+ years of experience and direct exposure to POS/transitional data analysis and digital initiative projects required for this senior role.",
      "llm_recommendations": [
        "Gain additional experience (e.g., internships, freelance projects, or short-term consulting) focusing on POS, e‑commerce or retail transaction data analytics.",
        "Highlight any relevant projects or coursework in the resume that involve transactional data, sales analytics, or digital channel optimization.",
        "Pursue certifications in advanced SQL, Power BI, or Tableau to demonstrate mastery of the required BI tools.",
        "Seek opportunities to collaborate with or shadow teams that manage kiosk, loyalty, or delivery channel initiatives.",
        "Emphasize transferable skills (data modeling, ETL, dashboarding, stakeholder communication) as evidence of readiness to learn and adapt quickly."
      ],
      "linkedin_keywords": [
        "Data Analyst",
        "Business Intelligence",
        "SQL",
        "Tableau",
        "Power BI",
        "Python",
        "Data Visualization",
        "ETL",
        "Data Engineer",
        "Analytics Engineer"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Amir_Boroumand.pdf",
      "job_rank": 1,
      "job_title": "Sr. Full Stack Developer with verification",
      "job_company": "Motion Recruitment",
      "job_id": "4331373865",
      "skill_score": 0.38235294117647056,
      "semantic_score": 0.7035752534866333,
      "topic_score": 0.8263441920280457,
      "final_score": 0.8548280640560038,
      "resume_skills_count": 22,
      "job_skills_count": 25,
      "matching_skills_count": 13,
      "resume_text_length": 8007,
      "resume_skills": [
        "agile",
        "aws",
        "c",
        "ci/cd",
        "docker",
        "github",
        "java",
        "javascript",
        "jenkins",
        "junit",
        "kubernetes",
        "oracle",
        "postgresql",
        "python",
        "react",
        "s3",
        "sap",
        "scrum",
        "splunk",
        "sql",
        "terraform",
        "typescript"
      ],
      "job_skills": [
        "agile",
        "aws",
        "c",
        "communication",
        "cypress",
        "db2",
        "docker",
        "ec2",
        "git",
        "github",
        "gitlab",
        "java",
        "javascript",
        "kubernetes",
        "microservices",
        "mongodb",
        "nosql",
        "oracle",
        "python",
        "s3",
        "scrum",
        "selenium",
        "software development",
        "sql",
        "usability testing"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate has the core qualifications (8+ years Java/J2EE, Spring Boot microservices, AWS services, Docker/Kubernetes, CI/CD with Git, Terraform for IaC, and solid test coverage experience) that align with the primary responsibilities of the Sr. Full‑Stack Developer role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Amira_Khatri_20251129_061557.pdf",
      "job_rank": 1,
      "job_title": "Desenvolvedor Front End Vue.js | UI",
      "job_company": "innolevels",
      "job_id": "4317267515",
      "skill_score": 0.15789473684210525,
      "semantic_score": 0.29290567594204453,
      "topic_score": 0.04704374447464943,
      "final_score": 0.30103133491229217,
      "resume_skills_count": 16,
      "job_skills_count": 6,
      "matching_skills_count": 3,
      "resume_text_length": 1787,
      "resume_skills": [
        "a/b testing",
        "aws",
        "azure",
        "bert",
        "collaboration",
        "data visualization",
        "docker",
        "fastapi",
        "feature engineering",
        "git",
        "kubernetes",
        "named entity recognition",
        "nlp",
        "python",
        "sentiment analysis",
        "text classification"
      ],
      "job_skills": [
        "angular",
        "aws",
        "git",
        "gitlab",
        "kubernetes",
        "vue.js"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate’s expertise is focused on NLP and backend data pipelines with no experience or proof of front‑end technologies such as Vue.js, JavaScript, or UI component frameworks required for this role.",
      "llm_recommendations": [
        "Acquire hands‑on Vue.js experience through small projects or online courses, documenting the work in a portfolio.",
        "Build a basic front‑end demo that uses Vuetify (or BootstrapVue) to showcase UI skills and GitHub for version control.",
        "Strengthen knowledge of JavaScript, HTML, and CSS fundamentals, and explore related frameworks like React or Angular to demonstrate versatility.",
        "Highlight any existing web‑deployment experience (Docker, Kubernetes, AWS) and relate it to front‑end hosting and CI/CD pipelines.",
        "Obtain a short certification or badge in front‑end development (e.g., freeCodeCamp or Udemy “The Complete Vue.js Course”)."
      ],
      "linkedin_keywords": [
        "Vue.js",
        "JavaScript",
        "Front End Development",
        "UI/UX Design",
        "HTML",
        "CSS",
        "Vuetify",
        "React",
        "Angular",
        "Node.js",
        "AWS",
        "Kubernetes",
        "Git",
        "Cloud Deployment",
        "Linux",
        "Full Stack",
        "Software Engineering"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Amira_Tan_20251129_060511.pdf",
      "job_rank": 1,
      "job_title": "AI/ML Software Engineer with verification",
      "job_company": "Guidehouse",
      "job_id": "4332300185",
      "skill_score": 0.18518518518518517,
      "semantic_score": 0.7573051678331331,
      "topic_score": 0.6038357615470886,
      "final_score": 0.7397240823400903,
      "resume_skills_count": 15,
      "job_skills_count": 17,
      "matching_skills_count": 5,
      "resume_text_length": 2706,
      "resume_skills": [
        "aws",
        "ci/cd",
        "data analysis",
        "data pipeline",
        "decision making",
        "docker",
        "feature engineering",
        "feature engineering pipelines",
        "gcp",
        "kubernetes",
        "machine learning",
        "python",
        "reporting",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "aws",
        "communication",
        "data pipeline",
        "deep learning",
        "javascript",
        "machine learning",
        "numpy",
        "oracle",
        "outreach",
        "pandas",
        "postgresql",
        "python",
        "rest apis",
        "software development",
        "software engineering",
        "sql",
        "typescript"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The résumé lacks the critical core requirements for this role—specifically, substantial experience with JavaScript/TypeScript (5+ years) and proven work building Retrieval‑Augmented Generation or chatbot/agentic AI applications, which are central to the position.",
      "llm_recommendations": [
        "Acquire hands‑on experience with JavaScript/TypeScript, focusing on building and consuming REST APIs and integrating with AWS services.",
        "Build a portfolio project involving a simple RAG system or chatbot (e.g., using LangChain or Azure OpenAI) to demonstrate relevant AI application skills.",
        "Pursue a federal “Public Trust” security clearance (or highlight any existing clearance) to meet the government client requirement.",
        "Highlight any cloud‑based MLOps or pipeline work that involved automated data ingestion, model serving, and CI/CD to show transferable dev‑ops expertise.",
        "Emphasize rapid learning and cross‑functional collaboration achievements to show capacity to acquire missing tech quickly."
      ],
      "linkedin_keywords": [
        "Python",
        "JavaScript",
        "TypeScript",
        "AWS",
        "RAG",
        "Retrieval Augmented Generation",
        "LangChain",
        "LangChain",
        "chatbot",
        "conversational AI",
        "MLOps",
        "cloud deployment",
        "data pipeline",
        "REST API",
        "security clearance."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Amit_Sharma.pdf",
      "job_rank": 1,
      "job_title": "Marketing Coordinat with verification",
      "job_company": "SERHANT.",
      "job_id": "4323900358",
      "skill_score": 0.043478260869565216,
      "semantic_score": 0.547554958449612,
      "topic_score": 0.7288231253623962,
      "final_score": 0.6539199531274822,
      "resume_skills_count": 20,
      "job_skills_count": 4,
      "matching_skills_count": 1,
      "resume_text_length": 12965,
      "resume_skills": [
        "account management",
        "business intelligence",
        "communication",
        "competitive analysis",
        "data transformation",
        "db2",
        "digital marketing",
        "mailchimp",
        "market research",
        "process optimization",
        "procurement",
        "reporting",
        "salesforce",
        "sap",
        "seo",
        "software development",
        "sql",
        "strategic planning",
        "vendor management",
        "zoho crm"
      ],
      "job_skills": [
        "airtable",
        "communication",
        "content creation",
        "project management"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate possesses a marketing background with substantial project‑management experience, strong communication and organizational skills, and a relevant MBA, all of which cover the essential duties of a Marketing Coordinator, and can readily adapt to the specific tools used by the team.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Ana_Martinez_20251129_064703.pdf",
      "job_rank": 1,
      "job_title": "Data Analyst",
      "job_company": "Wyndham Hotels & Resorts, Inc.",
      "job_id": "981851940397954",
      "skill_score": 0.3333333333333333,
      "semantic_score": 0.7035224051333193,
      "topic_score": 0.9552298784255981,
      "final_score": 0.8862507611863059,
      "resume_skills_count": 12,
      "job_skills_count": 8,
      "matching_skills_count": 5,
      "resume_text_length": 1766,
      "resume_skills": [
        "a/b testing",
        "collaboration",
        "data visualization",
        "etl",
        "excel",
        "inventory management",
        "power bi",
        "python",
        "r",
        "reporting",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "business intelligence",
        "data analysis",
        "data visualization",
        "power bi",
        "problem-solving skills",
        "reporting",
        "sql",
        "tableau"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "Ana Martinez possesses the essential skills in Tableau, Power BI, SQL, and data visualization, along with extensive analytical experience and a proven track record of delivering business‑intelligence insights, enabling her to perform the core functions of a Data Analyst.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Ari_Patel_20251129_063351.pdf",
      "job_rank": 1,
      "job_title": "Data Engineer AWS - Hybrid in NY",
      "job_company": "TechTriad",
      "job_id": "4317962527",
      "skill_score": 0.18518518518518517,
      "semantic_score": 0.7512963562431243,
      "topic_score": 0.8874087929725647,
      "final_score": 0.8528058015323178,
      "resume_skills_count": 15,
      "job_skills_count": 17,
      "matching_skills_count": 5,
      "resume_text_length": 1827,
      "resume_skills": [
        "airflow",
        "aws",
        "data analysis",
        "data cleaning",
        "data ingestion",
        "data pipeline",
        "etl",
        "mentoring",
        "numpy",
        "pandas",
        "python",
        "sagemaker",
        "snowflake",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "airflow",
        "aws",
        "ci/cd",
        "data pipeline",
        "databricks",
        "distributed systems",
        "feature store",
        "java",
        "kubernetes",
        "lambda",
        "machine learning",
        "mlflow",
        "pyspark",
        "python",
        "s3",
        "scala",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks several core requirements—especially experience with Databricks (Spark, Delta Lake), large‑scale AWS services (Glue, EMR, S3, Lambda), and Feature Store architectures—while falling short of the 7‑10+ year seniority level required.",
      "llm_recommendations": [
        "Gain hands‑on experience with Databricks and Spark by participating in open‑source projects or training courses (e.g., Databricks Academy).",
        "Build end‑to‑end data pipelines on AWS, covering S3, Glue, EMR, and Lambda, and showcase them in a portfolio or GitHub repo.",
        "Develop and document a Feature Store use‑case using Delta Lake or Feast to demonstrate familiarity with ML pipeline infrastructure.",
        "Highlight any CI/CD implementation (e.g., Airflow DAG CI, GitHub Actions) and pursue certification in AWS Data Analytics or related fields.",
        "Emphasize transferable ETL and data quality skills, and articulate a clear learning plan for cloud‑native tooling."
      ],
      "linkedin_keywords": [
        "Data Engineer",
        "AWS Data Engineer",
        "Databricks Engineer",
        "Spark Developer",
        "Feature Store",
        "ML Pipeline",
        "CI/CD",
        "Airflow",
        "Glue",
        "S3",
        "EMR",
        "Python",
        "Snowflake",
        "AWS Analytics",
        "BigData Engineer",
        "ETL Engineer"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Ariana_Patel_20251129_064307.pdf",
      "job_rank": 1,
      "job_title": "Product Designer, ChatGPT",
      "job_company": "ExecutivePlacements.com",
      "job_id": "4332440377",
      "skill_score": 0.0,
      "semantic_score": 0.5240644245224514,
      "topic_score": 0.3167407810688019,
      "final_score": 0.42040260279562663,
      "resume_skills_count": 16,
      "job_skills_count": 3,
      "matching_skills_count": 0,
      "resume_text_length": 2806,
      "resume_skills": [
        "bert",
        "data pipeline",
        "deep learning",
        "docker",
        "flask",
        "git",
        "kubernetes",
        "leadership",
        "machine learning",
        "microservices",
        "natural language processing",
        "nlp",
        "python",
        "sentiment analysis",
        "spark",
        "sql"
      ],
      "job_skills": [
        "communication",
        "product management",
        "user research"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume demonstrates deep expertise in NLP engineering, but it lacks direct experience or evidence of UI/UX, interaction design, or a portfolio that would qualify Ariana for a Product Designer role focused on design systems, user research, and visual design.",
      "llm_recommendations": [
        "Complete a design bootcamp or take courses in UX/UI and interaction design to build foundational skills.",
        "Build a portfolio of design projects—ideally redesigning or prototyping product features—to showcase ability to think through complex interaction problems.",
        "Gain hands‑on experience with design tools such as Figma, Sketch, or Adobe XD and document learning outcomes.",
        "Seek mentorship from a senior designer or participate in cross‑disciplinary design sprints to demonstrate collaboration with product and engineering teams.",
        "Highlight transferable storytelling, communication, and project leadership experience when applying for design roles."
      ],
      "linkedin_keywords": [
        "Product Design",
        "UX Design",
        "UI Design",
        "Interaction Design",
        "Design Systems",
        "Figma",
        "Sketch",
        "Human‑Computer Interaction",
        "Design Thinking",
        "User Research",
        "Agile Product Management",
        "Prototyping",
        "Visual Design",
        "Design Process",
        "Design Lead",
        "Interface Design",
        "Experience Design",
        "Customer Experience",
        "HCI",
        "Design Strategy."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Arianna_K_Patel_20251129_064943.pdf",
      "job_rank": 1,
      "job_title": "Data and Production Support Analyst with verification",
      "job_company": "VanEck",
      "job_id": "4310751775",
      "skill_score": 0.25,
      "semantic_score": 0.6207086813872278,
      "topic_score": 0.8354846835136414,
      "final_score": 0.7960725118378259,
      "resume_skills_count": 16,
      "job_skills_count": 14,
      "matching_skills_count": 6,
      "resume_text_length": 2392,
      "resume_skills": [
        "budgeting",
        "c",
        "data analysis",
        "data cleaning",
        "data pipeline",
        "data visualization",
        "leadership",
        "machine learning",
        "operational efficiency",
        "power bi",
        "python",
        "r",
        "reporting",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "business analysis",
        "communication",
        "critical thinking",
        "data analysis",
        "data pipeline",
        "data visualization",
        "multitasking",
        "operational efficiency",
        "problem solving",
        "reporting",
        "sql",
        "teamwork",
        "time management",
        "translation"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks the essential experience working in data operations within a financial firm and specific knowledge of investment data sets, which is a core requirement for this role.",
      "llm_recommendations": [
        "Seek opportunities or side projects that involve finance‑related data sets (e.g., market data, corporate actions, holdings).",
        "Pursue short courses or certifications in financial data management or investment analytics to demonstrate domain knowledge.",
        "Highlight any exposure to regulated or compliance‑heavy data environments to showcase transferable skills in data integrity and governance.",
        "Network with professionals in finance data teams to gain insight into domain terminology and workflows.",
        "Include examples of troubleshooting large data pipelines and validating data quality to align with the role’s operational focus."
      ],
      "linkedin_keywords": [
        "data operations",
        "financial data analysis",
        "investment data",
        "SQL",
        "data pipeline",
        "data quality assurance",
        "finance data management",
        "regulatory data",
        "portfolio analytics",
        "data ingestion."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Arielle_Martinez_20251129_063312.pdf",
      "job_rank": 1,
      "job_title": "Data Analyst",
      "job_company": "Sports Research",
      "job_id": "4317975045",
      "skill_score": 0.2,
      "semantic_score": 0.6570312389405527,
      "topic_score": 0.864525318145752,
      "final_score": 0.8086226228345218,
      "resume_skills_count": 20,
      "job_skills_count": 10,
      "matching_skills_count": 5,
      "resume_text_length": 2308,
      "resume_skills": [
        "a/b testing",
        "airflow",
        "data analysis",
        "data cleaning",
        "data pipeline",
        "data visualization",
        "etl",
        "forecasting",
        "healthcare analytics",
        "leadership",
        "machine learning",
        "numpy",
        "pandas",
        "power bi",
        "python",
        "r",
        "reporting",
        "snowflake",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "attention to detail",
        "business intelligence",
        "communication",
        "data analysis",
        "data visualization",
        "leadership",
        "problem-solving skills",
        "process improvement",
        "reporting",
        "sql"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate possesses the essential data analysis expertise, strong SQL and BI tool proficiency (Tableau, Power BI), and significant experience building dashboards and KPI reports, meeting all core requirements for the role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Arjun_Patel_20251129_063128.pdf",
      "job_rank": 1,
      "job_title": "Senior Engineer Data Science and Engineering with verification",
      "job_company": "TBO.COM",
      "job_id": "4319336877",
      "skill_score": 0.1,
      "semantic_score": 0.44574460063842625,
      "topic_score": 0.631335437297821,
      "final_score": 0.5846860170713113,
      "resume_skills_count": 16,
      "job_skills_count": 17,
      "matching_skills_count": 3,
      "resume_text_length": 2083,
      "resume_skills": [
        "a/b testing",
        "agile",
        "data analytics",
        "deep learning",
        "feature engineering",
        "feature engineering pipelines",
        "hadoop",
        "machine learning",
        "process improvement",
        "pyspark",
        "python",
        "r",
        "spark",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "athena",
        "aws",
        "aws athena",
        "aws glue",
        "c",
        "ci/cd",
        "data ingestion",
        "data pipeline",
        "etl",
        "git",
        "lambda",
        "leadership",
        "pyspark",
        "s3",
        "scala",
        "spark",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks the essential AWS and data‑engineering focus (e.g., Glue, Athena, Hudi, S3, EMR) required for building and maintaining the platform’s lake and pipelines, despite some Spark experience.",
      "llm_recommendations": [
        "Gain hands‑on experience with AWS Glue and related services through practical projects or a certification (e.g., AWS Certified Data Analytics – Specialty).",
        "Build a portfolio of end‑to‑end data pipelines using PySpark on EMR or Redshift, showcasing ETL patterns and data modeling.",
        "Learn Apache Hudi and time‑travel capabilities to align with the company’s incremental data processing needs.",
        "Strengthen knowledge of data lake architectures (S3, Lake Formation) and querying with Athena, demonstrating ability to expose data for analytics.",
        "Highlight transferable skills (Spark, SQL, Hadoop) in the resume and during interviews to show readiness to transition into data‑engineering responsibilities."
      ],
      "linkedin_keywords": [
        "AWS Glue",
        "Apache Hudi",
        "Athena",
        "EMR",
        "Data Lake",
        "PySpark",
        "SQL",
        "ETL",
        "Data Modeling",
        "Cloud Data Engineering"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Arjun_Raghavan_20251129_063831.pdf",
      "job_rank": 1,
      "job_title": "Senior DevOps Engineer with verification",
      "job_company": "Munich Re",
      "job_id": "4311393243",
      "skill_score": 0.2972972972972973,
      "semantic_score": 0.7599889492982013,
      "topic_score": 0.6909298896789551,
      "final_score": 0.8070795920730549,
      "resume_skills_count": 29,
      "job_skills_count": 19,
      "matching_skills_count": 11,
      "resume_text_length": 2750,
      "resume_skills": [
        "airflow",
        "aws",
        "azure",
        "bash",
        "ci/cd",
        "data analysis",
        "data analytics",
        "data pipeline",
        "docker",
        "feature engineering",
        "feature engineering pipelines",
        "flask",
        "forecasting",
        "gcp",
        "github",
        "github actions",
        "gitlab",
        "jenkins",
        "kubeflow",
        "kubernetes",
        "machine learning",
        "python",
        "s3",
        "sagemaker",
        "sales forecasting",
        "sql",
        "terraform",
        "vertex ai",
        "xgboost"
      ],
      "job_skills": [
        "aws",
        "azure",
        "ci/cd",
        "cloud infrastructure",
        "cloud native",
        "collaboration",
        "communication",
        "docker",
        "gcp",
        "github",
        "github actions",
        "incident response",
        "javascript",
        "kubernetes",
        "machine learning",
        "python",
        "software engineering",
        "terraform",
        "typescript"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate possesses the core DevOps skills required—Infrastructure‑as‑Code (Terraform), container orchestration (Kubernetes), cloud platforms (AWS/GCP/Azure), CI/CD pipelines, monitoring tools (Prometheus, Grafana, ELK), and a strong Python background—along with 12 years of relevant engineering experience, enabling them to perform the senior DevOps engineer duties effectively.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Arun_Venkatesh_20251129_060257.pdf",
      "job_rank": 1,
      "job_title": "Data Analytics Software Engineer (Trading)",
      "job_company": "Fintal Partners",
      "job_id": "4318639448",
      "skill_score": 0.16666666666666666,
      "semantic_score": 0.6750433862312761,
      "topic_score": 0.8123286366462708,
      "final_score": 0.7864050095323112,
      "resume_skills_count": 28,
      "job_skills_count": 14,
      "matching_skills_count": 6,
      "resume_text_length": 2552,
      "resume_skills": [
        "agile",
        "airflow",
        "aws",
        "bigquery",
        "ci/cd",
        "cloud platforms",
        "data ingestion",
        "data pipeline",
        "data visualization",
        "decision making",
        "docker",
        "etl",
        "gcp",
        "gitlab",
        "kubernetes",
        "looker",
        "power bi",
        "python",
        "redshift",
        "reporting",
        "s3",
        "scala",
        "scrum",
        "spark",
        "sql",
        "tableau",
        "terraform",
        "trend analysis"
      ],
      "job_skills": [
        "data analysis",
        "data analytics",
        "data pipeline",
        "databricks",
        "docker",
        "kubernetes",
        "machine learning",
        "numpy",
        "pandas",
        "python",
        "reporting",
        "scipy",
        "snowflake",
        "spark"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate possesses the core data‑engineering experience (Python, Spark, Kafka, real‑time pipelines, Docker/Kubernetes) and over a decade of relevant work that enables them to handle the high‑velocity, low‑latency data tasks required for the position.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Asha_Patel_20251129_063921.pdf",
      "job_rank": 1,
      "job_title": "Data Engineer AWS - Hybrid in NY",
      "job_company": "TechTriad",
      "job_id": "4317962527",
      "skill_score": 0.24242424242424243,
      "semantic_score": 0.7373004868864486,
      "topic_score": 0.8913971781730652,
      "final_score": 0.8593551761589067,
      "resume_skills_count": 24,
      "job_skills_count": 17,
      "matching_skills_count": 8,
      "resume_text_length": 2535,
      "resume_skills": [
        "airflow",
        "aws",
        "azure",
        "ci/cd",
        "cloud platforms",
        "customer segmentation",
        "deep learning",
        "docker",
        "feature engineering",
        "feature engineering pipelines",
        "gcp",
        "git",
        "jenkins",
        "kubernetes",
        "machine learning",
        "mlflow",
        "pandas",
        "power bi",
        "python",
        "sagemaker",
        "spark",
        "sql",
        "tableau",
        "terraform"
      ],
      "job_skills": [
        "airflow",
        "aws",
        "ci/cd",
        "data pipeline",
        "databricks",
        "distributed systems",
        "feature store",
        "java",
        "kubernetes",
        "lambda",
        "machine learning",
        "mlflow",
        "pyspark",
        "python",
        "s3",
        "scala",
        "sql"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate has nine years of data‑engineering experience, strong Python/Apache Spark skills, AWS knowledge, and proven pipeline CI/CD work—sufficiently transferable to design, build, and optimize Databricks and AWS data systems and a Feature Store.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Aswin.pdf",
      "job_rank": 1,
      "job_title": "Tenure-track faculty position in Data Science and Artificial Intelligence for Civil and Systems Engineering - all ranks",
      "job_company": "The Johns Hopkins University",
      "job_id": "4319246352",
      "skill_score": 0.0,
      "semantic_score": 0.4309280514717102,
      "topic_score": 0.5288758873939514,
      "final_score": 0.4799019694328308,
      "resume_skills_count": 13,
      "job_skills_count": 3,
      "matching_skills_count": 0,
      "resume_text_length": 11180,
      "resume_skills": [
        "aws",
        "azure",
        "c",
        "ci/cd",
        "docker",
        "gitlab",
        "java",
        "javascript",
        "kubernetes",
        "node.js",
        "python",
        "r",
        "sql"
      ],
      "job_skills": [
        "collaboration",
        "machine learning",
        "mentoring"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate has a CS PhD and strong software‑engineering research, but lacks the core expertise in data‑science/AI methods applied to civil and systems engineering, and has no record of publications or projects in that domain.",
      "llm_recommendations": [
        "Seek interdisciplinary collaborations with civil or systems engineering faculty to develop AI‑driven projects in construction, infrastructure, or sustainability.",
        "Publish at least one interdisciplinary paper that uses machine‑learning or data‑science techniques on civil‑engineering datasets to demonstrate domain relevance.",
        "Update your CV and teaching statement to highlight courses or workshops you have taught on data science, AI, or their applications in engineering contexts.",
        "Pursue coursework, a certificate, or a short‑term program in civil engineering analytics or structural modeling to build domain knowledge.",
        "Highlight your mentoring and inclusive‑excellence experience, tailoring it to support graduate students in cross‑disciplinary research."
      ],
      "linkedin_keywords": [
        "Data Science",
        "Artificial Intelligence",
        "Machine Learning",
        "Civil Engineering",
        "Systems Engineering",
        "Computational Modeling",
        "Structural Engineering",
        "Urban Planning",
        "Built Environment",
        "Sustainability",
        "Optimization",
        "Big Data."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Aswin_A_van_den_Berg.pdf",
      "job_rank": 1,
      "job_title": "Tenure-track faculty position in Data Science and Artificial Intelligence for Civil and Systems Engineering - all ranks",
      "job_company": "The Johns Hopkins University",
      "job_id": "4319246352",
      "skill_score": 0.0,
      "semantic_score": 0.4309280514717102,
      "topic_score": 0.5288758873939514,
      "final_score": 0.4799019694328308,
      "resume_skills_count": 13,
      "job_skills_count": 3,
      "matching_skills_count": 0,
      "resume_text_length": 11180,
      "resume_skills": [
        "aws",
        "azure",
        "c",
        "ci/cd",
        "docker",
        "gitlab",
        "java",
        "javascript",
        "kubernetes",
        "node.js",
        "python",
        "r",
        "sql"
      ],
      "job_skills": [
        "collaboration",
        "machine learning",
        "mentoring"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The applicant’s research and experience are focused on software transformation and programming languages, not on data science/AI applied to civil or systems engineering contexts; this omission of a record in AI/data‑science research and domain relevance precludes meeting the core job functions.",
      "llm_recommendations": [
        "Pursue collaborations with civil and systems engineering faculty to develop AI/data‑science projects that address infrastructure or environmental challenges.",
        "Publish at least one peer‑reviewed paper applying machine learning or data‑driven modeling to civil engineering problems (e.g., traffic flow, structural health monitoring).",
        "Obtain certifications or short courses in applied AI, such as deep learning for physical systems, to demonstrate competence in modern techniques.",
        "Highlight teaching and mentoring experience in university courses, and seek opportunities to develop interdisciplinary courses bridging computing and engineering.",
        "Build a clear research vision that links current programming language expertise to emerging AI tools for civil and systems engineering."
      ],
      "linkedin_keywords": [
        "data science",
        "machine learning",
        "AI",
        "civil engineering",
        "systems engineering",
        "infrastructure",
        "smart cities",
        "structural health monitoring",
        "transportation modeling",
        "building information modeling (BIM)",
        "sustainable engineering",
        "sensor networks",
        "simulation",
        "optimization",
        "uncertainty quantification",
        "environmental analytics",
        "computational modeling",
        "engineering education."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Avery_Chen_20251129_062555.pdf",
      "job_rank": 1,
      "job_title": "Data Analyst",
      "job_company": "Sports Research",
      "job_id": "4317975045",
      "skill_score": 0.2,
      "semantic_score": 0.6728466158670144,
      "topic_score": 0.9236457347869873,
      "final_score": 0.8385969402616007,
      "resume_skills_count": 14,
      "job_skills_count": 10,
      "matching_skills_count": 4,
      "resume_text_length": 2206,
      "resume_skills": [
        "business intelligence",
        "collaboration",
        "data pipeline",
        "data visualization",
        "excel",
        "machine learning",
        "power bi",
        "python",
        "r",
        "reporting",
        "sql",
        "statistical analysis",
        "strategic planning",
        "tableau"
      ],
      "job_skills": [
        "attention to detail",
        "business intelligence",
        "communication",
        "data analysis",
        "data visualization",
        "leadership",
        "problem-solving skills",
        "process improvement",
        "reporting",
        "sql"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate possesses essential experience and skills—proficient SQL, data visualization in Tableau/Power BI, proven ability to build dashboards and KPI reports, and a strong background in data cleaning, analysis, and stakeholder communication—indicating they can perform the core responsibilities of the Data Analyst role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Avery_Ortiz_20251129_062431.pdf",
      "job_rank": 1,
      "job_title": "Sr Data Analyst with verification",
      "job_company": "Horizontal Talent",
      "job_id": "4318091470",
      "skill_score": 0.18181818181818182,
      "semantic_score": 0.6760431729568072,
      "topic_score": 0.8575634360313416,
      "final_score": 0.8092027036769699,
      "resume_skills_count": 14,
      "job_skills_count": 12,
      "matching_skills_count": 4,
      "resume_text_length": 1913,
      "resume_skills": [
        "data cleaning",
        "data visualization",
        "etl",
        "excel",
        "git",
        "machine learning",
        "power bi",
        "python",
        "r",
        "reporting",
        "snowflake",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "azure",
        "azure data factory",
        "ci/cd",
        "data pipeline",
        "etl",
        "oracle",
        "python",
        "root cause analysis",
        "shell scripting",
        "snowflake",
        "sql",
        "sql server"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate meets the experience level and has strong ETL experience with Snowflake and Python, but lacks the indispensable Azure Data Factory and SSIS/Informatica expertise required for this role.",
      "llm_recommendations": [
        "Enroll in a Microsoft Azure Data Factory training course and obtain an Azure Data Engineer Associate certification.",
        "Gain hands‑on experience by building a pilot ETL pipeline in Azure Data Factory, demonstrating end‑to‑end data flow.",
        "Complete a short certification or project with SSIS or Informatica PowerCenter to show competency in those tools.",
        "Highlight transferable ETL skills, Python scripting, and Snowflake modeling in the resume to emphasize relevant expertise.",
        "Consider pursuing additional training in PowerShell scripting if time permits to cover additional preferred skills."
      ],
      "linkedin_keywords": [
        "Data Engineering",
        "Snowflake",
        "ETL",
        "Azure Data Factory",
        "SSIS",
        "Informatica PowerCenter",
        "PowerShell Scripting",
        "Python Scripting",
        "Data Warehousing",
        "Azure Data Engineer Associate"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Ayesha_Patel_20251129_060413.pdf",
      "job_rank": 1,
      "job_title": "Data Integration Engineer with verification",
      "job_company": "Boot Barn",
      "job_id": "4319462092",
      "skill_score": 0.15625,
      "semantic_score": 0.7057405710220337,
      "topic_score": 0.6333778500556946,
      "final_score": 0.7211905838921666,
      "resume_skills_count": 16,
      "job_skills_count": 21,
      "matching_skills_count": 5,
      "resume_text_length": 2385,
      "resume_skills": [
        "airflow",
        "aws",
        "bigquery",
        "business intelligence",
        "data pipeline",
        "etl",
        "kpi reporting",
        "machine learning",
        "power bi",
        "python",
        "reporting",
        "s3",
        "snowflake",
        "spark",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "azure",
        "azure data factory",
        "ci/cd",
        "communication",
        "data integration",
        "elt",
        "etl",
        "git",
        "mongodb",
        "problem solving",
        "python",
        "reporting",
        "scala",
        "spark",
        "sql",
        "sql server",
        "stakeholder management",
        "teamwork",
        "time management",
        "user stories",
        "validation rules"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate demonstrates extensive experience building Spark‑based ETL pipelines, orchestrating workflows with Airflow, modeling data warehouses, and enforcing data quality—core functions required for this role, and with a strong learning posture they can quickly bridge the remaining gaps (Azure Data Factory, SSIS, PowerShell scripts, and Data Vault fundamentals).",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Ben-Fishbeins-Resume-Digital-Marketing-Specialist.pdf",
      "job_rank": 1,
      "job_title": "Digital Marketing Specialist",
      "job_company": "ELOKON Group",
      "job_id": "4332476366",
      "skill_score": 0.18181818181818182,
      "semantic_score": 0.6005861104058725,
      "topic_score": 0.9426797032356262,
      "final_score": 0.813154196489704,
      "resume_skills_count": 11,
      "job_skills_count": 15,
      "matching_skills_count": 4,
      "resume_text_length": 2708,
      "resume_skills": [
        "content marketing",
        "conversion rate optimization",
        "digital marketing",
        "facebook ads",
        "google ads",
        "google analytics",
        "google search console",
        "on-page seo",
        "performance analysis",
        "reporting",
        "seo"
      ],
      "job_skills": [
        "a/b testing",
        "communication",
        "digital marketing",
        "email marketing",
        "facebook ads",
        "google analytics",
        "google tag manager",
        "lead generation",
        "mailchimp",
        "market research",
        "marketing automation",
        "salesforce",
        "sem",
        "seo",
        "technical seo"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks a key requirement: authorization to work in the United States without sponsorship, which is a mandatory prerequisite for this role.",
      "llm_recommendations": [
        "Obtain U.S. work authorization (e.g., H-1B, L-1, or green card) before applying.",
        "Highlight any U.S.-based project experience or remote collaboration with U.S. clients to demonstrate relevance.",
        "Acquire certifications in popular U.S. email marketing platforms (e.g., Mailchimp, Campaign Monitor) to strengthen marketing skill set.",
        "Showcase any experience managing B2B or industrial digital campaigns, if applicable, to align with the company's industry focus."
      ],
      "linkedin_keywords": [
        "digital marketing specialist",
        "SEO specialist",
        "SEM manager",
        "Google Ads",
        "Google Analytics",
        "WordPress",
        "content marketing",
        "email marketing",
        "paid media",
        "B2B marketing",
        "industrial marketing",
        "U.S. work authorization."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Boni Vasius Rosen - Resume.pdf",
      "job_rank": 1,
      "job_title": "Junior Machine Learning Engineer",
      "job_company": "GITAA",
      "job_id": "4331137647",
      "skill_score": 0.19444444444444445,
      "semantic_score": 0.6877746991976929,
      "topic_score": 0.832529604434967,
      "final_score": 0.8067892334075991,
      "resume_skills_count": 24,
      "job_skills_count": 19,
      "matching_skills_count": 7,
      "resume_text_length": 5442,
      "resume_skills": [
        "agile",
        "azure",
        "azure ml",
        "collaboration",
        "critical thinking",
        "data analysis",
        "data analytics",
        "data pipeline",
        "data visualization",
        "deep learning",
        "docker",
        "forecasting",
        "java",
        "jira",
        "machine learning",
        "natural language processing",
        "process improvement",
        "project management",
        "python",
        "r",
        "spark",
        "sql",
        "tableau",
        "text classification"
      ],
      "job_skills": [
        "aws",
        "azure",
        "ci/cd",
        "cloud platforms",
        "customer segmentation",
        "data pipeline",
        "docker",
        "fastapi",
        "feature engineering",
        "flask",
        "forecasting",
        "gcp",
        "kubernetes",
        "machine learning",
        "nlp",
        "nosql",
        "operational efficiency",
        "python",
        "sql"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate possesses the core technical skills (Python, ML libraries, pipeline design, data preprocessing and model deployment), has over three years of industry data‑science experience, and meets the education and experience thresholds required for the Junior Machine Learning Engineer role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "BryceTsuyukiResume.pdf",
      "job_rank": 1,
      "job_title": "Principal Software Engineer with verification",
      "job_company": "Microsoft",
      "job_id": "4332807035",
      "skill_score": 0.25,
      "semantic_score": 0.48250207617841867,
      "topic_score": 0.13667625188827515,
      "final_score": 0.4821918730250102,
      "resume_skills_count": 16,
      "job_skills_count": 9,
      "matching_skills_count": 5,
      "resume_text_length": 4145,
      "resume_skills": [
        "aws",
        "bash",
        "c",
        "django",
        "docker",
        "git",
        "java",
        "javascript",
        "jira",
        "kubernetes",
        "leadership",
        "notion",
        "postgresql",
        "python",
        "ruby",
        "terraform"
      ],
      "job_skills": [
        "azure",
        "c",
        "cloud computing",
        "distributed systems",
        "java",
        "javascript",
        "kubernetes",
        "python",
        "software engineering"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate meets only a few of the essential requirements (Kubernetes, Terraform, and some scripting), but lacks the required 6+ years of IP networking, distributed systems, and Linux experience, as well as 5+ years of SDN technology and container expertise needed for a Principal Software Engineer role.",
      "llm_recommendations": [
        "Gain hands‑on experience with IP networking and SDN (e.g., VXLAN, MPLS, EVPN) through a more senior site‑oriented or networking role, or by contributing to open‑source SDN projects.",
        "Pursue certifications that validate networking and Linux skills such as Cisco CCNP (or CCNA) and Linux Professional Institute Certification (LPIC‑3).",
        "Highlight any large‑scale clustering or networking work in future positions, framing it as distributed systems experience.",
        "Build a portfolio project that demonstrates end‑to‑end control plane and data plane implementation using Kubernetes and SDN constructs.",
        "Emphasize transferable skills (e.g., service mesh deployment, Terraform, monitoring) in resumes and interviews, showing readiness to ramp up quickly."
      ],
      "linkedin_keywords": [
        "Linux",
        "SDN",
        "MPLS",
        "EVPN",
        "VXLAN",
        "Kubernetes",
        "Docker",
        "Service Mesh",
        "Networking",
        "Distributed Systems",
        "Terraform",
        "DevOps",
        "Infrastructure Engineering",
        "Cloud Operations",
        "Site Reliability Engineering."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Cartographic_Analyst_andrew.pdf",
      "job_rank": 1,
      "job_title": "Training Instructor - Pittsburgh, PA with verification",
      "job_company": "VetJobs",
      "job_id": "4318849257",
      "skill_score": 0.07142857142857142,
      "semantic_score": 0.3741736373564062,
      "topic_score": 0.6631463170051575,
      "final_score": 0.5530414073821546,
      "resume_skills_count": 12,
      "job_skills_count": 3,
      "matching_skills_count": 1,
      "resume_text_length": 7772,
      "resume_skills": [
        "aws",
        "azure",
        "communication",
        "data analysis",
        "data visualization",
        "excel",
        "leadership",
        "microsoft excel",
        "presentation skills",
        "python",
        "sql",
        "time management"
      ],
      "job_skills": [
        "coaching",
        "communication",
        "mentoring"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate does not meet the essential requirement of having a Bachelor’s degree in a nuclear‑related field and prior experience working in a nuclear facility, which are core to the instructor role.",
      "llm_recommendations": [
        "Pursue formal education or certifications in Nuclear Engineering, Physics, or a related discipline to establish foundational knowledge.",
        "Complete an NRC Reactor Operator or Senior Reactor Operator license (or an equivalent nuclear operations credential) to satisfy the preferred licensing requirement.",
        "Enroll in specialized nuclear plant systems and safety courses (e.g., INPO or NRC training programs) to gain domain expertise.",
        "Highlight existing training and mentoring experience on résumé, framing it as transferable instructional capability while explicitly noting the intent to transition into the nuclear sector.",
        "Network with nuclear industry professionals and seek apprenticeship or short‑term placement opportunities within nuclear plants to acquire hands‑on exposure."
      ],
      "linkedin_keywords": [
        "Nuclear Plant Training",
        "Nuclear Engineering",
        "NRC Operator",
        "SAT Training",
        "Nuclear Safety",
        "In-Plant Training",
        "Instructor Certification",
        "Nuclear Operations",
        "Nuclear Licensing",
        "Defense Training."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Chris-Pepper-Resume.pdf",
      "job_rank": 1,
      "job_title": "Senior Software Engineer with verification",
      "job_company": "HSBC",
      "job_id": "4319495128",
      "skill_score": 0.0,
      "semantic_score": 0.5504853129386902,
      "topic_score": 0.5926564335823059,
      "final_score": 0.571570873260498,
      "resume_skills_count": 7,
      "job_skills_count": 6,
      "matching_skills_count": 0,
      "resume_text_length": 7643,
      "resume_skills": [
        "aws",
        "data analysis",
        "github",
        "machine learning",
        "oracle",
        "ssl",
        "tls"
      ],
      "job_skills": [
        "attention to detail",
        "collaboration",
        "problem-solving skills",
        "risk management",
        "software development",
        "time management"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate holds a CS degree, has extensive documentation and system‑engineering experience, strong analytical and collaborative skills, and a proven track record of learning new domains—enough to perform the core functions of the verification and compliance role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Connor-Scott-Business-Development-Manager.pdf",
      "job_rank": 1,
      "job_title": "Entry Level Sales Representative with verification",
      "job_company": "Calculated Hire",
      "job_id": "4319392031",
      "skill_score": 0.1111111111111111,
      "semantic_score": 0.6311210581994375,
      "topic_score": 0.6296197175979614,
      "final_score": 0.671440344798844,
      "resume_skills_count": 8,
      "job_skills_count": 2,
      "matching_skills_count": 1,
      "resume_text_length": 4145,
      "resume_skills": [
        "account management",
        "business analysis",
        "coaching",
        "communication",
        "data analytics",
        "market research",
        "negotiation",
        "requirements gathering"
      ],
      "job_skills": [
        "c",
        "communication"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate meets the essential qualifications—Bachelor’s degree in Sales and Marketing, proven experience building client relationships and presenting solutions to senior executives, strong communication skills, and a track record of meeting sales targets—indicating they can effectively perform the core functions of an entry‑level sales representative.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Corey_Reichle.pdf",
      "job_rank": 1,
      "job_title": "HPE ProLiant Server Engineer – L2 (Compute + VMware/Linux) with verification",
      "job_company": "Talent Worx",
      "job_id": "4332413251",
      "skill_score": 0.125,
      "semantic_score": 0.6254829838008324,
      "topic_score": 0.8430702090263367,
      "final_score": 0.7674920218618865,
      "resume_skills_count": 6,
      "job_skills_count": 3,
      "matching_skills_count": 1,
      "resume_text_length": 5345,
      "resume_skills": [
        "decision making",
        "go",
        "jira",
        "oracle",
        "php",
        "vpn"
      ],
      "job_skills": [
        "oracle",
        "perl",
        "shell scripting"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate’s 15+ years of L3‑style system administration—including WebSphere, Tomcat, Linux, networking, shell scripting and security‑clearanced employment—provides a strong foundation that can be leveraged to rapidly acquire IBM MQ and other required middleware skills for the HPE ProLiant Server Engineer role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Daniel_Cooper.pdf",
      "job_rank": 1,
      "job_title": "Software Engineer with verification",
      "job_company": "CACI International Inc",
      "job_id": "4319099667",
      "skill_score": 0.13793103448275862,
      "semantic_score": 0.688518464565282,
      "topic_score": 0.5437365174293518,
      "final_score": 0.6690754232735491,
      "resume_skills_count": 22,
      "job_skills_count": 11,
      "matching_skills_count": 4,
      "resume_text_length": 5398,
      "resume_skills": [
        "agile",
        "aws",
        "bigquery",
        "c",
        "collaboration",
        "github",
        "java",
        "javascript",
        "jira",
        "leadership",
        "mongodb",
        "mysql",
        "node.js",
        "postgresql",
        "python",
        "react",
        "s3",
        "siem",
        "splunk",
        "sql",
        "typescript",
        "wireshark"
      ],
      "job_skills": [
        "agile",
        "azure",
        "communication",
        "docker",
        "gitlab",
        "javascript",
        "oracle",
        "postgresql",
        "python",
        "reporting",
        "software development"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate blends 8+ years of technical experience with solid Python, Linux, and database development skills, proven Agile and DevSecOps practices, and strong cybersecurity expertise—sufficient to meet the core duties of the role, with only minor gaps such as containerization that can be quickly acquired on the job.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Danielle-Connolly-Resume.pdf",
      "job_rank": 1,
      "job_title": "Category Analyst, Own Brands with verification",
      "job_company": "Daymon",
      "job_id": "4323959918",
      "skill_score": 0.07142857142857142,
      "semantic_score": 0.5729662692218667,
      "topic_score": 0.2619950771331787,
      "final_score": 0.4590891965219853,
      "resume_skills_count": 13,
      "job_skills_count": 2,
      "matching_skills_count": 1,
      "resume_text_length": 5241,
      "resume_skills": [
        "a/b testing",
        "communication",
        "community management",
        "content marketing",
        "excel",
        "google analytics",
        "hubspot",
        "leadership",
        "merchandising",
        "ppc",
        "project management",
        "seo",
        "social media management"
      ],
      "job_skills": [
        "decision making",
        "excel"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks the required experience with the Retail Link system (a key core skill for running retail POS data analysis), which is essential for this Category Analyst role.",
      "llm_recommendations": [
        "Complete a short Retail Link training or certification to demonstrate familiarity with the platform.",
        "Gain hands‑on experience with a POS or retailer analytics database (e.g., via a freelance project, internship, or volunteer role).",
        "Highlight transferable Excel, PowerPoint, and data‑analysis expertise, noting any vendor partnership work, to show readiness to generate actionable insights.",
        "Build a portfolio of sample retail analysis reports or dashboards to showcase your ability to translate sales data into business decisions.",
        "Expand your network within the retail analytics community (e.g., LinkedIn groups, industry events) to uncover entry‑level opportunities that can provide the required domain experience."
      ],
      "linkedin_keywords": [
        "Retail Analysis",
        "Retail Link",
        "POS Analytics",
        "Category Management",
        "Vendor Management",
        "Data Analysis",
        "Excel",
        "PowerPoint",
        "Reporting",
        "E‑commerce Analytics",
        "Retail Analytics",
        "Sales Insights."
      ],
      "llm_error": null
    },
    {
      "resume_file": "David-Zhang-Resume.pdf",
      "job_rank": 1,
      "job_title": "Senior Information Security GRC Analyst",
      "job_company": "Veritas Search Group",
      "job_id": "4318083022",
      "skill_score": 0.05,
      "semantic_score": 0.5537660983940315,
      "topic_score": 0.542515218257904,
      "final_score": 0.5707336254096694,
      "resume_skills_count": 12,
      "job_skills_count": 9,
      "matching_skills_count": 1,
      "resume_text_length": 3222,
      "resume_skills": [
        "aws",
        "communication",
        "git",
        "mongodb",
        "mysql",
        "next.js",
        "python",
        "react",
        "s3",
        "seo",
        "sql",
        "typescript"
      ],
      "job_skills": [
        "communication",
        "data analysis",
        "excel",
        "incident response",
        "leadership",
        "power bi",
        "presentation skills",
        "reporting",
        "risk management"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks the core GRC, policy‑development, and regulatory framework experience required for a senior GRC analyst role.",
      "llm_recommendations": [
        "Pursue entry‑level GRC roles or internships focused on policy and compliance to build direct experience.",
        "Obtain industry certifications such as CISA or CISM and complete relevant coursework on NIST, SOX, PCI‑DSS, etc.",
        "Gain hands‑on exposure to GRC platforms (Archer, ServiceNow, OneTrust) through training programs or vendor workshops.",
        "Volunteer to draft or review security policies, risk assessments, and remediation plans within a corporate setting to demonstrate tangible results.",
        "Highlight any audit support or compliance-related projects in future resume versions to bridge the experience gap."
      ],
      "linkedin_keywords": [
        "GRC",
        "Information Security",
        "Risk Management",
        "Policy Development",
        "Regulatory Compliance",
        "NIST",
        "PCI‑DSS",
        "SOX Compliance",
        "Audit Support",
        "CISA",
        "CISM",
        "Archer",
        "ServiceNow",
        "OneTrust",
        "Cybersecurity Governance"
      ],
      "llm_error": null
    },
    {
      "resume_file": "David_J_Frederickson.pdf",
      "job_rank": 1,
      "job_title": "Procurement Executive",
      "job_company": "Rockhill Asia",
      "job_id": "4318667112",
      "skill_score": 0.2631578947368421,
      "semantic_score": 0.7769247160509073,
      "topic_score": 0.847687304019928,
      "final_score": 0.861699165289255,
      "resume_skills_count": 13,
      "job_skills_count": 11,
      "matching_skills_count": 5,
      "resume_text_length": 4331,
      "resume_skills": [
        "communication",
        "excel",
        "forecasting",
        "leadership",
        "logistics",
        "microsoft excel",
        "operational efficiency",
        "oracle",
        "process improvement",
        "procurement",
        "project management",
        "reporting",
        "sales operations"
      ],
      "job_skills": [
        "collaboration",
        "communication",
        "cost analysis",
        "cross-functional collaboration",
        "excel",
        "logistics",
        "merchandising",
        "negotiation",
        "procurement",
        "reporting",
        "stakeholder management"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate’s procurement manager role at Apple demonstrates direct experience in supplier management, cost negotiation, PO and invoice handling, and cross‑functional collaboration, matching the core responsibilities of the Procurement Executive position.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Dikshit_Khandelwal.pdf",
      "job_rank": 1,
      "job_title": "Software Engineer with verification",
      "job_company": "Calance",
      "job_id": "4318607942",
      "skill_score": 0.21212121212121213,
      "semantic_score": 0.5807317495346069,
      "topic_score": 0.48343944549560547,
      "final_score": 0.6313401677391746,
      "resume_skills_count": 18,
      "job_skills_count": 22,
      "matching_skills_count": 7,
      "resume_text_length": 5166,
      "resume_skills": [
        "ansible",
        "aws",
        "azure",
        "c",
        "ci/cd",
        "communication",
        "docker",
        "dynamodb",
        "ec2",
        "gcp",
        "go",
        "javascript",
        "kubernetes",
        "php",
        "python",
        "s3",
        "scala",
        "terraform"
      ],
      "job_skills": [
        "aws",
        "ci/cd",
        "communication",
        "docker",
        "ec2",
        "frontend development",
        "github",
        "github actions",
        "gitlab",
        "kubernetes",
        "lambda",
        "leadership",
        "mentoring",
        "microservices",
        "mongodb",
        "nosql",
        "react",
        "root cause analysis",
        "s3",
        "software development",
        "teamwork",
        "vue.js"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks several essential, non‑replaceable qualifications—such as a deep MongoDB background, proven senior‑level AWS architecture experience, and advanced observability/CI‑CD expertise—and is only a few months into a software engineering career, which precludes him from meeting the role’s 10‑+ years backend and Golang seniority expectations.",
      "llm_recommendations": [
        "Complete hands‑on Go microservices projects that integrate MongoDB, Docker, Kubernetes, and Helm to build a concrete portfolio.",
        "Undertake a focused AWS certification path (e.g., AWS Certified Solutions Architect – Professional) and architect a small‑scale, production‑grade cloud solution to demonstrate real‑world skill.",
        "Build CI/CD pipelines on GitLab or GitHub Actions, incorporating unit testing, SonarQube, and basic observability with OpenTelemetry or Prometheus, and document the outcomes.",
        "Seek mentorship or apprenticeship opportunities on a senior backend team to accumulate practical experience and a clearer track record.",
        "Actively contribute to open‑source Go projects or internal engineering forums to showcase problem‑solving, code quality, and collaboration at a senior level."
      ],
      "linkedin_keywords": [
        "Go",
        "Golang",
        "MongoDB",
        "AWS Architecture",
        "Kubernetes",
        "Docker",
        "Helm",
        "CI/CD",
        "GitLab",
        "GitHub Actions",
        "OpenTelemetry",
        "Datadog",
        "Cloud Native",
        "Backend Engineering",
        "Senior Software Engineer."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Dr_Maya_Patel_20251129_060920.pdf",
      "job_rank": 1,
      "job_title": "Deep Learning Engineer",
      "job_company": "Shields Group Search",
      "job_id": "4319372032",
      "skill_score": 0.041666666666666664,
      "semantic_score": 0.6211822258721467,
      "topic_score": 0.7775011658668518,
      "final_score": 0.7118691252082701,
      "resume_skills_count": 20,
      "job_skills_count": 5,
      "matching_skills_count": 1,
      "resume_text_length": 2942,
      "resume_skills": [
        "aws",
        "azure",
        "ci/cd",
        "data analysis",
        "deep learning",
        "docker",
        "etl",
        "git",
        "kubernetes",
        "machine learning",
        "mentoring",
        "natural language processing",
        "nlp",
        "nosql",
        "python",
        "sagemaker",
        "spark",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "collaboration",
        "data pipeline",
        "deep learning",
        "go",
        "teamwork"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate has a Ph.D., extensive PyTorch experience, a strong CV/vision background, and proven production‑level MLOps work, all of which cover the core requirements for the Deep Learning Engineer role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Elena_Moreno_20251129_064803.pdf",
      "job_rank": 1,
      "job_title": "AI/ML Software Engineer with verification",
      "job_company": "Guidehouse",
      "job_id": "4332300185",
      "skill_score": 0.18181818181818182,
      "semantic_score": 0.6300304752570118,
      "topic_score": 0.529812216758728,
      "final_score": 0.6562992830973481,
      "resume_skills_count": 22,
      "job_skills_count": 17,
      "matching_skills_count": 6,
      "resume_text_length": 2622,
      "resume_skills": [
        "a/b testing",
        "data analysis",
        "data analytics",
        "data ingestion",
        "data pipeline",
        "data visualization",
        "decision making",
        "docker",
        "feature engineering",
        "feature engineering pipelines",
        "kubernetes",
        "leadership",
        "machine learning",
        "natural language processing",
        "nlp",
        "numpy",
        "pandas",
        "python",
        "r",
        "sentiment analysis",
        "sql",
        "statistical analysis"
      ],
      "job_skills": [
        "aws",
        "communication",
        "data pipeline",
        "deep learning",
        "javascript",
        "machine learning",
        "numpy",
        "oracle",
        "outreach",
        "pandas",
        "postgresql",
        "python",
        "rest apis",
        "software development",
        "software engineering",
        "sql",
        "typescript"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The résumé lacks essential experience in JavaScript/TypeScript, AWS Cloud deployment, and Retrieval‑Augmented Generation (RAG) frameworks—skills critical for this role.",
      "llm_recommendations": [
        "Complete an intensive JavaScript/TypeScript bootcamp or online course and build small full‑stack projects to demonstrate proficiency.",
        "Develop a RAG prototype (using LangChain, Haystack, or similar) and publish the code and results (e.g., on GitHub or a personal project blog).",
        "Gain hands‑on AWS experience: complete the AWS Certified Developer – Associate exam, and deploy a sample ML model on SageMaker with Boto3 scripting.",
        "Highlight any chatbot or conversational AI work by detailing architecture, API design, and real‑time inference metrics.",
        "If open to federal work, begin the Public Trust clearance process or obtain a similar security clearance to meet eligibility."
      ],
      "linkedin_keywords": [
        "AI Engineer",
        "NLP Engineer",
        "Machine Learning Engineer",
        "Retrieval Augmented Generation",
        "RAG",
        "LangChain",
        "Haystack",
        "Docker",
        "Kubernetes",
        "AWS SageMaker",
        "Boto3",
        "JavaScript",
        "TypeScript",
        "REST API",
        "Cloud DevOps",
        "Data Pipeline",
        "Chatbot Development",
        "Transformer Models",
        "Multilingual NLP",
        "AI Chatbots."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Elena_Müller_20251129_064138.pdf",
      "job_rank": 1,
      "job_title": "Data Analytics Software Engineer (Trading)",
      "job_company": "Fintal Partners",
      "job_id": "4318639448",
      "skill_score": 0.16,
      "semantic_score": 0.6677584054038069,
      "topic_score": 0.8016034364700317,
      "final_score": 0.7771319735870122,
      "resume_skills_count": 15,
      "job_skills_count": 14,
      "matching_skills_count": 4,
      "resume_text_length": 1922,
      "resume_skills": [
        "airflow",
        "data analysis",
        "data integration",
        "data visualization",
        "decision making",
        "etl",
        "leadership",
        "machine learning",
        "operational efficiency",
        "power bi",
        "python",
        "r",
        "reporting",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "data analysis",
        "data analytics",
        "data pipeline",
        "databricks",
        "docker",
        "kubernetes",
        "machine learning",
        "numpy",
        "pandas",
        "python",
        "reporting",
        "scipy",
        "snowflake",
        "spark"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "Elena has strong Python programming, experience building and automating ETL pipelines with Airflow, and proven ability to handle large datasets and build real‑time dashboards, which map to the core responsibilities of designing scalable data pipelines and analytical systems for a trading environment.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Elena_S_Marin_20251129_060041.pdf",
      "job_rank": 1,
      "job_title": "Senior Research Scientist with verification",
      "job_company": "NVIDIA",
      "job_id": "4332735576",
      "skill_score": 0.16666666666666666,
      "semantic_score": 0.5851642067779367,
      "topic_score": 0.38122233748435974,
      "final_score": 0.5693277267759569,
      "resume_skills_count": 8,
      "job_skills_count": 6,
      "matching_skills_count": 2,
      "resume_text_length": 1837,
      "resume_skills": [
        "docker",
        "embeddings",
        "feature engineering",
        "leadership",
        "natural language processing",
        "nlp",
        "sentiment analysis",
        "text classification"
      ],
      "job_skills": [
        "deep learning",
        "machine learning",
        "natural language processing",
        "nlp",
        "python",
        "translation"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks the essential academic credentials, proven research output, and direct experience in speech‐recognition and large‑scale training required for a Senior Research Scientist role at NVIDIA.",
      "llm_recommendations": [
        "Pursue advanced formal education (e.g., PhD or equivalent research program) focusing on Deep Learning for Speech and NLP.",
        "Lead or join research projects that yield publishable papers in top NLP/ASR conferences (ICASSP, ACL, Interspeech).",
        "Contribute to open‑source projects (NeMo, Hugging Face, Kaldi) to demonstrate community engagement.",
        "Build experience in ASR and TTS by developing prototype models and participating in relevant competitions.",
        "Engage with academia as a reviewer or workshop organizer to build credibility in the research community."
      ],
      "linkedin_keywords": [
        "Deep Learning",
        "Speech Recognition",
        "NLP",
        "PyTorch",
        "Transformers",
        "ASR",
        "TTS",
        "Large‑Scale Training",
        "Open Source Contributions",
        "Research Scientist",
        "Publication Record."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Garrick_Hilliard.pdf",
      "job_rank": 1,
      "job_title": "Senior QA Automation Engineer (Playwright / Selenium | eCommerce Domain) with verification",
      "job_company": "Sky Systems, Inc. (SkySys)",
      "job_id": "4332444261",
      "skill_score": 0.09090909090909091,
      "semantic_score": 0.6352561929669276,
      "topic_score": 0.6097471714019775,
      "final_score": 0.6568197110767751,
      "resume_skills_count": 16,
      "job_skills_count": 8,
      "matching_skills_count": 2,
      "resume_text_length": 1448,
      "resume_skills": [
        "aws",
        "c",
        "communication",
        "critical thinking",
        "cypress",
        "dynamodb",
        "java",
        "javascript",
        "lambda",
        "mysql",
        "php",
        "rust",
        "salesforce",
        "sql",
        "sql server",
        "unit testing"
      ],
      "job_skills": [
        "agile",
        "communication",
        "confluence",
        "javascript",
        "jira",
        "scrum",
        "selenium",
        "typescript"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate does not meet the essential requirements of senior‑level QA automation (5+ years of experience, Playwright/Selenium proficiency, and eCommerce testing background).",
      "llm_recommendations": [
        "Gain hands‑on experience with Playwright or Selenium through side projects or online courses.",
        "Build eCommerce‑related test suites, preferably for web and mobile platforms, to demonstrate domain knowledge.",
        "Pursue certifications or structured training in automated testing methodologies to bolster credibility.",
        "Highlight transferable skills (JavaScript, Cypress automation, Agile familiarity) in a tailored resume and cover letter.",
        "Leverage past projects that involved integration with payment gateways (Stripe) to underscore experience with critical eCommerce systems."
      ],
      "linkedin_keywords": [
        "Playwright",
        "Selenium",
        "JavaScript QA",
        "Automation Engineer",
        "eCommerce testing",
        "Cypress",
        "Agile QA",
        "Test Automation Framework",
        "Test Automation Engineer",
        "QA Automation Senior",
        "Frontend QA",
        "Mobile QA",
        "Cross‑browser testing",
        "End‑to‑end testing",
        "Jira",
        "Confluence"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Ginny_Kim_Resume.pdf",
      "job_rank": 1,
      "job_title": "Full Stack Senior Software Engineer - Post Trade Systems with verification",
      "job_company": "KKR",
      "job_id": "4317227328",
      "skill_score": 0.08695652173913043,
      "semantic_score": 0.5955829379309986,
      "topic_score": 0.6560333967208862,
      "final_score": 0.6583465875584691,
      "resume_skills_count": 27,
      "job_skills_count": 23,
      "matching_skills_count": 4,
      "resume_text_length": 5356,
      "resume_skills": [
        "communication",
        "data analysis",
        "docker",
        "etl",
        "excel",
        "forecasting",
        "git",
        "github",
        "knn",
        "lda",
        "logistic regression",
        "market research",
        "metabase",
        "mongodb",
        "nosql",
        "operational efficiency",
        "pandas",
        "postgresql",
        "power query",
        "python",
        "r",
        "sap",
        "sap erp",
        "sql",
        "topic modeling",
        "trend analysis",
        "xgboost"
      ],
      "job_skills": [
        "analytical thinking",
        "attention to detail",
        "aws",
        "change management",
        "cloud computing",
        "communication",
        "critical thinking",
        "ec2",
        "excel",
        "fastapi",
        "fp&a",
        "lambda",
        "leadership",
        "next.js",
        "problem-solving skills",
        "python",
        "react",
        "redshift",
        "reporting",
        "sql",
        "swift",
        "teamwork",
        "workflow automation"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate does not meet the essential core requirements—no evidence of 4‑6 years in financial services, nor experience with fixed‑income post‑trade systems, DTCC/SWIFT, AWS or test‑driven Python development needed for the role.",
      "llm_recommendations": [
        "Gain hands‑on experience in financial trading environments (e.g., internships or project work involving trade confirmations, settlements or DTCC/SWIFT workflows).",
        "Complete relevant technical training or certifications (AWS Certified Developer, Python TDD courses, REST API development workshops).",
        "Build a portfolio project that simulates post‑trade processing using Python, Aurora/MySQL, and deploy it on AWS services to demonstrate required stack competence.",
        "Seek mentorship or volunteer in roles that allow shadowing senior engineers on post‑trade systems to develop domain knowledge and leadership exposure.",
        "Highlight transferable skills (SQL, ETL, dashboarding) in updated resume and LinkedIn profile, framing them as applicable to finance and full‑stack development."
      ],
      "linkedin_keywords": [
        "Python Developer",
        "Financial Services",
        "Post-Trade Systems",
        "DTCC",
        "SWIFT",
        "Fixed Income",
        "AWS",
        "Aurora RDS",
        "Test‑Driven Development",
        "REST API",
        "Full Stack Engineer",
        "Senior Software Engineer",
        "Mendix",
        "Data Engineering."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Ira_Patel_20251129_061250.pdf",
      "job_rank": 1,
      "job_title": "Data Integration Engineer with verification",
      "job_company": "Boot Barn",
      "job_id": "4319462092",
      "skill_score": 0.21428571428571427,
      "semantic_score": 0.6624207496643066,
      "topic_score": 0.6472581028938293,
      "final_score": 0.7288024063621248,
      "resume_skills_count": 13,
      "job_skills_count": 21,
      "matching_skills_count": 6,
      "resume_text_length": 1913,
      "resume_skills": [
        "airflow",
        "aws",
        "data pipeline",
        "etl",
        "financial reporting",
        "git",
        "pytest",
        "python",
        "reporting",
        "snowflake",
        "spark",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "azure",
        "azure data factory",
        "ci/cd",
        "communication",
        "data integration",
        "elt",
        "etl",
        "git",
        "mongodb",
        "problem solving",
        "python",
        "reporting",
        "scala",
        "spark",
        "sql",
        "sql server",
        "stakeholder management",
        "teamwork",
        "time management",
        "user stories",
        "validation rules"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate brings five years of proven pipeline development, cloud-based ETL, Spark, SQL, Python, and automated testing experience that aligns with core data integration tasks, and has the skill set to quickly acquire the required SSIS, Azure Data Factory, PowerShell, and Data‑Vault modeling knowledge.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Ivy_Chen_20251129_062308.pdf",
      "job_rank": 1,
      "job_title": "AI/Machine Learning (NLP) Engineer with verification",
      "job_company": "Aslase",
      "job_id": "4332070318",
      "skill_score": 0.23076923076923078,
      "semantic_score": 0.6912168514286867,
      "topic_score": 0.5631640553474426,
      "final_score": 0.7132234256831267,
      "resume_skills_count": 19,
      "job_skills_count": 13,
      "matching_skills_count": 6,
      "resume_text_length": 2076,
      "resume_skills": [
        "airflow",
        "aws",
        "azure",
        "data analysis",
        "data analytics",
        "data pipeline",
        "docker",
        "feature engineering",
        "gcp",
        "machine learning",
        "natural language processing",
        "nlp",
        "prototyping",
        "python",
        "r",
        "reporting",
        "sagemaker",
        "sql",
        "statistical analysis"
      ],
      "job_skills": [
        "ci/cd",
        "data pipeline",
        "docker",
        "embeddings",
        "github",
        "kubernetes",
        "machine learning",
        "mlflow",
        "nlp",
        "prototyping",
        "python",
        "rest apis",
        "spark"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume shows strong general ML experience but lacks evidence of LLM API integration, LangChain/LlamaIndex orchestration, vector‑db work, or RAG-specific prompt engineering required to perform the core functions of this role.",
      "llm_recommendations": [
        "Gain hands‑on experience with OpenAI or Anthropic LLM APIs through personal projects or a short bootcamp.",
        "Build a portfolio of LLM‑based applications using LangChain or LlamaIndex, demonstrating orchestration and prompt design.",
        "Learn vector database fundamentals (Pinecone, Weaviate, FAISS) and integrate embeddings pipelines into a project.",
        "Document any MLOps work (Docker, Kubernetes, CI/CD, MLflow) that involves model deployment, monitoring, or versioning to showcase relevant pipelines.",
        "Highlight transferable deep‑learning, cloud, and pipeline expertise in cover letters or interviews, stressing quick relearning of new NLP tools."
      ],
      "linkedin_keywords": [
        "LLM integration",
        "LangChain",
        "LlamaIndex",
        "vector database",
        "RAG",
        "prompt engineering",
        "OpenAI API",
        "Anthropic",
        "Pinecone",
        "FAISS",
        "MLOps",
        "Docker",
        "Kubernetes",
        "CI/CD",
        "MLflow",
        "NLP engineer",
        "Deep learning",
        "AWS SageMaker",
        "Cloud ML",
        "Airflow",
        "Python ML",
        "TensorFlow",
        "PyTorch."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Jacob_Lee_Resume_2024.pdf",
      "job_rank": 1,
      "job_title": "Phó ban tài chính (Xây lắp, Năng lượng, BĐS)",
      "job_company": "NextCraze",
      "job_id": "4319346720",
      "skill_score": 0.3333333333333333,
      "semantic_score": 0.24416272272619455,
      "topic_score": 0.24416272272619455,
      "final_score": 0.49610848181746303,
      "resume_skills_count": 2,
      "job_skills_count": 2,
      "matching_skills_count": 1,
      "resume_text_length": 5557,
      "resume_skills": [
        "c",
        "r"
      ],
      "job_skills": [
        "c",
        "go"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate’s experience is concentrated in software development and open‑source projects, with no evidence of finance, budgeting, cost control or relevant industry experience required for a Deputy Finance officer role in construction, energy or real estate.",
      "llm_recommendations": [
        "Acquire foundational finance knowledge through courses or certifications (e.g., CPA, CFA level I, or a finance fundamentals bootcamp).",
        "Seek entry‑level or internship opportunities in corporate finance to gain hands‑on budgeting and financial analysis experience.",
        "Highlight any project cost estimating, resource budgeting or financial planning undertaken in software projects, and articulate how those skills translate to financial management in other industries."
      ],
      "linkedin_keywords": [
        "Financial Analyst",
        "Budgeting",
        "Cost Control",
        "Financial Planning",
        "Risk Management",
        "Financial Reporting",
        "Construction Finance",
        "Energy Finance",
        "Real Estate Finance",
        "Deputy Finance Officer"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Jae_Kim_20251129_060339.pdf",
      "job_rank": 1,
      "job_title": "Data Analyst",
      "job_company": "Sports Research",
      "job_id": "4317975045",
      "skill_score": 0.15,
      "semantic_score": 0.7438049163702081,
      "topic_score": 0.8942950367927551,
      "final_score": 0.8461924800942594,
      "resume_skills_count": 13,
      "job_skills_count": 10,
      "matching_skills_count": 3,
      "resume_text_length": 1654,
      "resume_skills": [
        "c",
        "data cleaning",
        "data visualization",
        "etl",
        "excel",
        "healthcare analytics",
        "power bi",
        "python",
        "r",
        "reporting",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "attention to detail",
        "business intelligence",
        "communication",
        "data analysis",
        "data visualization",
        "leadership",
        "problem-solving skills",
        "process improvement",
        "reporting",
        "sql"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate has 5 years of data analysis experience, strong SQL skills, proficiency with BI tools (Power BI and Tableau) for dashboard creation, KPI development, and cross‑functional reporting—all core requirements for the role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Jason_Stahl-Resume-1.pdf",
      "job_rank": 1,
      "job_title": "IT Security Analyst with verification",
      "job_company": "Stellar",
      "job_id": "4283914363",
      "skill_score": 0.058823529411764705,
      "semantic_score": 0.6743510961532593,
      "topic_score": 0.8780636191368103,
      "final_score": 0.7893716307247386,
      "resume_skills_count": 7,
      "job_skills_count": 11,
      "matching_skills_count": 1,
      "resume_text_length": 5478,
      "resume_skills": [
        "bash",
        "incident response",
        "loss prevention",
        "network security",
        "siem",
        "splunk",
        "wireshark"
      ],
      "job_skills": [
        "budgeting",
        "communication",
        "excel",
        "financial reporting",
        "negotiation",
        "penetration testing",
        "power bi",
        "procurement",
        "reporting",
        "siem",
        "vulnerability assessment"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate demonstrates key cybersecurity experience (SIEM, vulnerability remediation, pen‑testing), holds relevant certifications, and has vendor interaction and management experience that translate to the role’s essential functions.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Javier_Martinez_20251129_061437.pdf",
      "job_rank": 1,
      "job_title": "Senior Software Manager / Director of Engineering with verification",
      "job_company": "ThreadBeast",
      "job_id": "4318647215",
      "skill_score": 0.08823529411764706,
      "semantic_score": 0.41052588820457747,
      "topic_score": 0.6059500575065613,
      "final_score": 0.5516287399565486,
      "resume_skills_count": 18,
      "job_skills_count": 19,
      "matching_skills_count": 3,
      "resume_text_length": 2446,
      "resume_skills": [
        "aws",
        "bert",
        "ci/cd",
        "data analytics",
        "docker",
        "machine learning",
        "nlp",
        "pandas",
        "power bi",
        "python",
        "r",
        "reporting",
        "sagemaker",
        "sentiment analysis",
        "sql",
        "tableau",
        "text classification",
        "xgboost"
      ],
      "job_skills": [
        "ci/cd",
        "cloud infrastructure",
        "collaboration",
        "communication",
        "cross-functional collaboration",
        "forecasting",
        "gcp",
        "github",
        "github actions",
        "inventory management",
        "javascript",
        "leadership",
        "machine learning",
        "node.js",
        "operational efficiency",
        "php",
        "python",
        "react",
        "software engineering"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate does not demonstrate the essential engineering leadership experience or full‑stack and infrastructure expertise required for a senior management role.",
      "llm_recommendations": [
        "Seek opportunities to lead cross‑functional engineering projects or volunteer as a technical lead to build managerial experience.",
        "Gain hands‑on experience with front‑end/back‑end development and cloud architecture, focusing on GCP as the preferred platform.",
        "Pursue certifications in cloud engineering (e.g., Google Cloud Professional Cloud Architect) and software architecture to demonstrate technical breadth.",
        "Expand knowledge of subscription/e‑commerce domains and data‑driven product personalization to align with the industry context.",
        "Highlight any mentoring, code‑review, or project‑management responsibilities already performed to showcase transferable leadership skills."
      ],
      "linkedin_keywords": [
        "software engineering manager",
        "full-stack development",
        "cloud architecture",
        "GCP",
        "AI/ML engineering",
        "eCommerce",
        "subscription services",
        "team leadership",
        "product ownership",
        "CI/CD manager"
      ],
      "llm_error": null
    },
    {
      "resume_file": "JimDunneLinkedIn.pdf",
      "job_rank": 1,
      "job_title": "Junior Dot Net Developer",
      "job_company": "Greysoft",
      "job_id": "4332434216",
      "skill_score": 0.3333333333333333,
      "semantic_score": 0.41994412004781145,
      "topic_score": 0.5365630388259888,
      "final_score": 0.6521690529579334,
      "resume_skills_count": 6,
      "job_skills_count": 6,
      "matching_skills_count": 3,
      "resume_text_length": 9726,
      "resume_skills": [
        "c",
        "leadership",
        "network security",
        "oracle",
        "sql",
        "sql server"
      ],
      "job_skills": [
        "angular",
        "c",
        "javascript",
        "software development",
        "sql",
        "sql server"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate possesses extensive experience with C#, ASP.NET, SQL Server, OOP, and SDLC processes, meeting the core technical requirements for a Junior Dot Net Developer role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Jin_Liu_20251129_060129.pdf",
      "job_rank": 1,
      "job_title": "Machine Learning Engineer with verification",
      "job_company": "AmeriHealth Caritas",
      "job_id": "4332716222",
      "skill_score": 0.23809523809523808,
      "semantic_score": 0.6748905986417053,
      "topic_score": 0.8674152493476868,
      "final_score": 0.8256403230435779,
      "resume_skills_count": 14,
      "job_skills_count": 12,
      "matching_skills_count": 5,
      "resume_text_length": 2363,
      "resume_skills": [
        "a/b testing",
        "aws",
        "azure",
        "c",
        "ci/cd",
        "data pipeline",
        "etl",
        "gcp",
        "hive",
        "integration testing",
        "machine learning",
        "python",
        "spark",
        "sql"
      ],
      "job_skills": [
        "azure",
        "azure databricks",
        "azure ml",
        "ci/cd",
        "databricks",
        "feature store",
        "machine learning",
        "mlflow",
        "pyspark",
        "python",
        "software engineering",
        "sql"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate demonstrates strong transferable skills in Python, SQL, Spark, Azure cloud, and DevOps CI/CD, along with proven experience building and monitoring end‑to‑end ML pipelines, indicating they can effectively learn and implement Azure Databricks–specific components for the role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Jin_Soo_Park_20251129_060711.pdf",
      "job_rank": 1,
      "job_title": "Sr Data Analyst with verification",
      "job_company": "Horizontal Talent",
      "job_id": "4318091470",
      "skill_score": 0.12,
      "semantic_score": 0.5977920115652211,
      "topic_score": 0.8514409065246582,
      "final_score": 0.7576624839595469,
      "resume_skills_count": 16,
      "job_skills_count": 12,
      "matching_skills_count": 3,
      "resume_text_length": 2290,
      "resume_skills": [
        "airflow",
        "c",
        "cost analysis",
        "customer segmentation",
        "data cleaning",
        "data visualization",
        "etl",
        "excel",
        "logistics",
        "power bi",
        "project management",
        "python",
        "r",
        "reporting",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "azure",
        "azure data factory",
        "ci/cd",
        "data pipeline",
        "etl",
        "oracle",
        "python",
        "root cause analysis",
        "shell scripting",
        "snowflake",
        "sql",
        "sql server"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks key technical tools (Azure Data Factory, Snowflake, Informatica/SSIS) that are essential for the core data engineering responsibilities of this role.",
      "llm_recommendations": [
        "Acquire hands‑on experience with Azure Data Factory (e.g., through a project or online course).",
        "Build proficiency in Snowflake and other relational databases (Oracle, SQL Server) via pilot ETL or data warehouse work.",
        "Learn and apply ETL tools such as Informatica PowerCenter or SSIS to meet the required skill set.",
        "Highlight existing SQL and Python ETL experience, emphasizing transferable skills and rapid learning ability."
      ],
      "linkedin_keywords": [
        "Azure Data Factory",
        "Snowflake",
        "Informatica",
        "SSIS",
        "data warehousing",
        "ETL",
        "data engineering."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Jonathan_Gin.pdf",
      "job_rank": 1,
      "job_title": "Senior Software Engineer (Ruby)",
      "job_company": "Luminor Group",
      "job_id": "4311546134",
      "skill_score": 0.32,
      "semantic_score": 0.6062001351237388,
      "topic_score": 0.49764248728752136,
      "final_score": 0.6953064916198284,
      "resume_skills_count": 16,
      "job_skills_count": 17,
      "matching_skills_count": 8,
      "resume_text_length": 4260,
      "resume_skills": [
        "docker",
        "express.js",
        "git",
        "github",
        "go",
        "java",
        "javascript",
        "mysql",
        "python",
        "react",
        "ruby",
        "ruby on rails",
        "shopify",
        "splunk",
        "sql",
        "typescript"
      ],
      "job_skills": [
        "aws",
        "ci/cd",
        "cloud platforms",
        "collaboration",
        "communication",
        "docker",
        "frontend development",
        "git",
        "gitlab",
        "java",
        "kubernetes",
        "mysql",
        "node.js",
        "react",
        "ruby",
        "ruby on rails",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks a primary location in the Baltics and does not demonstrate full senior‑level experience with AWS, enterprise authentication and robust background processing—all core to the role’s day‑to‑day responsibilities.",
      "llm_recommendations": [
        "Gain hands‑on AWS experience (e.g., Lambda, RDS, SQS) through small projects or an AWS Certified Solutions Architect course.",
        "Build expertise in secure, regulated application development, including authentication, authorization, and data encryption best practices.",
        "Highlight or obtain experience with background job frameworks like Sidekiq, Resque, or DelayedJob in Ruby.",
        "Obtain knowledge of JRuby or demonstrate ability to integrate Ruby with Java components.",
        "If serious about this role, consider relocating to the Baltic region or securing a local office/remote arrangement that matches the company’s location preference."
      ],
      "linkedin_keywords": [
        "Ruby on Rails",
        "SQL",
        "Docker",
        "Kubernetes",
        "AWS",
        "CI/CD",
        "background processing",
        "secure coding",
        "enterprise authentication",
        "JRuby"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Jordan_Thompson_20251129_064621.pdf",
      "job_rank": 1,
      "job_title": "Data Engineer AWS - Hybrid in NY",
      "job_company": "TechTriad",
      "job_id": "4317962527",
      "skill_score": 0.14705882352941177,
      "semantic_score": 0.765633326953652,
      "topic_score": 0.8340575098991394,
      "final_score": 0.8292799157166316,
      "resume_skills_count": 22,
      "job_skills_count": 17,
      "matching_skills_count": 5,
      "resume_text_length": 1906,
      "resume_skills": [
        "a/b testing",
        "aws",
        "aws glue",
        "azure",
        "data pipeline",
        "data visualization",
        "deep learning",
        "etl",
        "feature engineering",
        "feature engineering pipelines",
        "git",
        "hadoop",
        "leadership",
        "machine learning",
        "power bi",
        "python",
        "r",
        "reporting",
        "spark",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "airflow",
        "aws",
        "ci/cd",
        "data pipeline",
        "databricks",
        "distributed systems",
        "feature store",
        "java",
        "kubernetes",
        "lambda",
        "machine learning",
        "mlflow",
        "pyspark",
        "python",
        "s3",
        "scala",
        "sql"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "Jordan has extensive data engineering experience with Spark, Python, and AWS Glue, plus 13 years of relevant work, indicating he could develop and optimize the required pipelines and feature store systems.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Jun_Li_20251129_064843.pdf",
      "job_rank": 1,
      "job_title": "Machine Learning Engineer with verification",
      "job_company": "AmeriHealth Caritas",
      "job_id": "4332716222",
      "skill_score": 0.14705882352941177,
      "semantic_score": 0.7048678852420878,
      "topic_score": 0.7931507229804993,
      "final_score": 0.7859197005655151,
      "resume_skills_count": 27,
      "job_skills_count": 12,
      "matching_skills_count": 5,
      "resume_text_length": 2576,
      "resume_skills": [
        "a/b testing",
        "aws",
        "azure",
        "azure ml",
        "customer segmentation",
        "data analysis",
        "data pipeline",
        "decision making",
        "deep learning",
        "etl",
        "forecasting",
        "gradient boosting",
        "hadoop",
        "leadership",
        "logistic regression",
        "machine learning",
        "nlp",
        "product analytics",
        "python",
        "r",
        "reporting",
        "sales forecasting",
        "scala",
        "spark",
        "sql",
        "sql server",
        "tableau"
      ],
      "job_skills": [
        "azure",
        "azure databricks",
        "azure ml",
        "ci/cd",
        "databricks",
        "feature store",
        "machine learning",
        "mlflow",
        "pyspark",
        "python",
        "software engineering",
        "sql"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate has extensive Python, SQL and Spark experience, has implemented cloud‑native ML workflows on Azure, and a strong data‑science background that will enable rapid learning of Azure Databricks, MLFlow, Feature Store and associated CI/CD tooling.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Justin-Tidrow-Resume-2025.pdf",
      "job_rank": 1,
      "job_title": "Graphic Designer with verification",
      "job_company": "Apex Tool Group, LLC",
      "job_id": "4332712686",
      "skill_score": 0.0,
      "semantic_score": 0.6408753395080566,
      "topic_score": 0.7567789554595947,
      "final_score": 0.6988271474838257,
      "resume_skills_count": 8,
      "job_skills_count": 3,
      "matching_skills_count": 0,
      "resume_text_length": 2016,
      "resume_skills": [
        "bigcommerce",
        "email marketing",
        "magento",
        "project management",
        "salesforce",
        "seo",
        "shopify",
        "woocommerce"
      ],
      "job_skills": [
        "attention to detail",
        "brand strategy",
        "excel"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks demonstrable experience with packaging design, die lines, and print production processes—critical core requirements for the role.",
      "llm_recommendations": [
        "Undertake a short course or project focused on product packaging design, including die-line creation and 3‑D mock‑ups.",
        "Gain hands‑on experience with CMYK/spot colour workflows and print‑ready file preparation (e.g., PDFs, layered PSDs).",
        "Highlight any previous print collateral work (brochures, flyers) and note transferable skills such as typography, layout, and client communication.",
        "Pursue relevant certifications (e.g., Adobe Certified Expert in InDesign/Illustrator) to validate print design proficiency.",
        "Include concrete examples of packaging or product launch collateral design in the portfolio or resume to demonstrate capability."
      ],
      "linkedin_keywords": [
        "packaging design",
        "die line",
        "CMYK",
        "spot color",
        "print production",
        "Adobe InDesign",
        "Adobe Illustrator",
        "product packaging",
        "brand collateral",
        "print-ready files"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Kas Kiatsukasem Resume.pdf",
      "job_rank": 1,
      "job_title": "Sales Data Analyst Intern Summer 2026 with verification",
      "job_company": "Westinghouse Electric Company",
      "job_id": "4319105940",
      "skill_score": 0.0,
      "semantic_score": 0.6158218109342198,
      "topic_score": 0.6612558960914612,
      "final_score": 0.6385388535128405,
      "resume_skills_count": 12,
      "job_skills_count": 3,
      "matching_skills_count": 0,
      "resume_text_length": 4786,
      "resume_skills": [
        "fp&a",
        "go",
        "go-to-market",
        "gong",
        "hubspot",
        "nlp",
        "outreach",
        "revops",
        "salesforce",
        "sql",
        "tableau",
        "zoominfo"
      ],
      "job_skills": [
        "pipeline management",
        "sap",
        "teamwork"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate’s extensive data‑analysis skills, CRM pipeline experience, SQL and Tableau proficiency, and proven ability to build dashboards and automate sales processes give them the transferable capabilities to perform the core tasks of the Sales Data Analyst internship.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Keita_Tanaka_20251129_065145.pdf",
      "job_rank": 1,
      "job_title": "DevSecOps",
      "job_company": "QualiZeal",
      "job_id": "4332406515",
      "skill_score": 0.3333333333333333,
      "semantic_score": 0.7549492266804454,
      "topic_score": 0.6496957540512085,
      "final_score": 0.8015483269105513,
      "resume_skills_count": 18,
      "job_skills_count": 18,
      "matching_skills_count": 9,
      "resume_text_length": 1755,
      "resume_skills": [
        "aws",
        "azure",
        "ci/cd",
        "cloud platforms",
        "customer segmentation",
        "data ingestion",
        "docker",
        "etl",
        "gcp",
        "git",
        "github",
        "github actions",
        "kubernetes",
        "mlflow",
        "python",
        "rest apis",
        "sagemaker",
        "sql"
      ],
      "job_skills": [
        "ansible",
        "aws",
        "azure",
        "bash",
        "bitbucket",
        "ci/cd",
        "cypress",
        "docker",
        "git",
        "github",
        "github actions",
        "gitlab",
        "jenkins",
        "junit",
        "kubernetes",
        "pytest",
        "python",
        "terraform"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks key infrastructure automation (Terraform/Ansible) and security scanning (Trivy, Snyk, OWASP ZAP) skills that are essential for a DevSecOps role.",
      "llm_recommendations": [
        "Undertake hands‑on projects or certifications in Terraform and Ansible to demonstrate infrastructure‑as‑code proficiency.",
        "Gain experience with container security tools such as Trivy or Snyk and OWASP ZAP, documenting results in a portfolio or GitHub repo.",
        "Highlight existing CI/CD, Docker, Kubernetes, Git, Prometheus/Grafana, and cloud (AWS/Azure) experience in the resume and LinkedIn profile to showcase transferable skills.",
        "Build a small end‑to‑end demo pipeline that includes CI/CD, containerization, Terraform provisioning, and security scanning.",
        "Obtain a relevant certification (e.g., AWS Certified DevOps Engineer, Certified Kubernetes Administrator) to validate competency."
      ],
      "linkedin_keywords": [
        "DevOps Engineer",
        "CI/CD",
        "Kubernetes",
        "Docker",
        "Helm",
        "Terraform",
        "Ansible",
        "Prometheus",
        "Grafana",
        "AWS",
        "Azure",
        "Trivy",
        "Snyk",
        "OWASP ZAP",
        "GitOps",
        "ArgoCD",
        "Argo Workflows"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Keon_Lee_20251129_065330.pdf",
      "job_rank": 1,
      "job_title": "Head of Engineering",
      "job_company": "IENERGY ®",
      "job_id": "4318666102",
      "skill_score": 0.037037037037037035,
      "semantic_score": 0.5686518292815593,
      "topic_score": 0.3873540759086609,
      "final_score": 0.497336176573069,
      "resume_skills_count": 22,
      "job_skills_count": 6,
      "matching_skills_count": 1,
      "resume_text_length": 2463,
      "resume_skills": [
        "a/b testing",
        "agile",
        "api design",
        "c",
        "communication",
        "data pipeline",
        "deep learning",
        "docker",
        "etl",
        "git",
        "java",
        "kubernetes",
        "machine learning",
        "named entity recognition",
        "natural language processing",
        "nlp",
        "python",
        "scrum",
        "sentiment analysis",
        "sql",
        "translation",
        "unit testing"
      ],
      "job_skills": [
        "collaboration",
        "communication",
        "leadership",
        "project management",
        "reporting",
        "software development"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The résumé lacks critical core requirements—specifically documented MERN‑stack development experience and proven leadership of a sizable engineering team—making it unlikely to meet the day‑to‑day needs of a Head of Engineering role.",
      "llm_recommendations": [
        "Obtain hands‑on experience with MERN stack technologies (MongoDB, Express, React, Node.js) through personal or open‑source projects.",
        "Seek a role that involves managing and scaling engineering teams, even if in a senior developer capacity, to build documented team‑leadership experience.",
        "Highlight transferable project‑management and mentorship responsibilities explicitly to show ownership and decision‑making abilities.",
        "Pursue certifications or courses in full‑stack web development (e.g., AWS, Microsoft Azure, or Google Cloud) that cover MERN technologies.",
        "Emphasize past cross‑functional collaboration and agile delivery experience, tying them to engineering leadership outcomes."
      ],
      "linkedin_keywords": [
        "Head of Engineering",
        "Engineering Manager",
        "Software Development Manager",
        "MERN Stack",
        "Full‑Stack Development",
        "Team Leadership",
        "Project Management",
        "Agile Scrum",
        "JavaScript",
        "Node.js",
        "React",
        "MongoDB",
        "Senior Software Engineer",
        "Engineering Lead."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Kharee_Smith.pdf",
      "job_rank": 1,
      "job_title": "Software Engineer - AI with verification",
      "job_company": "Newfold Digital",
      "job_id": "4331327644",
      "skill_score": 0.08,
      "semantic_score": 0.6745429039001465,
      "topic_score": 0.6500925421714783,
      "final_score": 0.6893323051929474,
      "resume_skills_count": 12,
      "job_skills_count": 15,
      "matching_skills_count": 2,
      "resume_text_length": 3363,
      "resume_skills": [
        "azure",
        "cloud platforms",
        "collaboration",
        "conflict resolution",
        "github",
        "inventory management",
        "operational efficiency",
        "problem solving",
        "process improvement",
        "project management",
        "r",
        "salesforce"
      ],
      "job_skills": [
        "api design",
        "azure",
        "bitbucket",
        "c",
        "ci/cd",
        "communication",
        "docker",
        "fastapi",
        "github",
        "github actions",
        "jenkins",
        "leadership",
        "mentoring",
        "python",
        "roadmap planning"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks the core backend engineering, LLM orchestration, and large‑scale API architecture experience required for this role.",
      "llm_recommendations": [
        "Undertake hands‑on projects or a bootcamp focused on Python FastAPI and .NET C# to build and launch RESTful APIs.",
        "Build a simple Retrieval‑Augmented Generation pipeline using a vector store (e.g., Chroma or pgvector) to demonstrate end‑to‑end RAG capabilities.",
        "Gain practical experience with Docker, CI/CD pipelines, and IaC (Terraform or Azure Bicep) by contributing to open‑source or internal dev‑ops projects.",
        "Deepen PostgreSQL expertise—practice schema design, indexing, and Alembic migrations at scale.",
        "Highlight any API or backend work in your portfolio, even if it was part of a smaller project, to show familiarity with micro‑service patterns.",
        "Pursue certifications or training in relevant AI frameworks (e.g., Semantic Kernel, LangChain) to showcase knowledge of LLM orchestration."
      ],
      "linkedin_keywords": [
        "Python FastAPI",
        ".NET C#",
        "REST API",
        "LLM",
        "Semantic Kernel",
        "RAG",
        "PostgreSQL",
        "Docker",
        "CI/CD",
        "Azure",
        "AI Ops"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Kibaek Resume - Nov 2025 2P.pdf",
      "job_rank": 1,
      "job_title": "AI Platform Engineer with verification",
      "job_company": "XpertDirect",
      "job_id": "4318466539",
      "skill_score": 0.1388888888888889,
      "semantic_score": 0.7402925800928378,
      "topic_score": 0.8025380969047546,
      "final_score": 0.8031632081517412,
      "resume_skills_count": 28,
      "job_skills_count": 13,
      "matching_skills_count": 5,
      "resume_text_length": 3886,
      "resume_skills": [
        "aws",
        "business intelligence",
        "c",
        "ci/cd",
        "cloud infrastructure",
        "docker",
        "fastapi",
        "flask",
        "git",
        "github",
        "github actions",
        "hubspot",
        "java",
        "javascript",
        "jenkins",
        "leadership",
        "mentoring",
        "netsuite",
        "nlp",
        "node.js",
        "pca",
        "php",
        "postgresql",
        "python",
        "react",
        "redshift",
        "typescript",
        "xgboost"
      ],
      "job_skills": [
        "airflow",
        "aws",
        "ci/cd",
        "distributed systems",
        "docker",
        "fastapi",
        "gcp",
        "kubeflow",
        "kubernetes",
        "microservices",
        "mlflow",
        "python",
        "terraform"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks explicit experience with Kubernetes, Terraform, and AWS EKS—critical for architecting and scaling the ML platform required for this role.",
      "llm_recommendations": [
        "Acquire hands‑on experience with Kubernetes (minikube, EKS) through small projects or labs.",
        "Complete a Terraform certification or build a sample infrastructure as code project in AWS.",
        "Gain familiarity with MLOps tools such as MLflow and Airflow; contribute to an open‑source project or set up CI/CD pipelines that incorporate these tools.",
        "Highlight transferable skills (containerization, Python, FastAPI, CI/CD pipelines) and emphasize rapid learning capability on your résumé.",
        "Seek mentorship or internal training on cloud‑native ML frameworks like Kubeflow and GPU cluster management."
      ],
      "linkedin_keywords": [
        "Kubernetes",
        "Terraform",
        "AWS EKS",
        "MLOps",
        "CI/CD",
        "Docker",
        "Python",
        "FastAPI",
        "Cloud Infrastructure",
        "MLflow",
        "Airflow",
        "Kubeflow",
        "DevOps",
        "AI platform engineering",
        "Kubernetes cluster management."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Kushagra_s Resume.pdf",
      "job_rank": 1,
      "job_title": "Software Architect with verification",
      "job_company": "Allwyn Lottery Solutions",
      "job_id": "4332707295",
      "skill_score": 0.18181818181818182,
      "semantic_score": 0.3967685103416443,
      "topic_score": 0.5272516012191772,
      "final_score": 0.5598264092748816,
      "resume_skills_count": 23,
      "job_skills_count": 16,
      "matching_skills_count": 6,
      "resume_text_length": 5651,
      "resume_skills": [
        "agile",
        "android development",
        "aws",
        "backend development",
        "ci/cd",
        "docker",
        "ec2",
        "git",
        "github",
        "github actions",
        "java",
        "jenkins",
        "jira",
        "kotlin",
        "kubernetes",
        "lambda",
        "microservices",
        "mysql",
        "postgresql",
        "project management",
        "s3",
        "sap",
        "sql"
      ],
      "job_skills": [
        "account management",
        "apollo",
        "aws",
        "azure",
        "communication",
        "docker",
        "java",
        "junit",
        "kotlin",
        "kubernetes",
        "mentoring",
        "postgresql",
        "rest apis",
        "scala",
        "software development",
        "software engineering"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate does not yet meet the core requirement of 5+ years as a senior Java developer/architect, nor does they have specific experience in the lottery or iGaming domain or proven large‑scale production architecture roles needed for an architect position.",
      "llm_recommendations": [
        "Gain additional years of senior development experience, focusing on large‑scale, production‑grade Java/Spring applications and architecture responsibilities.",
        "Pursue projects or roles that specifically involve designing and maintaining systems with high concurrency, distributed computing, and real‑time data processing.",
        "Acquire or highlight experience in the iGaming or lottery space, such as working with payment, player account, or live‑gaming infrastructures.",
        "Obtain certifications or training in enterprise architecture frameworks (e.g., TOGAF, Microservices Design Patterns) and advanced cloud architecture (AWS/Azure).",
        "Showcase portfolio or case studies that demonstrate end‑to‑end architecture design, performance tuning, and security hardening in complex, multi‑service environments."
      ],
      "linkedin_keywords": [
        "Java Architecture",
        "Senior Java Developer",
        "Spring Boot",
        "Microservices Design",
        "Distributed Systems",
        "Enterprise Architecture",
        "Cloud Architecture",
        "AWS",
        "Docker",
        "Kubernetes",
        "PostgreSQL",
        "iGaming",
        "Lottery Systems",
        "REST API Design",
        "CI/CD",
        "Automated Testing",
        "Security Architecture",
        "Concurrency",
        "Performance Engineering."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Kyson-Xu-Senior-Marketing-Manager-Resume-2025-.pdf",
      "job_rank": 1,
      "job_title": "Marketing Manager",
      "job_company": "Hangry",
      "job_id": "4319148231",
      "skill_score": 0.1875,
      "semantic_score": 0.6295280268929022,
      "topic_score": 0.8537920117378235,
      "final_score": 0.7900987656937323,
      "resume_skills_count": 12,
      "job_skills_count": 7,
      "matching_skills_count": 3,
      "resume_text_length": 9672,
      "resume_skills": [
        "c",
        "collaboration",
        "communication",
        "go",
        "go-to-market",
        "hubspot",
        "leadership",
        "mailchimp",
        "salesforce",
        "sem",
        "seo",
        "sql"
      ],
      "job_skills": [
        "agile",
        "collaboration",
        "communication",
        "cross-functional collaboration",
        "leadership",
        "stakeholder management",
        "teamwork"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The resume demonstrates more than five years of senior marketing experience, including managerial responsibilities, cross‑functional collaboration, data‑driven strategic planning, budget stewardship, and proven results across multiple brands, meeting the core requirements for the Marketing Manager role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Lee_lab_associate.pdf",
      "job_rank": 1,
      "job_title": "Software Engineer, Data Integration",
      "job_company": "Chan Zuckerberg Biohub Network",
      "job_id": "4311154172",
      "skill_score": 0.0,
      "semantic_score": 0.29689151607650766,
      "topic_score": 0.39871954917907715,
      "final_score": 0.3478055326277924,
      "resume_skills_count": 3,
      "job_skills_count": 9,
      "matching_skills_count": 0,
      "resume_text_length": 2828,
      "resume_skills": [
        "excel",
        "mentoring",
        "sas"
      ],
      "job_skills": [
        "communication",
        "data integration",
        "javascript",
        "next.js",
        "problem solving",
        "python",
        "react",
        "software development",
        "software engineering"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks critical technical skills such as programming proficiency (Python/JavaScript) and database design required for the role.",
      "llm_recommendations": [
        "Acquire hands‑on experience in Python and JavaScript through online courses or coding boot camps focused on data handling and web development.",
        "Build a portfolio of small projects (e.g., a simple data tracking web app using React) to demonstrate front‑end framework skills and database design.",
        "Gain practical exposure to scientific image formats (TIFF, Zarr) or data pipelines, ideally via collaborations with biology labs or open‑source imaging projects.",
        "Highlight any transferable analytical or data organization experience (e.g., Excel/Access databases) in the résumé to showcase data management aptitude.",
        "Pursue a CS or software engineering certification (e.g., Google’s Associate Android Developer, AWS Cloud Practitioner) to validate foundational knowledge."
      ],
      "linkedin_keywords": [
        "Python",
        "JavaScript",
        "React",
        "Next.js",
        "database design",
        "data integration",
        "bioinformatics",
        "scientific imaging",
        "data pipelines",
        "software engineering."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Leena_Patel_20251129_055926.pdf",
      "job_rank": 1,
      "job_title": "Sr Data Analyst with verification",
      "job_company": "Horizontal Talent",
      "job_id": "4318091470",
      "skill_score": 0.23809523809523808,
      "semantic_score": 0.6808016515733792,
      "topic_score": 0.8993420004844666,
      "final_score": 0.8400547245934651,
      "resume_skills_count": 14,
      "job_skills_count": 12,
      "matching_skills_count": 5,
      "resume_text_length": 1770,
      "resume_skills": [
        "a/b testing",
        "bigquery",
        "data pipeline",
        "data visualization",
        "etl",
        "git",
        "leadership",
        "power bi",
        "python",
        "reporting",
        "snowflake",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "azure",
        "azure data factory",
        "ci/cd",
        "data pipeline",
        "etl",
        "oracle",
        "python",
        "root cause analysis",
        "shell scripting",
        "snowflake",
        "sql",
        "sql server"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate has five years of data engineering experience with Snowflake, ETL pipeline design, data modeling, and Python scripting—skills directly transferable to building and managing Azure Data Factory/SSIS pipelines in the role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Leila_Nguyen_20251129_062022.pdf",
      "job_rank": 1,
      "job_title": "SQL Developer - Remote / Telecommute",
      "job_company": "ExecutivePlacements.com",
      "job_id": "4332021588",
      "skill_score": 0.05555555555555555,
      "semantic_score": 0.45379496973141964,
      "topic_score": 0.711820125579834,
      "final_score": 0.6059849061192031,
      "resume_skills_count": 18,
      "job_skills_count": 1,
      "matching_skills_count": 1,
      "resume_text_length": 2759,
      "resume_skills": [
        "agile",
        "data pipeline",
        "data visualization",
        "decision making",
        "etl",
        "forecasting",
        "machine learning",
        "outreach",
        "power bi",
        "project management",
        "python",
        "r",
        "reporting",
        "sales forecasting",
        "snowflake",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "sql"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate demonstrates strong SQL, data modeling, ETL, and business process development experience that aligns with the core requirements of the SQL Developer role, and her proven ability to learn new technologies suggests she can quickly master PL/SQL and related responsibilities.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Lena_Müller_20251129_063235.pdf",
      "job_rank": 1,
      "job_title": "ML/AI Engineer with verification",
      "job_company": "Munich Re",
      "job_id": "4310483254",
      "skill_score": 0.17142857142857143,
      "semantic_score": 0.6766263851487412,
      "topic_score": 0.7771725058555603,
      "final_score": 0.7737166834160677,
      "resume_skills_count": 18,
      "job_skills_count": 23,
      "matching_skills_count": 6,
      "resume_text_length": 1714,
      "resume_skills": [
        "airflow",
        "aws",
        "data analytics",
        "data pipeline",
        "data visualization",
        "docker",
        "feature engineering",
        "forecasting",
        "git",
        "github",
        "machine learning",
        "python",
        "r",
        "reporting",
        "sales forecasting",
        "spark",
        "sql",
        "statistical analysis"
      ],
      "job_skills": [
        "azure",
        "azure databricks",
        "ci/cd",
        "collaboration",
        "data analysis",
        "databricks",
        "docker",
        "dvc",
        "elt",
        "etl",
        "feature engineering",
        "feature engineering pipelines",
        "feature store",
        "git",
        "leadership",
        "machine learning",
        "mlflow",
        "numpy",
        "pandas",
        "python",
        "software development",
        "software engineering",
        "spark"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate shows strong Python, ML, feature engineering, and pipeline experience, but lacks the critical Databricks, Azure Data Lake, and Azure AI skill set required for this role.",
      "llm_recommendations": [
        "Acquire hands‑on experience with Azure Databricks and related Azure data services through a short course or project.",
        "Integrate MLFlow, DVC, and dbt into a personal or volunteer data pipeline to demonstrate proficiency with feature store tooling.",
        "Showcase Docker‑based inference endpoint deployments and CI/CD skills by adding an example workflow to a public repo (e.g., GitHub Actions with Azure DevOps).",
        "Highlight any informal leadership or mentorship roles in projects to address the leadership expectation.",
        "Include certifications such as Microsoft Certified: Azure Data Engineer Associate or Databricks Certified Data Engineer."
      ],
      "linkedin_keywords": [
        "Databricks",
        "Azure Data Lake",
        "Azure AI Search",
        "MLFlow",
        "DVC",
        "dbt",
        "Spark",
        "Feature Engineering",
        "CI/CD",
        "DevOps."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Liam_Chen_20251129_062105.pdf",
      "job_rank": 1,
      "job_title": "AI Software Engineer with verification",
      "job_company": "XpertDirect",
      "job_id": "4318464252",
      "skill_score": 0.2916666666666667,
      "semantic_score": 0.7463685051689027,
      "topic_score": 0.7837648987770081,
      "final_score": 0.83358891389751,
      "resume_skills_count": 20,
      "job_skills_count": 11,
      "matching_skills_count": 7,
      "resume_text_length": 1981,
      "resume_skills": [
        "a/b testing",
        "aws",
        "ci/cd",
        "data visualization",
        "decision trees",
        "docker",
        "feature engineering",
        "hadoop",
        "kubernetes",
        "linear regression",
        "machine learning",
        "numpy",
        "pandas",
        "pyspark",
        "python",
        "r",
        "sagemaker",
        "spark",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "aws",
        "aws lambda",
        "docker",
        "fastapi",
        "forecasting",
        "lambda",
        "machine learning",
        "pandas",
        "python",
        "sagemaker",
        "sql"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate possesses the core technical skills (Python, TensorFlow/PyTorch, SQL, AWS SageMaker, Docker) and 7+ years of ML experience, along with proven MLOps and large‑scale data pipeline expertise that make them well‑qualified to develop and deploy the predictive models required for this energy‑optimization role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Lin_Wei_Chen_20251129_065230.pdf",
      "job_rank": 1,
      "job_title": "Data Scientist, Watchlist",
      "job_company": "RemoteHunter",
      "job_id": "4319177929",
      "skill_score": 0.2857142857142857,
      "semantic_score": 0.6451112454997054,
      "topic_score": 0.9192549586296082,
      "final_score": 0.8444165014747549,
      "resume_skills_count": 16,
      "job_skills_count": 11,
      "matching_skills_count": 6,
      "resume_text_length": 2523,
      "resume_skills": [
        "a/b testing",
        "data analysis",
        "data visualization",
        "deep learning",
        "docker",
        "etl",
        "hadoop",
        "kubernetes",
        "leadership",
        "machine learning",
        "python",
        "r",
        "spark",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "adaptability",
        "aws",
        "databricks",
        "feature engineering",
        "hadoop",
        "machine learning",
        "python",
        "r",
        "spark",
        "sql",
        "xgboost"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate possesses all essential technical skills (Python, R, SQL, scikit‑learn, TensorFlow, Spark/Hadoop), extensive experience with large‑scale data processing and machine learning pipelines in risk scoring, and a proven track record of translating business problems into data‑driven solutions and communicating findings.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Lin_Zhou_20251129_063736.pdf",
      "job_rank": 1,
      "job_title": "AI/ML Software Engineer with verification",
      "job_company": "Guidehouse",
      "job_id": "4332300185",
      "skill_score": 0.22580645161290322,
      "semantic_score": 0.6601328062852851,
      "topic_score": 0.590946614742279,
      "final_score": 0.7100952597526055,
      "resume_skills_count": 21,
      "job_skills_count": 17,
      "matching_skills_count": 7,
      "resume_text_length": 1864,
      "resume_skills": [
        "agile",
        "aws",
        "data analysis",
        "data cleaning",
        "data visualization",
        "docker",
        "feature engineering",
        "feature engineering pipelines",
        "git",
        "javascript",
        "machine learning",
        "node.js",
        "python",
        "react",
        "reporting",
        "sagemaker",
        "scrum",
        "software development",
        "software engineering",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "aws",
        "communication",
        "data pipeline",
        "deep learning",
        "javascript",
        "machine learning",
        "numpy",
        "oracle",
        "outreach",
        "pandas",
        "postgresql",
        "python",
        "rest apis",
        "software development",
        "software engineering",
        "sql",
        "typescript"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate meets the basic 5‑year Python and JavaScript experience and has general ML and cloud experience, but lacks the required concrete experience building Retrieval‑Augmented Generation (RAG) systems, chatbots, and associated frameworks (e.g., LangChain, Haystack).",
      "llm_recommendations": [
        "Develop hands‑on projects or coursework that build RAG pipelines and implement re‑ranking strategies.",
        "Build a simple AI chatbot or conversational agent using popular frameworks (LangChain, crewAI) to demonstrate ability in that area.",
        "Gain familiarity with search technologies like Elasticsearch/OpenSearch and integrate them into ML pipelines.",
        "Strengthen knowledge of core Python data science libraries (NumPy, Pandas) and deep‑learning frameworks (PyTorch/TensorFlow) with applied projects.",
        "Pursue a federal Public Trust clearance or obtain a similar credential to meet the security requirement."
      ],
      "linkedin_keywords": [
        "AI Software Engineer",
        "Machine Learning Engineer",
        "Retrieval‑Augmented Generation",
        "RAG Engineer",
        "LangChain",
        "PyTorch",
        "TensorFlow",
        "AWS SageMaker",
        "AWS Lambda",
        "REST API",
        "JavaScript",
        "Node.js",
        "Data Pipeline",
        "Scikit‑learn",
        "NumPy",
        "Pandas",
        "Elasticsearch",
        "PostgreSQL",
        "Cloud ML Engineer."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Linh_Nguyen_20251129_061818.pdf",
      "job_rank": 1,
      "job_title": "Machine Learning Engineer",
      "job_company": "Sepal AI",
      "job_id": "4319216770",
      "skill_score": 0.17391304347826086,
      "semantic_score": 0.6449410895950602,
      "topic_score": 0.706609308719635,
      "final_score": 0.7321621210430262,
      "resume_skills_count": 20,
      "job_skills_count": 7,
      "matching_skills_count": 4,
      "resume_text_length": 2221,
      "resume_skills": [
        "aws",
        "ci/cd",
        "data analysis",
        "data visualization",
        "deep learning",
        "docker",
        "feature engineering",
        "forecasting",
        "gcp",
        "hadoop",
        "machine learning",
        "numpy",
        "pandas",
        "power bi",
        "python",
        "r",
        "sagemaker",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "attention to detail",
        "data ingestion",
        "forecasting",
        "machine learning",
        "pandas",
        "python",
        "xgboost"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate demonstrates 6 years of enterprise ML engineering, solid Python expertise with pandas and scikit‑learn, experience in time‑series analytics, predictive modeling (churn/revenue forecasting), and full‑cycle pipeline development—all of which satisfy the core requirements for evaluating and building predictive tasks at Sepal AI.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Luca_Moretti_20251129_064336.pdf",
      "job_rank": 1,
      "job_title": "Data Analyst",
      "job_company": "Helic & Co.",
      "job_id": "4332495253",
      "skill_score": 0.4375,
      "semantic_score": 0.6300830465505702,
      "topic_score": 0.9276444911956787,
      "final_score": 0.8756108699911325,
      "resume_skills_count": 11,
      "job_skills_count": 12,
      "matching_skills_count": 7,
      "resume_text_length": 1746,
      "resume_skills": [
        "data visualization",
        "etl",
        "excel",
        "forecasting",
        "power bi",
        "python",
        "r",
        "reporting",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "communication",
        "critical thinking",
        "data analysis",
        "data analytics",
        "data visualization",
        "excel",
        "looker",
        "power bi",
        "python",
        "r",
        "sql",
        "tableau"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate possesses all essential technical skills (SQL, Python, Tableau/Power BI), has a relevant bachelor’s degree, and demonstrates 6 years of experience in data collection, cleaning, analysis, dashboard creation and stakeholder communication, making him fully capable of performing the core responsibilities of this Data Analyst role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Lukas_Weber_20251129_060558.pdf",
      "job_rank": 1,
      "job_title": "Sr. Data Engineer",
      "job_company": "RemoteHunter",
      "job_id": "4319158781",
      "skill_score": 0.16129032258064516,
      "semantic_score": 0.7180125377193189,
      "topic_score": 0.8142316937446594,
      "final_score": 0.8038443551300555,
      "resume_skills_count": 22,
      "job_skills_count": 14,
      "matching_skills_count": 5,
      "resume_text_length": 2594,
      "resume_skills": [
        "agile",
        "airflow",
        "aws",
        "data pipeline",
        "data visualization",
        "decision making",
        "docker",
        "etl",
        "excel",
        "gcp",
        "git",
        "hadoop",
        "leadership",
        "machine learning",
        "power bi",
        "python",
        "r",
        "reporting",
        "spark",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "airflow",
        "athena",
        "ci/cd",
        "communication",
        "data pipeline",
        "dynamodb",
        "elt",
        "etl",
        "java",
        "python",
        "risk management",
        "sql",
        "teamwork",
        "terraform"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks essential proficiency with the core data infrastructure technologies (ClickHouse, Postgres, Athena, Kafka) and real‑time streaming required for the role, making basic job performance unlikely.",
      "llm_recommendations": [
        "Gain hands‑on experience with ClickHouse, PostgreSQL, and Athena through self‑study projects or paid training courses.",
        "Build a small real‑time pipeline using Kafka combined with a time‑series store (e.g., ClickHouse) to demonstrate competency in streaming data architectures.",
        "Highlight existing skills in large‑scale ETL, Airflow orchestration, and cloud platforms (AWS/GCP) while explicitly stating the ability to quickly master the organization’s specific stack.",
        "Obtain relevant certifications (e.g., AWS Certified Data Analytics, Certified Airflow Professional) to validate advanced data engineering knowledge.",
        "Emphasize transferability of Hadoop/Spark and data modeling experience when positioning for similar roles."
      ],
      "linkedin_keywords": [
        "Data Engineer",
        "ClickHouse",
        "Athena",
        "PostgreSQL",
        "Kafka",
        "Airflow",
        "ETL",
        "Spark",
        "Python",
        "SQL",
        "Big Data",
        "Streaming",
        "Real‑time Analytics",
        "Cloud Data Engineering."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Léa_Dubois_20251129_062337.pdf",
      "job_rank": 1,
      "job_title": "People Analytics Lead with verification",
      "job_company": "Deloitte",
      "job_id": "4331386846",
      "skill_score": 0.11764705882352941,
      "semantic_score": 0.5802498209625386,
      "topic_score": 0.666553258895874,
      "final_score": 0.6677072411140056,
      "resume_skills_count": 10,
      "job_skills_count": 9,
      "matching_skills_count": 2,
      "resume_text_length": 1668,
      "resume_skills": [
        "collaboration",
        "data analysis",
        "data visualization",
        "docker",
        "feature engineering",
        "kubernetes",
        "machine learning",
        "python",
        "r",
        "sql"
      ],
      "job_skills": [
        "data transformation",
        "data visualization",
        "decision making",
        "excel",
        "leadership",
        "people analytics",
        "reporting",
        "sql",
        "tableau"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks the essential Tableau experience (at least 3 years with Tableau Desktop and related tools) required for the role, along with demonstrated proficiency in Microsoft Office (Excel, PowerPoint) for high‑quality reporting and storytelling.",
      "llm_recommendations": [
        "Complete a Tableau Desktop certification and build a portfolio of dashboards to demonstrate hands‑on experience.",
        "Highlight any Excel‑based reporting, PowerPoint presentations, and data storytelling skills in existing projects to show transferable visualization expertise.",
        "Seek out or develop projects involving HR or talent data (e.g., workforce analytics) to align with the job’s domain focus."
      ],
      "linkedin_keywords": [
        "Tableau",
        "People Analytics",
        "HR Analytics",
        "Data Visualization",
        "Power BI",
        "Excel",
        "PowerPoint",
        "Dashboard Design",
        "Data Storytelling",
        "Talent Analytics"
      ],
      "llm_error": null
    },
    {
      "resume_file": "MARC_CHEN.pdf",
      "job_rank": 1,
      "job_title": "Digital Growth & AEO Specialist (Contract-to-Hire) with verification",
      "job_company": "AdOmni",
      "job_id": "4316964338",
      "skill_score": 0.15384615384615385,
      "semantic_score": 0.588405514454514,
      "topic_score": 0.8785853981971741,
      "final_score": 0.7744961553526373,
      "resume_skills_count": 12,
      "job_skills_count": 3,
      "matching_skills_count": 2,
      "resume_text_length": 2383,
      "resume_skills": [
        "a/b testing",
        "digital marketing",
        "facebook ads",
        "google ads",
        "google analytics",
        "hubspot",
        "linkedin ads",
        "mailchimp",
        "mixpanel",
        "power bi",
        "ppc",
        "seo"
      ],
      "job_skills": [
        "client success",
        "digital marketing",
        "seo"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks the critical, hands‑on SEO schema/structured data expertise and AEO experience required for this specialized role, despite having sufficient overall digital marketing tenure.",
      "llm_recommendations": [
        "Complete a formal course or certification in Schema Markup and Structured Data to demonstrate hands‑on SEO skills.",
        "Pursue a short, intensive training in Answer Engine Optimization (AEO) to build foundational knowledge and confidence.",
        "Highlight any paid‑media and analytics achievements that can translate into performance marketing and AEO insights.",
        "Showcase transferable experience from B2C or mix‑of‑market campaigns to illustrate ability to adapt strategies for B2B audiences.",
        "Emphasize ongoing learning, such as recent certifications or side projects involving new search technologies."
      ],
      "linkedin_keywords": [
        "SEO",
        "schema markup",
        "structured data",
        "AI search optimization",
        "answer engine optimization",
        "AEO",
        "Google Search Console",
        "Google Analytics 4",
        "paid media",
        "demand generation",
        "B2B marketing",
        "digital marketing strategy",
        "performance marketing",
        "lead generation",
        "marketing automation."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Mara_Nganga_20251129_141031.pdf",
      "job_rank": 1,
      "job_title": "Frontend Developer with verification",
      "job_company": "IoThink Solutions",
      "job_id": "4278788470",
      "skill_score": 0.125,
      "semantic_score": 0.4267128745783695,
      "topic_score": 0.39972081780433655,
      "final_score": 0.4865647404174339,
      "resume_skills_count": 14,
      "job_skills_count": 13,
      "matching_skills_count": 3,
      "resume_text_length": 2136,
      "resume_skills": [
        "aws",
        "bert",
        "ci/cd",
        "data ingestion",
        "data pipeline",
        "docker",
        "embeddings",
        "git",
        "kubernetes",
        "natural language processing",
        "nlp",
        "python",
        "sagemaker",
        "spark"
      ],
      "job_skills": [
        "angular",
        "attention to detail",
        "c",
        "ci/cd",
        "collaboration",
        "communication",
        "docker",
        "frontend development",
        "javascript",
        "kubernetes",
        "problem-solving skills",
        "software engineering",
        "typescript"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks the core frontend skills—Angular/Blazor, C#/TypeScript/JavaScript, and web UI development—which are essential for the role.",
      "llm_recommendations": [
        "Complete a short online course or build a small project using Angular (or React) to gain hands‑on frontend experience.",
        "Learn C#, TypeScript, and JavaScript fundamentals, focusing on building client‑side applications and integrating with RESTful APIs.",
        "Showcase existing API, Docker, Kubernetes, and CI/CD skills while highlighting how they can transfer to frontend workflows.",
        "Attend a workshop or obtain a certification in modern web development (e.g., Microsoft’s Front-End Development or Angular certification).",
        "Update the résumé to emphasize any UI/UX or client‑side work, even if minimal, and express a strong willingness to learn frontend technologies."
      ],
      "linkedin_keywords": [
        "Frontend Developer",
        "Angular",
        "TypeScript",
        "C#",
        ".NET Blazor",
        "JavaScript",
        "WebSockets",
        "API Integration",
        "Docker",
        "Kubernetes",
        "CI/CD",
        "IoT Frontend",
        "Web Development",
        "React",
        "Frontend Engineer",
        "Node.js",
        "HTML",
        "CSS."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Mara_OConnor_20251129_141129.pdf",
      "job_rank": 1,
      "job_title": "Senior DevOps Engineer with verification",
      "job_company": "Munich Re",
      "job_id": "4311393243",
      "skill_score": 0.23529411764705882,
      "semantic_score": 0.7735930911777348,
      "topic_score": 0.6683363914489746,
      "final_score": 0.7866200962984476,
      "resume_skills_count": 23,
      "job_skills_count": 19,
      "matching_skills_count": 8,
      "resume_text_length": 2237,
      "resume_skills": [
        "agile",
        "airflow",
        "aws",
        "azure",
        "ci/cd",
        "data analytics",
        "data pipeline",
        "docker",
        "gcp",
        "gitlab",
        "gradient boosting",
        "jenkins",
        "kubeflow",
        "kubernetes",
        "power bi",
        "prototyping",
        "python",
        "r",
        "reporting",
        "scrum",
        "sql",
        "tableau",
        "terraform"
      ],
      "job_skills": [
        "aws",
        "azure",
        "ci/cd",
        "cloud infrastructure",
        "cloud native",
        "collaboration",
        "communication",
        "docker",
        "gcp",
        "github",
        "github actions",
        "incident response",
        "javascript",
        "kubernetes",
        "machine learning",
        "python",
        "software engineering",
        "terraform",
        "typescript"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate possesses the essential skills—Terraform, Kubernetes, CI/CD with ArgoCD/GitLab, multi-cloud (AWS, Azure, GCP), Python, and Prometheus/Grafana monitoring—along with eight years of relevant experience, indicating they can perform the core job functions.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Marcus_Chen_20251129_062636.pdf",
      "job_rank": 1,
      "job_title": "Senior Software Engineer",
      "job_company": "Delmar Nord",
      "job_id": "4261066094",
      "skill_score": 0.1,
      "semantic_score": 0.7113125748224614,
      "topic_score": 0.5154879093170166,
      "final_score": 0.652060217862765,
      "resume_skills_count": 23,
      "job_skills_count": 10,
      "matching_skills_count": 3,
      "resume_text_length": 2316,
      "resume_skills": [
        "agile",
        "airflow",
        "aws",
        "aws redshift",
        "bigquery",
        "cloud platforms",
        "data analysis",
        "data pipeline",
        "data visualization",
        "decision making",
        "etl",
        "gcp",
        "git",
        "java",
        "machine learning",
        "product analytics",
        "python",
        "redshift",
        "reporting",
        "software engineering",
        "spark",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "aws",
        "ci/cd",
        "cloud native",
        "communication",
        "data analytics",
        "git",
        "microservices",
        "problem-solving skills",
        "python",
        "terraform"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate has strong Python and AWS experience, has built web services, managed CI pipelines, and demonstrates a decade of engineering practice, providing transferable skills that indicate they can effectively design and build the required cloud-native microservices, with a learning curve for containerization and infrastructure‑as‑code.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Marcus_Thompson_20251129_061516.pdf",
      "job_rank": 1,
      "job_title": "Data Analytics Software Engineer (Trading)",
      "job_company": "Fintal Partners",
      "job_id": "4318639448",
      "skill_score": 0.2857142857142857,
      "semantic_score": 0.7139577269554341,
      "topic_score": 0.8066668510437012,
      "final_score": 0.8287944921425483,
      "resume_skills_count": 22,
      "job_skills_count": 14,
      "matching_skills_count": 8,
      "resume_text_length": 2341,
      "resume_skills": [
        "a/b testing",
        "aws",
        "data analysis",
        "data analytics",
        "data cleaning",
        "data pipeline",
        "data visualization",
        "decision making",
        "etl",
        "feature engineering",
        "leadership",
        "machine learning",
        "numpy",
        "pandas",
        "python",
        "r",
        "redshift",
        "reporting",
        "risk analysis",
        "s3",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "data analysis",
        "data analytics",
        "data pipeline",
        "databricks",
        "docker",
        "kubernetes",
        "machine learning",
        "numpy",
        "pandas",
        "python",
        "reporting",
        "scipy",
        "snowflake",
        "spark"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The résumé demonstrates strong Python, pandas, and pipeline skills, but lacks the essential distributed‑processing, real‑time streaming, and high‑frequency trading experience required for the core data‑engineering responsibilities of this role.",
      "llm_recommendations": [
        "Gain hands‑on experience with distributed data frameworks such as Spark, Dask, or Trino through personal projects or training courses.",
        "Build a portfolio project that ingests and processes high‑velocity, real‑time trading data to demonstrate low‑latency data pipeline skills.",
        "Acquire familiarity with data lake technologies (Databricks, Snowflake, Parquet, Iceberg) and showcase this in a GitHub repository or case study.",
        "Learn Linux and containerization (Docker, Kubernetes) by setting up micro‑service environments and publishing demos.",
        "Highlight any quantitative finance or finance‑related analytics experience more explicitly, including any real‑time data work you may have done."
      ],
      "linkedin_keywords": [
        "Data Engineering",
        "Big Data",
        "Apache Spark",
        "Dask",
        "Snowflake",
        "Databricks",
        "Python",
        "ETL",
        "Real-Time Analytics",
        "High-Frequency Trading",
        "Machine Learning",
        "Cloud Data Warehouse."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Marina_Alvarez_20251129_064217.pdf",
      "job_rank": 1,
      "job_title": "AI/Machine Learning (NLP) Engineer with verification",
      "job_company": "Aslase",
      "job_id": "4332070318",
      "skill_score": 0.15384615384615385,
      "semantic_score": 0.7048593197637216,
      "topic_score": 0.6826281547546387,
      "final_score": 0.7408600853731524,
      "resume_skills_count": 17,
      "job_skills_count": 13,
      "matching_skills_count": 4,
      "resume_text_length": 1675,
      "resume_skills": [
        "aws",
        "bert",
        "business intelligence",
        "data analysis",
        "docker",
        "feature engineering",
        "machine learning",
        "nlp",
        "numpy",
        "pandas",
        "python",
        "reporting",
        "sagemaker",
        "sentiment analysis",
        "sql",
        "tableau",
        "text classification"
      ],
      "job_skills": [
        "ci/cd",
        "data pipeline",
        "docker",
        "embeddings",
        "github",
        "kubernetes",
        "machine learning",
        "mlflow",
        "nlp",
        "prototyping",
        "python",
        "rest apis",
        "spark"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks several essential core skills for this role, particularly experience with LLM APIs, AI orchestration frameworks (LangChain/LlamaIndex), vector databases (Pinecone, Weaviate, FAISS), and prompt engineering/RAG pipelines, which are critical for the job’s core responsibilities.",
      "llm_recommendations": [
        "Build hands‑on projects using OpenAI/Anthropic APIs to fine‑tune LLMs and integrate them into inference pipelines.",
        "Gain familiarity with LangChain, LlamaIndex or crewAI by creating small agent‑based prototypes or contributing to open‑source examples.",
        "Set up and experiment with vector databases such as Pinecone or FAISS, including embeddings generation and RAG workflows.",
        "Acquire basic MLOps knowledge: Docker/K8s deployment, CI/CD with tools like GitHub Actions or Jenkins, and model monitoring (e.g., MLflow, Prometheus).",
        "Highlight transferable skills in the resume: Python, Docker, cloud deployment, and any previous experience with REST APIs, emphasizing the ability to learn and adapt quickly."
      ],
      "linkedin_keywords": [
        "LLM Integration",
        "LangChain",
        "LlamaIndex",
        "Pinecone",
        "Weaviate",
        "FAISS",
        "Prompt Engineering",
        "Retrieval Augmented Generation",
        "MLOps",
        "Docker",
        "Kubernetes",
        "CI/CD",
        "OpenAI API",
        "Anthropic",
        "Hugging Face Transformers",
        "NLP Engineer",
        "Machine Learning Engineer."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Marta_Kovács_20251129_061159.pdf",
      "job_rank": 1,
      "job_title": "Data Analytics Software Engineer (Trading)",
      "job_company": "Fintal Partners",
      "job_id": "4318639448",
      "skill_score": 0.2857142857142857,
      "semantic_score": 0.6757634878158761,
      "topic_score": 0.797086775302887,
      "final_score": 0.8117322368281297,
      "resume_skills_count": 13,
      "job_skills_count": 14,
      "matching_skills_count": 6,
      "resume_text_length": 2601,
      "resume_skills": [
        "airflow",
        "data analysis",
        "data pipeline",
        "elt",
        "etl",
        "power bi",
        "python",
        "reporting",
        "snowflake",
        "spark",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "data analysis",
        "data analytics",
        "data pipeline",
        "databricks",
        "docker",
        "kubernetes",
        "machine learning",
        "numpy",
        "pandas",
        "python",
        "reporting",
        "scipy",
        "snowflake",
        "spark"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate possesses essential data engineering skills—Python, scalable pipelines, Spark, Snowflake, and Airflow—that align with the role’s core responsibilities and indicates they can learn the specific trading domain nuances.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Matt.pdf",
      "job_rank": 1,
      "job_title": "Front Office Interest Rates Quantitative Analyst - New York with verification",
      "job_company": "Santander Bank, N.A.",
      "job_id": "4308348996",
      "skill_score": 0.25,
      "semantic_score": 0.46326334883978104,
      "topic_score": 0.3660828471183777,
      "final_score": 0.5610048234843095,
      "resume_skills_count": 6,
      "job_skills_count": 4,
      "matching_skills_count": 2,
      "resume_text_length": 4201,
      "resume_skills": [
        "c",
        "coaching",
        "excel",
        "java",
        "leadership",
        "python"
      ],
      "job_skills": [
        "c",
        "python",
        "risk management",
        "rust"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate does not have the necessary experience in quantitative analytics for capital markets, nor the required programming proficiency or financial domain knowledge (fixed‑income products, pricing theory) to perform the core job functions.",
      "llm_recommendations": [
        "Gain hands‑on experience in quantitative finance by completing a structured internship or project focused on pricing models, risk analytics, or interest‑rate derivatives.",
        "Deepen programming skills in C++ and Python (and optionally Rust) through specialized courses or self‑driven projects that solve real capital‑market problems.",
        "Study fixed‑income analytics, term structure modeling, and option pricing theory; consider short‑course certificates or a relevant Master’s degree to build foundational knowledge.",
        "Build a portfolio of quantitative projects (e.g., curve fitting, pricing algorithms, risk metrics) that can be demonstrated on GitHub or a professional website.",
        "Network with finance professionals and join fintech or quant communities to stay current on industry practices and expand professional references."
      ],
      "linkedin_keywords": [
        "quantitative finance",
        "interest rate models",
        "fixed income analytics",
        "pricing models",
        "C++",
        "Python",
        "risk management",
        "derivatives",
        "financial engineering",
        "capital markets."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Maya_Hernandez_20251129_062719.pdf",
      "job_rank": 1,
      "job_title": "Data Analytics Software Engineer (Trading)",
      "job_company": "Fintal Partners",
      "job_id": "4318639448",
      "skill_score": 0.28,
      "semantic_score": 0.6992204189300736,
      "topic_score": 0.7924491763114929,
      "final_score": 0.817001054286964,
      "resume_skills_count": 18,
      "job_skills_count": 14,
      "matching_skills_count": 7,
      "resume_text_length": 2216,
      "resume_skills": [
        "airflow",
        "data analysis",
        "data analytics",
        "data pipeline",
        "data visualization",
        "excel",
        "forecasting",
        "inventory management",
        "leadership",
        "machine learning",
        "power bi",
        "python",
        "r",
        "reporting",
        "snowflake",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "data analysis",
        "data analytics",
        "data pipeline",
        "databricks",
        "docker",
        "kubernetes",
        "machine learning",
        "numpy",
        "pandas",
        "python",
        "reporting",
        "scipy",
        "snowflake",
        "spark"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks critical experience with real‑time trading data pipelines, distributed analytics engines (Spark/Dask/Trino), and the high‑velocity, low‑latency focus required for an HFT environment.",
      "llm_recommendations": [
        "Take hands‑on projects or courses in real‑time data streaming (Kafka, Apache Flink) and distributed processing (Spark, Dask).",
        "Build a portfolio showcasing Python data pipelines on large datasets, using libraries such as Pandas, NumPy, and scikit‑learn.",
        "Gain practical exposure to JupyterHub/JupyterLab and cloud data lake tools (Databricks, Snowflake, Iceberg, Parquet).",
        "Familiarize yourself with Linux, Docker/Kubernetes, and basic machine‑learning concepts applied to market data."
      ],
      "linkedin_keywords": [
        "Python",
        "Snowflake",
        "Airflow",
        "SQL",
        "Data Pipelines",
        "Data Engineering",
        "ETL",
        "Pandas",
        "NumPy",
        "scikit‑learn",
        "Spark",
        "Dask",
        "Trino",
        "Iceberg",
        "Parquet",
        "Jupyter",
        "High Frequency Trading",
        "Quantitative Finance",
        "Distributed Computing",
        "Machine Learning."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Maya_Patel_20251129_062934.pdf",
      "job_rank": 1,
      "job_title": "Data Analytics Software Engineer (Trading)",
      "job_company": "Fintal Partners",
      "job_id": "4318639448",
      "skill_score": 0.10344827586206896,
      "semantic_score": 0.6522311749733941,
      "topic_score": 0.8094112873077393,
      "final_score": 0.7586673106777494,
      "resume_skills_count": 18,
      "job_skills_count": 14,
      "matching_skills_count": 3,
      "resume_text_length": 2892,
      "resume_skills": [
        "a/b testing",
        "aws",
        "azure",
        "c",
        "cloud platforms",
        "data analytics",
        "data visualization",
        "decision making",
        "feature engineering",
        "forecasting",
        "leadership",
        "machine learning",
        "natural language processing",
        "python",
        "r",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "data analysis",
        "data analytics",
        "data pipeline",
        "databricks",
        "docker",
        "kubernetes",
        "machine learning",
        "numpy",
        "pandas",
        "python",
        "reporting",
        "scipy",
        "snowflake",
        "spark"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "Maya's extensive experience with Python, real‑time data pipelines (AWS EMR), large‑scale data storage, and cloud‑based analytics infrastructure aligns with the core requirements of the role, and her track record indicates she can rapidly acquire any remaining tools such as Jupyter, Spark, or container orchestration.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "MeenalGupta (10).pdf",
      "job_rank": 1,
      "job_title": "Senior iOS Developer with verification",
      "job_company": "Kajabi",
      "job_id": "4299254728",
      "skill_score": 0.09090909090909091,
      "semantic_score": 0.547380805015564,
      "topic_score": 0.5961505770683289,
      "final_score": 0.6106960827654059,
      "resume_skills_count": 8,
      "job_skills_count": 16,
      "matching_skills_count": 2,
      "resume_text_length": 3772,
      "resume_skills": [
        "a/b testing",
        "c",
        "git",
        "github",
        "javascript",
        "matlab",
        "mongodb",
        "salesforce"
      ],
      "job_skills": [
        "agile",
        "apollo",
        "ci/cd",
        "circleci",
        "coaching",
        "collaboration",
        "communication",
        "confluence",
        "git",
        "github",
        "ios development",
        "jira",
        "leadership",
        "reporting",
        "swift",
        "unit testing"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate does not have any documented iOS or Swift experience, which is a critical core requirement for this senior iOS developer role.",
      "llm_recommendations": [
        "Complete a focused iOS development course (e.g., Apple’s SwiftUI & UIKit curriculum) and build at least three end‑to‑end apps to demonstrate proficiency.",
        "Contribute to open‑source iOS projects or add iOS features to existing projects to gain hands‑on experience with Swift, SwiftUI, MVVM, and async/await.",
        "Acquire practical exposure to iOS deployment (Fastlane, TestFlight, App Store Connect) and add these processes to a portfolio project.",
        "Obtain a certification or display a portfolio that showcases unit testing with XCTest, dependency injection, and Core Data usage to cover architecture best practices.",
        "Highlight transferable skills such as unit testing, CI/CD, and API integration in a cover letter, emphasizing the ability to quickly adopt new platform tools."
      ],
      "linkedin_keywords": [
        "iOS Developer",
        "Swift",
        "SwiftUI",
        "UIKit",
        "MVVM",
        "iOS Architecture",
        "Swift Concurrency",
        "Async/Await",
        "Xcode",
        "Fastlane",
        "Unit Testing",
        "XCTest",
        "App Store Connect",
        "TestFlight",
        "Git",
        "Agile."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Mia_Rodriguez_20251129_063642.pdf",
      "job_rank": 1,
      "job_title": "Senior Data Engineer",
      "job_company": "Envision Employment Solutions",
      "job_id": "4332319156",
      "skill_score": 0.36666666666666664,
      "semantic_score": 0.7116574473176878,
      "topic_score": 0.931459903717041,
      "final_score": 0.8869871611609974,
      "resume_skills_count": 20,
      "job_skills_count": 21,
      "matching_skills_count": 11,
      "resume_text_length": 2410,
      "resume_skills": [
        "agile",
        "airflow",
        "aws",
        "aws redshift",
        "business intelligence",
        "data analysis",
        "data pipeline",
        "elt",
        "etl",
        "git",
        "machine learning",
        "performance analysis",
        "power bi",
        "python",
        "redshift",
        "reporting",
        "snowflake",
        "spark",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "airflow",
        "aws",
        "bigquery",
        "ci/cd",
        "collaboration",
        "communication",
        "data pipeline",
        "databricks",
        "docker",
        "elt",
        "etl",
        "gcp",
        "java",
        "kubernetes",
        "python",
        "redshift",
        "reporting",
        "scala",
        "snowflake",
        "spark",
        "sql"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate possesses the essential skills and experience—8 years in data engineering, strong SQL and Python proficiency, proven work with Airflow, dbt, Snowflake, and Redshift, and cloud‑based pipeline design—indicating they can perform the core responsibilities of the Senior Data Engineer role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Michael_Xu.pdf",
      "job_rank": 1,
      "job_title": "Full Stack Engineer",
      "job_company": "IOSSERVICES",
      "job_id": "4318448366",
      "skill_score": 0.26666666666666666,
      "semantic_score": 0.6541839448103949,
      "topic_score": 0.6932657361030579,
      "final_score": 0.7607315496682661,
      "resume_skills_count": 15,
      "job_skills_count": 23,
      "matching_skills_count": 8,
      "resume_text_length": 2377,
      "resume_skills": [
        "angular",
        "aws",
        "c",
        "c++",
        "git",
        "go",
        "java",
        "javascript",
        "mongodb",
        "mysql",
        "nosql",
        "postgresql",
        "python",
        "sql",
        "typescript"
      ],
      "job_skills": [
        "aws",
        "ci/cd",
        "cloud platforms",
        "communication",
        "docker",
        "git",
        "github",
        "github actions",
        "go",
        "javascript",
        "jenkins",
        "jira",
        "kubernetes",
        "microservices",
        "mongodb",
        "next.js",
        "node.js",
        "nosql",
        "postgresql",
        "python",
        "react",
        "rust",
        "terraform"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate meets all core requirements with experience in React, Node.js, Go, PostgreSQL, HTML/CSS, Git, and over four years of full‑stack development.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Mina_Rahman_20251129_140708.pdf",
      "job_rank": 1,
      "job_title": "AI Software Engineer with verification",
      "job_company": "XpertDirect",
      "job_id": "4318464252",
      "skill_score": 0.2692307692307692,
      "semantic_score": 0.6958184242248585,
      "topic_score": 0.7405862808227539,
      "final_score": 0.7940709499212429,
      "resume_skills_count": 22,
      "job_skills_count": 11,
      "matching_skills_count": 7,
      "resume_text_length": 2014,
      "resume_skills": [
        "aws",
        "ci/cd",
        "collaboration",
        "data cleaning",
        "data ingestion",
        "data visualization",
        "deep learning",
        "docker",
        "feature engineering",
        "feature store",
        "forecasting",
        "kubernetes",
        "machine learning",
        "mlflow",
        "python",
        "random forest",
        "reporting",
        "sagemaker",
        "sql",
        "statistical analysis",
        "tableau",
        "xgboost"
      ],
      "job_skills": [
        "aws",
        "aws lambda",
        "docker",
        "fastapi",
        "forecasting",
        "lambda",
        "machine learning",
        "pandas",
        "python",
        "sagemaker",
        "sql"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate possesses more than five years of ML/AI experience with core tools such as Python, TensorFlow, PyTorch, SQL, Docker, and AWS SageMaker, along with proven time‑series forecasting and end‑to‑end MLOps capabilities, indicating they can reasonably perform the essential job functions even without explicit SCADA or IoT experience.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Ming_Chen_20251129_064421.pdf",
      "job_rank": 1,
      "job_title": "IL0202 – Data Scientist with verification",
      "job_company": "RR Donnelley",
      "job_id": "4331336561",
      "skill_score": 0.17073170731707318,
      "semantic_score": 0.6664709250015584,
      "topic_score": 0.8786994814872742,
      "final_score": 0.8114121197636622,
      "resume_skills_count": 18,
      "job_skills_count": 30,
      "matching_skills_count": 7,
      "resume_text_length": 1771,
      "resume_skills": [
        "customer segmentation",
        "data analysis",
        "data visualization",
        "decision making",
        "etl",
        "feature engineering",
        "forecasting",
        "hadoop",
        "machine learning",
        "operational efficiency",
        "python",
        "r",
        "reporting",
        "sales forecasting",
        "sql",
        "statistical analysis",
        "tableau",
        "xgboost"
      ],
      "job_skills": [
        "agile",
        "athena",
        "aws",
        "aws redshift",
        "business intelligence",
        "cloud computing",
        "data analytics",
        "data pipeline",
        "data visualization",
        "feature engineering",
        "git",
        "hadoop",
        "java",
        "javascript",
        "jenkins",
        "lambda",
        "machine learning",
        "matplotlib",
        "mysql",
        "postgresql",
        "pyspark",
        "python",
        "r",
        "redshift",
        "s3",
        "sagemaker",
        "seaborn",
        "shell scripting",
        "snowflake",
        "tableau"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate does not meet the critical core qualification of holding a Master’s degree in Data Science, Computer Science, or a related field, which is explicitly required for this role.",
      "llm_recommendations": [
        "Pursue a Master’s degree in Data Science, Computer Science, or a related discipline (online or part‑time if feasible).",
        "Acquire AWS certification (e.g., AWS Certified Solutions Architect or Data Analytics) to demonstrate familiarity with required cloud services.",
        "Add Java or PySpark experience through a short project or coursework, and include it prominently on the resume.",
        "Highlight any exposure to PostgreSQL/MySQL, Agile methodologies, and version control (Git) to address additional role expectations.",
        "Obtain a relevant certification in Machine Learning or Big Data Platforms (e.g., Certified SnowPro or Tableau Desktop Specialist) to strengthen the skill set."
      ],
      "linkedin_keywords": [
        "Data Scientist",
        "Machine Learning Engineer",
        "AWS Certified",
        "Python",
        "SQL",
        "Tableau",
        "Hadoop",
        "Big Data",
        "Feature Engineering",
        "Predictive Modeling",
        "Cloud Data Engineering",
        "Agile Data Science",
        "Statistical Modeling",
        "Data Engineering",
        "ETL",
        "Data Visualization",
        "Python ML",
        "R Data Science",
        "Python for Data Science."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Minho_Kim_20251129_061917.pdf",
      "job_rank": 1,
      "job_title": "Data Engineer AWS - Hybrid in NY",
      "job_company": "TechTriad",
      "job_id": "4317962527",
      "skill_score": 0.38461538461538464,
      "semantic_score": 0.7441980700138695,
      "topic_score": 0.9411957859992981,
      "final_score": 0.9031981095425131,
      "resume_skills_count": 19,
      "job_skills_count": 17,
      "matching_skills_count": 10,
      "resume_text_length": 1708,
      "resume_skills": [
        "airflow",
        "aws",
        "ci/cd",
        "data analysis",
        "data analytics",
        "data pipeline",
        "databricks",
        "docker",
        "etl",
        "gcp",
        "github",
        "github actions",
        "kubernetes",
        "machine learning",
        "mlflow",
        "performance management",
        "python",
        "spark",
        "sql"
      ],
      "job_skills": [
        "airflow",
        "aws",
        "ci/cd",
        "data pipeline",
        "databricks",
        "distributed systems",
        "feature store",
        "java",
        "kubernetes",
        "lambda",
        "machine learning",
        "mlflow",
        "pyspark",
        "python",
        "s3",
        "scala",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate does not meet the critical non‑technical requirement that applicants must be US Citizens or Green Card holders and locals in New York, which is a hard prerequisite for this role.",
      "llm_recommendations": [
        "Pursue US citizenship or obtain a Green Card before applying to positions that require local residency.",
        "Seek roles that accept remote or international candidates with similar technical expertise.",
        "Leverage an employer that sponsors work visas and clearly communicate relocation plans.",
        "Highlight in the application how your existing skills (Databricks, AWS, Python, CI/CD) align with the job while addressing the relocation or visa requirement.",
        "Keep your LinkedIn profile updated with relevant certifications (AWS Certified Data Analytics, PySpark, etc.) to improve eligibility."
      ],
      "linkedin_keywords": [
        "Data Engineer",
        "AWS",
        "Databricks",
        "MLOps",
        "CI/CD",
        "Python",
        "Feature Store",
        "Apache Spark",
        "NiFi",
        "Airflow",
        "MLflow",
        "Kubernetes",
        "data pipeline."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Mira_Kim_20251129_140816.pdf",
      "job_rank": 1,
      "job_title": "AI/ML Software Engineer with verification",
      "job_company": "Guidehouse",
      "job_id": "4332300185",
      "skill_score": 0.10344827586206896,
      "semantic_score": 0.6276045850633234,
      "topic_score": 0.6825710535049438,
      "final_score": 0.6907683897030164,
      "resume_skills_count": 15,
      "job_skills_count": 17,
      "matching_skills_count": 3,
      "resume_text_length": 2978,
      "resume_skills": [
        "aws",
        "azure",
        "bert",
        "c",
        "ci/cd",
        "deep learning",
        "docker",
        "gcp",
        "java",
        "kubernetes",
        "named entity recognition",
        "natural language processing",
        "nlp",
        "python",
        "translation"
      ],
      "job_skills": [
        "aws",
        "communication",
        "data pipeline",
        "deep learning",
        "javascript",
        "machine learning",
        "numpy",
        "oracle",
        "outreach",
        "pandas",
        "postgresql",
        "python",
        "rest apis",
        "software development",
        "software engineering",
        "sql",
        "typescript"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate does not meet several essential core requirements: no JavaScript/TypeScript experience, no demonstrated RAG or LangChain/Haystack work, and no evidence of Elasticsearch or relational‑database usage, all of which are critical for the role.",
      "llm_recommendations": [
        "Acquire hands‑on experience with JavaScript/TypeScript, building web APIs and front‑end integrations.",
        "Gain proficiency in RAG architectures and tools such as LangChain or Haystack through projects or short courses.",
        "Develop skills in Elasticsearch/OpenSearch and relational databases (e.g., PostgreSQL) for data‑pipeline work.",
        "Highlight any security clearances or related credentials, or obtain a Public Trust clearance if needed.",
        "Emphasize transferable API design, AWS deployment, and AI chatbot integration in the resume to bridge skill gaps."
      ],
      "linkedin_keywords": [
        "JavaScript",
        "TypeScript",
        "LangChain",
        "Haystack",
        "Elasticsearch",
        "AWS",
        "RAG",
        "REST API",
        "SQL",
        "PyTorch"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Mira_Patel_20251129_061342.pdf",
      "job_rank": 1,
      "job_title": "AI/ML Software Engineer with verification",
      "job_company": "Guidehouse",
      "job_id": "4332300185",
      "skill_score": 0.125,
      "semantic_score": 0.6291219813154958,
      "topic_score": 0.6273423433303833,
      "final_score": 0.6747031420325721,
      "resume_skills_count": 10,
      "job_skills_count": 17,
      "matching_skills_count": 3,
      "resume_text_length": 2216,
      "resume_skills": [
        "a/b testing",
        "aws",
        "aws lambda",
        "ci/cd",
        "docker",
        "lambda",
        "leadership",
        "machine learning",
        "nlp",
        "python"
      ],
      "job_skills": [
        "aws",
        "communication",
        "data pipeline",
        "deep learning",
        "javascript",
        "machine learning",
        "numpy",
        "oracle",
        "outreach",
        "pandas",
        "postgresql",
        "python",
        "rest apis",
        "software development",
        "software engineering",
        "sql",
        "typescript"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks the required 5 years of hands‑on JavaScript/TypeScript experience and has no demonstrated RAG or LangChain‑style application experience, which are core to the role’s responsibilities.",
      "llm_recommendations": [
        "Gain practical JavaScript/TypeScript skills through side projects or open‑source contributions that integrate with AI services.",
        "Build a small RAG prototype using LangChain, Haystack, or crewAI to show end‑to‑end retrieval–augmented generation capability.",
        "Develop and deploy a REST API for an NLP model (e.g., FastAPI) to demonstrate integration and web service skills.",
        "Expand knowledge of AWS IaC tools (CloudFormation/Terraform) and model deployment practices on AWS SageMaker or ECS.",
        "Highlight transferable data pipeline, chatbot, and AWS experience in the résumé and include metrics or case studies."
      ],
      "linkedin_keywords": [
        "AI Engineer",
        "NLP Engineer",
        "Machine Learning Engineer",
        "Data Engineer",
        "Chatbot Developer",
        "Python AI",
        "AWS ML Engineer",
        "RAG Developer",
        "JavaScript ML",
        "LangChain",
        "Haystack",
        "REST API AI",
        "Conversational AI",
        "AWS Infrastructure-as-Code"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Nadia_Mahmoud_20251129_140854.pdf",
      "job_rank": 1,
      "job_title": "Sr. Analyst, Digital Initiatives",
      "job_company": "Toast",
      "job_id": "4331625169",
      "skill_score": 0.38095238095238093,
      "semantic_score": 0.7468849118810788,
      "topic_score": 0.9088107347488403,
      "final_score": 0.8934296049092607,
      "resume_skills_count": 15,
      "job_skills_count": 14,
      "matching_skills_count": 8,
      "resume_text_length": 1766,
      "resume_skills": [
        "aws",
        "aws glue",
        "bash",
        "data analysis",
        "data visualization",
        "etl",
        "machine learning",
        "power bi",
        "python",
        "r",
        "reporting",
        "sql",
        "sql server",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "business intelligence",
        "communication",
        "data analysis",
        "data analytics",
        "data visualization",
        "excel",
        "power bi",
        "presentation skills",
        "python",
        "r",
        "reporting",
        "sql",
        "stakeholder management",
        "tableau"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate demonstrates the core data analytics, SQL, Power BI/Tableau, stakeholder engagement, and business‑acumen skills required for the Sr. Analyst role, with 9 years of relevant experience.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Nick_Pomes_Resume.pdf",
      "job_rank": 1,
      "job_title": "Social Media & Content Intern",
      "job_company": "Tasker AI",
      "job_id": "4332409175",
      "skill_score": 0.08333333333333333,
      "semantic_score": 0.5207223737024667,
      "topic_score": 0.8824037909507751,
      "final_score": 0.7264328254660691,
      "resume_skills_count": 11,
      "job_skills_count": 2,
      "matching_skills_count": 1,
      "resume_text_length": 3625,
      "resume_skills": [
        "client communication",
        "collaboration",
        "communication",
        "content creation",
        "digital marketing",
        "leadership",
        "reporting",
        "seo",
        "social media analytics",
        "social media management",
        "strategic planning"
      ],
      "job_skills": [
        "collaboration",
        "community management"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate has extensive experience managing social media for a major university, creating and editing short‑form video, maintaining content calendars, analyzing engagement metrics, and demonstrating storytelling and copywriting skills—core capabilities required by the internship.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Nikhil_Shah_20251129_065017.pdf",
      "job_rank": 1,
      "job_title": "Sr./Staff Data Scientist",
      "job_company": "RemoteHunter",
      "job_id": "4319347697",
      "skill_score": 0.17391304347826086,
      "semantic_score": 0.5934733859864934,
      "topic_score": 0.8238397240638733,
      "final_score": 0.759324980238195,
      "resume_skills_count": 14,
      "job_skills_count": 13,
      "matching_skills_count": 4,
      "resume_text_length": 1928,
      "resume_skills": [
        "a/b testing",
        "customer segmentation",
        "data pipeline",
        "data visualization",
        "feature engineering",
        "feature engineering pipelines",
        "gradient boosting",
        "hadoop",
        "machine learning",
        "python",
        "snowflake",
        "spark",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "aws",
        "azure",
        "communication",
        "data analysis",
        "data cleaning",
        "data transformation",
        "distributed systems",
        "feature engineering",
        "gcp",
        "machine learning",
        "problem-solving skills",
        "python",
        "spark"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate meets all essential qualifications: 5+ years of data science experience, a proven fraud detection model built with Spark MLlib, strong Python and distributed systems skills, and demonstrated ability to deploy models and communicate insights.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Nora_Schaefer_20251129_061101.pdf",
      "job_rank": 1,
      "job_title": "Sr. Data Engineer",
      "job_company": "RemoteHunter",
      "job_id": "4319158781",
      "skill_score": 0.2692307692307692,
      "semantic_score": 0.7165805955625342,
      "topic_score": 0.8048734664916992,
      "final_score": 0.8251466765198161,
      "resume_skills_count": 19,
      "job_skills_count": 14,
      "matching_skills_count": 7,
      "resume_text_length": 2493,
      "resume_skills": [
        "agile",
        "airflow",
        "aws",
        "ci/cd",
        "data analysis",
        "data pipeline",
        "decision making",
        "docker",
        "etl",
        "git",
        "java",
        "power bi",
        "pyspark",
        "python",
        "snowflake",
        "software development",
        "spark",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "airflow",
        "athena",
        "ci/cd",
        "communication",
        "data pipeline",
        "dynamodb",
        "elt",
        "etl",
        "java",
        "python",
        "risk management",
        "sql",
        "teamwork",
        "terraform"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks critical deep proficiency with core distributed data systems (ClickHouse, Athena, Kafka) and specific database experience (Postgres) that are essential for building and maintaining the production ETL/ELT pipelines required for this role.",
      "llm_recommendations": [
        "Pursue hands‑on projects or certifications to gain proficiency in ClickHouse, Athena, and Kafka, focusing on real‑time, low‑latency pipeline scenarios.",
        "Highlight transferable experience with Spark, Snowflake, and Airflow, demonstrating ability to architect scalable data workflows and draw parallels to distributed systems.",
        "Show concrete examples of performance tuning, sharding, or partitioning in existing environments to evidence capability with large‑scale data storage solutions.",
        "Obtain a Postgres or PostgreSQL certification and add documented use cases to your portfolio to address the missing database skill."
      ],
      "linkedin_keywords": [
        "real‑time data pipeline",
        "Kafka",
        "ClickHouse",
        "Athena",
        "Postgres",
        "data engineering",
        "Airflow",
        "Python",
        "SQL",
        "Spark",
        "Snowflake",
        "ETL",
        "data modeling",
        "distributed data systems."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Raymond Cao.pdf",
      "job_rank": 1,
      "job_title": "Systematic Literature Review – Life Sciences (Freelancer/Consultant)",
      "job_company": "CapeStart",
      "job_id": "4332443471",
      "skill_score": 0.0,
      "semantic_score": 0.3318117558956146,
      "topic_score": 0.3088737428188324,
      "final_score": 0.3203427493572235,
      "resume_skills_count": 8,
      "job_skills_count": 1,
      "matching_skills_count": 0,
      "resume_text_length": 3522,
      "resume_skills": [
        "client communication",
        "data analytics",
        "excel",
        "forecasting",
        "leadership",
        "microsoft excel",
        "outreach",
        "social media management"
      ],
      "job_skills": [
        "reporting"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate’s background in finance, real estate, and entrepreneurship lacks the essential experience in designing, executing, and reporting systematic literature reviews in the life sciences domain, as well as knowledge of PRISMA, GPP, and related industry standards.",
      "llm_recommendations": [
        "Enroll in an accredited systematic review training program (e.g., Cochrane, PRISMA workshops) to build foundational knowledge and skills.",
        "Gain hands‑on experience by partnering with a research team or completing a small rapid review project to demonstrate proficiency in literature searching, screening, data extraction, and quality assessment.",
        "Complete a certification in evidence synthesis or health research methodology and include a portfolio of completed reviews on the résumé.",
        "Highlight analytical strengths (e.g., advanced Excel, VBA, data modeling) as transferable skills while framing them within the context of systematic review analysis (stats, meta-analysis).",
        "Create or update a professional LinkedIn profile with keywords such as “systematic review,” “PRISMA,” “PRISMA flow,” “rapid review,” “evidence synthesis,” and “life sciences literature.”"
      ],
      "linkedin_keywords": [
        "systematic literature review",
        "PRISMA",
        "GPP",
        "rapid review",
        "evidence synthesis",
        "systematic review methodology",
        "meta-analysis",
        "health sciences",
        "life sciences research",
        "Cochrane",
        "systematic review consulting",
        "literature-based deliverables",
        "research protocol development."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Resume Example 1.pdf",
      "job_rank": 1,
      "job_title": "Procurement Executive",
      "job_company": "Rockhill Asia",
      "job_id": "4318667112",
      "skill_score": 0.2631578947368421,
      "semantic_score": 0.7769247160509073,
      "topic_score": 0.847687304019928,
      "final_score": 0.861699165289255,
      "resume_skills_count": 13,
      "job_skills_count": 11,
      "matching_skills_count": 5,
      "resume_text_length": 4331,
      "resume_skills": [
        "communication",
        "excel",
        "forecasting",
        "leadership",
        "logistics",
        "microsoft excel",
        "operational efficiency",
        "oracle",
        "process improvement",
        "procurement",
        "project management",
        "reporting",
        "sales operations"
      ],
      "job_skills": [
        "collaboration",
        "communication",
        "cost analysis",
        "cross-functional collaboration",
        "excel",
        "logistics",
        "merchandising",
        "negotiation",
        "procurement",
        "reporting",
        "stakeholder management"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate has direct procurement experience, strong supplier management, cost negotiation, PO handling, Excel expertise, and the requisite communication skills, fully covering the essential requirements for the Procurement Executive role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Resume Example 2.pdf",
      "job_rank": 1,
      "job_title": "Systematic Literature Review – Life Sciences (Freelancer/Consultant)",
      "job_company": "CapeStart",
      "job_id": "4332443471",
      "skill_score": 0.0,
      "semantic_score": 0.3318117558956146,
      "topic_score": 0.3088737428188324,
      "final_score": 0.3203427493572235,
      "resume_skills_count": 8,
      "job_skills_count": 1,
      "matching_skills_count": 0,
      "resume_text_length": 3522,
      "resume_skills": [
        "client communication",
        "data analytics",
        "excel",
        "forecasting",
        "leadership",
        "microsoft excel",
        "outreach",
        "social media management"
      ],
      "job_skills": [
        "reporting"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks any evidence of experience in systematic literature reviews, PRISMA/GPP methodology, or life‑science research, which are critical for delivering the full‑cycle SLR work required for this role.",
      "llm_recommendations": [
        "Enroll in an online course or workshop covering systematic review methodology, PRISMA guidelines, and rapid review techniques (e.g., from Cochrane or IQR).",
        "Volunteer or join a research project (academic, NGO, or industry) that involves literature searching, screening, data extraction, and reporting to build concrete experience.",
        "Develop a portfolio of literature‑review deliverables or case studies, and highlight any analytical or reporting tasks performed in current or past roles.",
        "Obtain a certification (e.g., Certified Evidence Synthesis Consultant or similar) to formally signal expertise in evidence synthesis.",
        "Build foundational knowledge of life‑science topics through self‑study or MOOCs to demonstrate domain relevance to potential clients."
      ],
      "linkedin_keywords": [
        "systematic literature review",
        "PRISMA",
        "GPP",
        "rapid review",
        "life sciences research",
        "evidence synthesis",
        "meta-analysis",
        "protocol development",
        "data extraction",
        "quality appraisal",
        "literature search",
        "systematic review methodology",
        "research methodology",
        "academic writing",
        "healthcare research",
        "research analytics",
        "evidence-based review",
        "scientific literature review",
        "review coordination."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Resume Jeffrey Nord Systems Administrator.pdf",
      "job_rank": 1,
      "job_title": "Microsoft Entra ID",
      "job_company": "Cubical Operations LLP",
      "job_id": "4332402375",
      "skill_score": 0.0,
      "semantic_score": 0.3410060608986658,
      "topic_score": 0.6519741415977478,
      "final_score": 0.4964901012482068,
      "resume_skills_count": 4,
      "job_skills_count": 1,
      "matching_skills_count": 0,
      "resume_text_length": 5934,
      "resume_skills": [
        "excel",
        "project management",
        "software development",
        "user research"
      ],
      "job_skills": [
        "azure"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate does not possess the essential Azure AD and Microsoft Intune administration experience required for the Entra ID role, nor does he hold the key certifications (SC‑300, AZ‑500, etc.) that demonstrate proficiency in identity, device management, and compliance features fundamental to this position.",
      "llm_recommendations": [
        "Enroll in a hands‑on Azure AD & Intune training program (e.g., Microsoft Learn, LinkedIn Learning) and complete the SC‑300: Identity and Access Administrator certification.",
        "Build a small pilot project that sets up Azure AD Connect, configures Conditional Access, and deploys Intune MDM policies to demonstrate practical skills.",
        "Highlight any existing Windows Server or Active Directory experience on a portfolio or resume, emphasizing roles that involved user and device provisioning, Group Policy, or hybrid identity solutions.",
        "Pursue supplemental certifications such as AZ‑500: Azure Security Engineer Associate to reinforce security fundamentals.",
        "Update the résumé to include keywords related to Entra ID, Azure AD, Intune, Conditional Access, PIM, and Office 365 admin to improve visibility in applicant tracking systems."
      ],
      "linkedin_keywords": [
        "Microsoft Entra ID",
        "Azure AD",
        "Microsoft Intune",
        "Microsoft Endpoint Manager",
        "Conditional Access",
        "MFA",
        "Azure AD Connect",
        "PIM",
        "Windows Hello",
        "SSO",
        "Office 365 Administration",
        "SharePoint Online",
        "Microsoft Purview",
        "Data Classification",
        "Rights Management",
        "SCCM",
        "Autopilot",
        "BitLocker",
        "Azure Security Engineer."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Rina_Tanaka_20251129_064106.pdf",
      "job_rank": 1,
      "job_title": "Data and Production Support Analyst with verification",
      "job_company": "VanEck",
      "job_id": "4310751775",
      "skill_score": 0.15384615384615385,
      "semantic_score": 0.6792125701904297,
      "topic_score": 0.8099855780601501,
      "final_score": 0.7838915242598608,
      "resume_skills_count": 16,
      "job_skills_count": 14,
      "matching_skills_count": 4,
      "resume_text_length": 2057,
      "resume_skills": [
        "customer retention",
        "data analysis",
        "data cleaning",
        "data visualization",
        "excel",
        "hadoop",
        "machine learning",
        "oracle",
        "power bi",
        "python",
        "r",
        "reporting",
        "spark",
        "sql",
        "sql server",
        "tableau"
      ],
      "job_skills": [
        "business analysis",
        "communication",
        "critical thinking",
        "data analysis",
        "data pipeline",
        "data visualization",
        "multitasking",
        "operational efficiency",
        "problem solving",
        "reporting",
        "sql",
        "teamwork",
        "time management",
        "translation"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks the essential financial domain knowledge and specific industry experience required for managing investment data pipelines and performing market data analysis in this role.",
      "llm_recommendations": [
        "Acquire domain knowledge by completing finance or asset‑management training courses (e.g., Bloomberg Market Concepts, CFA Level 1).",
        "Gain hands‑on experience with financial data sets via internships, freelance projects, or open‑source financial data pipelines.",
        "Highlight transferable skills in SQL, ETL, and data quality by illustrating how they were applied in complex data environments (e.g., large‑scale data ingestion, automated monitoring).",
        "Pursue certifications in data warehouse or data integration tools (e.g., SQL Server Integration Services, Informatica).",
        "Emphasize past experience with real‑time analytics and troubleshooting to demonstrate capability to adapt to production support tasks."
      ],
      "linkedin_keywords": [
        "Data Analyst",
        "SQL",
        "ETL",
        "Data Pipeline",
        "Financial Data",
        "Market Data",
        "Data Operations",
        "BI",
        "Power BI",
        "Python",
        "R",
        "Tableau",
        "Data Quality",
        "Asset Management",
        "Asset Analyst",
        "Investment Data",
        "Portfolio Operations",
        "Data Engineering",
        "Business Analysis",
        "Tableau Developer",
        "Power BI Developer",
        "SQL Developer."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Robbie-Shawn-Resume-2025.pdf",
      "job_rank": 1,
      "job_title": "Senior Digital Marketing Manager with verification",
      "job_company": "Argano",
      "job_id": "4318098498",
      "skill_score": 0.041666666666666664,
      "semantic_score": 0.6558196153748276,
      "topic_score": 0.7800678610801697,
      "final_score": 0.7296960824680195,
      "resume_skills_count": 12,
      "job_skills_count": 13,
      "matching_skills_count": 1,
      "resume_text_length": 10819,
      "resume_skills": [
        "aws",
        "c",
        "communication",
        "klaviyo",
        "logistics",
        "magento",
        "mailchimp",
        "netsuite",
        "r",
        "seo",
        "shopify",
        "woocommerce"
      ],
      "job_skills": [
        "collaboration",
        "communication",
        "content marketing",
        "digital marketing",
        "email marketing",
        "google analytics",
        "hubspot",
        "lead generation",
        "marketing automation",
        "oracle",
        "project management",
        "salesforce",
        "sem"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume demonstrates strong e‑commerce marketing expertise but lacks the core B2B demand‑generation, marketing‑automation, Oracle ecosystem, SEM, and email‑marketing skills required for this senior digital‑marketing role.",
      "llm_recommendations": [
        "Complete a HubSpot and Salesforce certification to prove automation and CRM proficiency.",
        "Build hands‑on SEM (Google Ads, Bing Ads) and email‑marketing experience through a side project or freelance work.",
        "Gain exposure to Oracle products or partner with a tech consultancy to understand Oracle ecosystem marketing.",
        "Highlight any cross‑functional stakeholder collaboration or ABM‑style initiatives from past roles to showcase transferable demand‑generation experience.",
        "Develop a portfolio of B2B marketing case studies, including content syndication and pipeline‑focused campaigns, to showcase relevant results."
      ],
      "linkedin_keywords": [
        "Digital Marketing Manager",
        "B2B Marketing",
        "Demand Generation",
        "Marketing Automation",
        "HubSpot",
        "Salesforce",
        "SEM",
        "SEO",
        "Social Media Marketing",
        "Email Marketing",
        "Oracle Marketing",
        "ABM",
        "Lead Generation",
        "Campaign Analytics",
        "Enterprise Marketing."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Robert_Reeves.pdf",
      "job_rank": 1,
      "job_title": "Security Analyst with verification",
      "job_company": "Tyto Athene, LLC",
      "job_id": "4332097196",
      "skill_score": 0.2631578947368421,
      "semantic_score": 0.6480541808653638,
      "topic_score": 0.7954285740852356,
      "final_score": 0.7949673307712735,
      "resume_skills_count": 15,
      "job_skills_count": 9,
      "matching_skills_count": 5,
      "resume_text_length": 4320,
      "resume_skills": [
        "ansible",
        "aws",
        "bash",
        "communication",
        "gcp",
        "git",
        "github",
        "github actions",
        "mentoring",
        "python",
        "s3",
        "siem",
        "software development",
        "teamwork",
        "terraform"
      ],
      "job_skills": [
        "ansible",
        "aws",
        "azure",
        "communication",
        "gcp",
        "incident response",
        "reporting",
        "splunk",
        "terraform"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "Robert Reeves has over four years of systems administration experience, including cloud (AWS, GCP), automation (Terraform, Ansible), and SIEM/Splunk usage, which provide the core technical foundation for vulnerability scanning, patch management, and incident response required for the role. His documented responsibilities for maintaining infrastructure, producing documentation, and communicating effectively with stakeholders demonstrate the necessary client‑facing and reporting competencies as well as the independence and strategic thinking the position requires.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Roselin_Burgos.pdf",
      "job_rank": 1,
      "job_title": "Quality Analyst with verification",
      "job_company": "Esferasoft Solutions Pvt Ltd.",
      "job_id": "4318439314",
      "skill_score": 0.08695652173913043,
      "semantic_score": 0.6643317735689614,
      "topic_score": 0.880854070186615,
      "final_score": 0.7923674504101546,
      "resume_skills_count": 21,
      "job_skills_count": 4,
      "matching_skills_count": 2,
      "resume_text_length": 4014,
      "resume_skills": [
        "angular",
        "django",
        "express.js",
        "flask",
        "full stack development",
        "git",
        "github",
        "java",
        "javascript",
        "jenkins",
        "jira",
        "mongodb",
        "mysql",
        "node.js",
        "nosql",
        "python",
        "rest apis",
        "scrum",
        "software development",
        "sql",
        "sqlite"
      ],
      "job_skills": [
        "agile",
        "jira",
        "reporting",
        "scrum"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate demonstrates extensive manual testing experience, test planning, bug tracking with Jira, regression and database validation, Agile/Scrum participation, and API testing with Postman, all of which cover the core responsibilities of this Quality Analyst role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Sina_Li_20251129_062203.pdf",
      "job_rank": 1,
      "job_title": "AI/Machine Learning (NLP) Engineer with verification",
      "job_company": "Aslase",
      "job_id": "4332070318",
      "skill_score": 0.22580645161290322,
      "semantic_score": 0.7301564273378068,
      "topic_score": 0.7441105842590332,
      "final_score": 0.7964904561020026,
      "resume_skills_count": 25,
      "job_skills_count": 13,
      "matching_skills_count": 7,
      "resume_text_length": 2798,
      "resume_skills": [
        "a/b testing",
        "aws",
        "bert",
        "ci/cd",
        "customer segmentation",
        "data analysis",
        "data cleaning",
        "decision trees",
        "deep learning",
        "docker",
        "feature engineering",
        "gcp",
        "jenkins",
        "kubernetes",
        "logistic regression",
        "machine learning",
        "natural language processing",
        "nlp",
        "python",
        "r",
        "reporting",
        "sentiment analysis",
        "spark",
        "sql",
        "statistical analysis"
      ],
      "job_skills": [
        "ci/cd",
        "data pipeline",
        "docker",
        "embeddings",
        "github",
        "kubernetes",
        "machine learning",
        "mlflow",
        "nlp",
        "prototyping",
        "python",
        "rest apis",
        "spark"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks explicit experience with core tools required by the role—LLM APIs (e.g., OpenAI), AI orchestration frameworks such as LangChain, and vector‑database integrations (Pinecone, Weaviate, FAISS)—which are essential for the defined job functions.",
      "llm_recommendations": [
        "Build a small prototype that integrates an LLM API (OpenAI/Anthropic) using LangChain to demonstrate prompt engineering and orchestration skills.",
        "Gain hands‑on experience with a vector database (Pinecone or Weaviate) by implementing an embeddings pipeline for a RAG use case.",
        "Highlight any existing work on embeddings or retrieval‑augmented generation, even if done with custom code, to show familiarity with the underlying concepts.",
        "Pursue short courses or certifications focused on LLM deployments, LangChain, and vector‑database technologies to fill the gaps and showcase learning agility.",
        "Update the résumé and LinkedIn profile with keywords and project details that reflect these new skills, making the candidate more discoverable for similar roles."
      ],
      "linkedin_keywords": [
        "LLM integration",
        "OpenAI API",
        "LangChain",
        "RAG",
        "vector database",
        "Pinecone",
        "Weaviate",
        "FAISS",
        "prompt engineering",
        "no-code AI",
        "n8n",
        "MLOps",
        "AI orchestration",
        "transformer fine‑tuning",
        "embeddings pipeline"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Sofia_Nikolić_20251129_062513.pdf",
      "job_rank": 1,
      "job_title": "Data and Production Support Analyst with verification",
      "job_company": "VanEck",
      "job_id": "4310751775",
      "skill_score": 0.16666666666666666,
      "semantic_score": 0.6762909083019653,
      "topic_score": 0.8426767587661743,
      "final_score": 0.7995698612783915,
      "resume_skills_count": 21,
      "job_skills_count": 14,
      "matching_skills_count": 5,
      "resume_text_length": 2180,
      "resume_skills": [
        "a/b testing",
        "customer segmentation",
        "data pipeline",
        "data visualization",
        "decision making",
        "digital marketing",
        "etl",
        "excel",
        "feature prioritization",
        "forecasting",
        "leadership",
        "microsoft excel",
        "operational efficiency",
        "power bi",
        "python",
        "r",
        "reporting",
        "sales forecasting",
        "sql",
        "sql server",
        "tableau"
      ],
      "job_skills": [
        "business analysis",
        "communication",
        "critical thinking",
        "data analysis",
        "data pipeline",
        "data visualization",
        "multitasking",
        "operational efficiency",
        "problem solving",
        "reporting",
        "sql",
        "teamwork",
        "time management",
        "translation"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The resume demonstrates strong SQL proficiency, extensive ETL and data pipeline experience (including SSIS), proven ability to ensure data integrity and resolve data issues, and a track record of process improvement and communication with stakeholders—all essential for the Data and Production Support Analyst role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Steven_J_Vik_Incident_Response_Resume.pdf",
      "job_rank": 1,
      "job_title": "Cyber Defense Engineer",
      "job_company": "Confidential",
      "job_id": "4319304178",
      "skill_score": 0.2,
      "semantic_score": 0.6781907283720594,
      "topic_score": 0.9263315200805664,
      "final_score": 0.8418088993810503,
      "resume_skills_count": 15,
      "job_skills_count": 3,
      "matching_skills_count": 3,
      "resume_text_length": 5237,
      "resume_skills": [
        "agile",
        "bash",
        "c",
        "collaboration",
        "communication",
        "go",
        "incident response",
        "problem solving",
        "process optimization",
        "python",
        "r",
        "risk analysis",
        "risk management",
        "siem",
        "splunk"
      ],
      "job_skills": [
        "incident response",
        "python",
        "siem"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks experience with Azure Sentinel, a mandatory requirement for this Cyber Defense Engineer role, and does not demonstrate KQL or API development skills essential for the core responsibilities.",
      "llm_recommendations": [
        "Enroll in an Azure Sentinel training course and complete the Azure Sentinel Associate certification.",
        "Gain hands‑on experience by building a test SIEM environment with Azure Sentinel, writing KQL queries, and creating basic playbooks or automations.",
        "Highlight existing SIEM and automation expertise (Splunk, LogRhythm, Python, PowerShell) that can transfer to Sentinel to strengthen the application.",
        "Pursue Azure Sentinel or Microsoft Defender for Cloud certifications to validate knowledge of Microsoft security tooling.",
        "Document any API integration projects or PowerShell scripting for SIEM tools to demonstrate relevant development skills."
      ],
      "linkedin_keywords": [
        "Azure Sentinel",
        "SIEM",
        "SOC",
        "Incident Response",
        "PowerShell",
        "Python",
        "KQL",
        "API Development",
        "Microsoft Security",
        "Security Automation",
        "Threat Detection",
        "Log Ingestion",
        "Multi‑Tenant Security",
        "MSP Security",
        "Azure Security."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Yin_Zhang_20251129_061731.pdf",
      "job_rank": 1,
      "job_title": "Data Analyst",
      "job_company": "Helic & Co.",
      "job_id": "4332495253",
      "skill_score": 0.3181818181818182,
      "semantic_score": 0.6836525767470771,
      "topic_score": 0.913669228553772,
      "final_score": 0.8627233427161985,
      "resume_skills_count": 17,
      "job_skills_count": 12,
      "matching_skills_count": 7,
      "resume_text_length": 2356,
      "resume_skills": [
        "collaboration",
        "data analytics",
        "data cleaning",
        "data pipeline",
        "data visualization",
        "forecasting",
        "hadoop",
        "machine learning",
        "matlab",
        "power bi",
        "python",
        "r",
        "sas",
        "spark",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "communication",
        "critical thinking",
        "data analysis",
        "data analytics",
        "data visualization",
        "excel",
        "looker",
        "power bi",
        "python",
        "r",
        "sql",
        "tableau"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate possesses all essential technical proficiencies (Python, R, SQL, Tableau, Power BI) and extensive experience handling large datasets, statistical modeling, and stakeholder communication, making them well‑suited to perform the core responsibilities of the Data Analyst role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "campfield-resume.pdf",
      "job_rank": 1,
      "job_title": "DevSecOps",
      "job_company": "QualiZeal",
      "job_id": "4332406515",
      "skill_score": 0.4444444444444444,
      "semantic_score": 0.6433027770103691,
      "topic_score": 0.5199306011199951,
      "final_score": 0.7675648272584346,
      "resume_skills_count": 21,
      "job_skills_count": 18,
      "matching_skills_count": 12,
      "resume_text_length": 8389,
      "resume_skills": [
        "ansible",
        "aws",
        "azure",
        "bash",
        "c",
        "ci/cd",
        "communication",
        "docker",
        "experiment design",
        "github",
        "gitlab",
        "jenkins",
        "kubernetes",
        "leadership",
        "postgresql",
        "project management",
        "puppet",
        "python",
        "ruby",
        "software development",
        "terraform"
      ],
      "job_skills": [
        "ansible",
        "aws",
        "azure",
        "bash",
        "bitbucket",
        "ci/cd",
        "cypress",
        "docker",
        "git",
        "github",
        "github actions",
        "gitlab",
        "jenkins",
        "junit",
        "kubernetes",
        "pytest",
        "python",
        "terraform"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate possesses the core DevSecOps competencies in CI/CD, container orchestration, infrastructure-as-code, scripting, Git, and cloud (AWS/Azure) and has practical leadership experience that indicates they can perform the essential job functions effectively.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "head_of_ai_emmanuel.pdf",
      "job_rank": 1,
      "job_title": "Engineering Manager - Training & Inference Platform with verification",
      "job_company": "Wayve",
      "job_id": "4263540927",
      "skill_score": 0.0,
      "semantic_score": 0.6237159967422485,
      "topic_score": 0.6202677488327026,
      "final_score": 0.6219918727874756,
      "resume_skills_count": 9,
      "job_skills_count": 9,
      "matching_skills_count": 0,
      "resume_text_length": 5755,
      "resume_skills": [
        "azure",
        "c",
        "docker",
        "flask",
        "gcp",
        "github",
        "nlp",
        "python",
        "sql"
      ],
      "job_skills": [
        "collaboration",
        "communication",
        "incident response",
        "kubernetes",
        "leadership",
        "machine learning",
        "mentoring",
        "mlflow",
        "software engineering"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks direct experience with core platform tools such as Flyte (or a comparable orchestration framework) and GPU cluster management via Kubernetes/AKS, which are essential for building and scaling the training and inference platforms described.",
      "llm_recommendations": [
        "Gain hands‑on experience with Flyte, Ray, or similar large‑scale orchestration tools through side projects or contributing to open‑source initiatives.",
        "Build a small cluster using Kubernetes/AKS to experiment with GPU provisioning, job scheduling, and fault tolerance.",
        "Document and quantify platform reliability metrics (e.g., uptime, scalability) in a portfolio or case study to demonstrate operational excellence.",
        "Pursue certifications or training focused on Kubernetes, distributed machine learning, and cloud‑scale ML infrastructure (e.g., GCP Cloud Architect, Azure AI Engineer).",
        "Highlight any past engineering leadership roles that involved platform‑level decision making or cross‑team collaboration to strengthen the manager narrative."
      ],
      "linkedin_keywords": [
        "Kubernetes",
        "Flyte",
        "GPU Scheduling",
        "Machine Learning Platform",
        "ML Ops",
        "Distributed Training",
        "Azure AKS",
        "GCP AI",
        "Cloud Architecture",
        "Scheduler Development"
      ],
      "llm_error": null
    },
    {
      "resume_file": "musa_iftikhar_resume.pdf",
      "job_rank": 1,
      "job_title": "Senior Software Engineer with verification",
      "job_company": "Global Payments Inc.",
      "job_id": "4300866362",
      "skill_score": 0.0,
      "semantic_score": 0.4987173079679801,
      "topic_score": 0.24840562045574188,
      "final_score": 0.373561464211861,
      "resume_skills_count": 2,
      "job_skills_count": 3,
      "matching_skills_count": 0,
      "resume_text_length": 8883,
      "resume_skills": [
        "c",
        "r"
      ],
      "job_skills": [
        "agile",
        "db2",
        "software development"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks the essential mainframe and COBOL/JCL/TSO/VSAM skills required for the Senior Software Engineer verification role, which are critical to performing the core job functions.",
      "llm_recommendations": [
        "Acquire hands‑on experience with IBM Mainframe technologies, such as COBOL, TSO/ISPF, and JCL, through targeted training or a volunteer project.",
        "Seek certification or coursework in mainframe development and DB2/IMS to demonstrate commitment and accelerate learning.",
        "Highlight any experience with batch processing, production support, or system monitoring to illustrate transferable problem‑solving and support skills.",
        "Network with former mainframe engineers or join relevant industry communities (e.g., IBM Tech Community) to gain mentorship and access to entry‑level opportunities.",
        "Update the résumé to include relevant coursework, projects, or internships that showcase new mainframe competencies."
      ],
      "linkedin_keywords": [
        "mainframe development",
        "COBOL",
        "DB2",
        "JCL",
        "TSO/ISPF",
        "VSAM",
        "CA7/11",
        "Endevor",
        "batch processing",
        "production support."
      ],
      "llm_error": null
    },
    {
      "resume_file": "sofia.pdf",
      "job_rank": 1,
      "job_title": "Software Engineer II, Full Stack, YouTube Channel Memberships with verification",
      "job_company": "Google",
      "job_id": "4310353060",
      "skill_score": 0.2727272727272727,
      "semantic_score": 0.6644749639458949,
      "topic_score": 0.25731420516967773,
      "final_score": 0.6079233342238446,
      "resume_skills_count": 18,
      "job_skills_count": 10,
      "matching_skills_count": 6,
      "resume_text_length": 4416,
      "resume_skills": [
        "agile",
        "c",
        "collaboration",
        "full stack development",
        "git",
        "java",
        "javascript",
        "leadership",
        "mongodb",
        "mysql",
        "node.js",
        "python",
        "ruby",
        "ruby on rails",
        "software development",
        "software engineering",
        "sql",
        "swift"
      ],
      "job_skills": [
        "c",
        "full stack development",
        "go",
        "java",
        "javascript",
        "leadership",
        "machine learning",
        "natural language processing",
        "python",
        "typescript"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate does not meet the essential requirement of at least one year of professional full‑stack development experience and proven data‑structures/algorithms work.",
      "llm_recommendations": [
        "Secure a full‑time or extended contract role in software engineering that provides at least a year of back‑end and front‑end development work.",
        "Emphasise any personal or academic projects that involved end‑to‑end feature delivery, and present them as “full‑stack” examples.",
        "Pursue additional independent learning or certifications (e.g., Google Cloud Developer, Coursera Full‑Stack Nanodegree) to demonstrate continuous skill development.",
        "Highlight the breadth of languages (Java, Python, C++, JavaScript, Node.js) and environments (Unix/Linux, mobile) to underline versatility.",
        "Network through university alumni or LinkedIn groups to uncover internship or junior engineering opportunities that can bridge the experience gap."
      ],
      "linkedin_keywords": [
        "software engineer",
        "full-stack developer",
        "back-end developer",
        "front-end developer",
        "Java",
        "Python",
        "Node.js",
        "HTML/CSS",
        "data structures",
        "algorithms",
        "Unix/Linux",
        "internship",
        "junior engineer",
        "co-op",
        "Google engineering",
        "software development intern",
        "senior software engineer",
        "full-stack development",
        "JavaScript",
        "C++ projects."
      ],
      "llm_error": null
    }
  ]
}
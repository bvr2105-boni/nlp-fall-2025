{
  "evaluation_timestamp": "20251130_043738",
  "total_resumes": 113,
  "evaluation_mode": "individual_top_job_per_resume",
  "description": "Each resume is matched with its top matching job from database (same logic as Resume Matching page)",
  "results": [
    {
      "resume_file": "2024-jeffchiarelli-resume.pdf",
      "job_title": "Senior Marketing Manager (Mandaluyong)",
      "job_company": "Filinvest Development Corporation",
      "job_id": "4332447038",
      "skill_score": 0.20833333333333334,
      "semantic_score": 0.6534501727282841,
      "topic_score": 0.6534501727282841,
      "final_score": 0.45314759500055624,
      "resume_skills_count": 21,
      "job_skills_count": 8,
      "matching_skills_count": 5,
      "resume_text_length": 6978,
      "resume_skills": [
        "a/b testing",
        "campaign optimization",
        "communication",
        "competitive positioning",
        "data analysis",
        "data analytics",
        "digital marketing",
        "email marketing",
        "google ads",
        "google analytics",
        "hubspot",
        "lambda",
        "lead generation",
        "leadership",
        "outreach",
        "ppc",
        "project management",
        "salesforce",
        "sem",
        "seo",
        "strategic planning"
      ],
      "job_skills": [
        "collaboration",
        "communication",
        "cross-functional collaboration",
        "digital marketing",
        "lead generation",
        "leadership",
        "project management",
        "stakeholder management"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks clear evidence of field marketing activation, trade merchandising, and onsite event experience required for the Senior Marketing Manager role.",
      "llm_recommendations": [
        "Highlight any event planning, trade show, or in‑store marketing projects you managed, including budgets and execution details.",
        "Add specific achievements related to physical activations, pop‑up merchandising, or on‑site campaign coordination.",
        "Emphasize collaboration with sales teams and alignment of marketing initiatives with sales funnels and conversion goals.",
        "Include any leadership or mentoring experience with junior marketing staff or cross‑functional teams.",
        "Showcase project management examples that demonstrate tight timeline delivery and stakeholder stakeholder communication."
      ],
      "linkedin_keywords": [
        "field marketing",
        "trade show management",
        "merchandising",
        "onsite activations",
        "event marketing",
        "brand strategy",
        "digital & physical campaigns",
        "sales funnel",
        "leadership",
        "stakeholder management",
        "lead generation",
        "cross‑functional collaboration."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Aisha_Kaur_20251129_061632.pdf",
      "job_title": "ML Engineer (Outside IR35, £550 per day) with verification",
      "job_company": "Oliver Bernard",
      "job_id": "4318473151",
      "skill_score": 0.35294117647058826,
      "semantic_score": 0.7064733155337999,
      "topic_score": 0.7064733155337999,
      "final_score": 0.5473838529553546,
      "resume_skills_count": 17,
      "job_skills_count": 6,
      "matching_skills_count": 6,
      "resume_text_length": 1878,
      "resume_skills": [
        "a/b testing",
        "airflow",
        "aws",
        "azure",
        "ci/cd",
        "data ingestion",
        "docker",
        "etl",
        "feature engineering",
        "gcp",
        "git",
        "gitlab",
        "kubernetes",
        "machine learning",
        "python",
        "spark",
        "sql"
      ],
      "job_skills": [
        "aws",
        "ci/cd",
        "docker",
        "gcp",
        "kubernetes",
        "machine learning"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks specific experience with GenAI models, GraphQL APIs, event‑driven streaming (e.g., Kafka), and the GenAI monitoring tools (LangSmith, Langfuse) requested in the job posting, making it an incomplete match.",
      "llm_recommendations": [
        "Highlight or add projects involving generative AI models (e.g., GPT‑style, diffusion models).",
        "Include experience building GraphQL APIs or detail any REST to GraphQL migration work.",
        "Add any event‑driven architecture or streaming use cases, such as Kafka, Pulsar, or Spark Structured Streaming.",
        "Mention tools and practices for GenAI monitoring/alerting (LangSmith, Langfuse, LLM‑as‑a‑Judge).",
        "Emphasize chat‑agent or customer‑service system integrations if available, or note experience with conversational AI."
      ],
      "linkedin_keywords": [
        "ML Engineer",
        "MLOps",
        "Generative AI",
        "GraphQL",
        "Kafka",
        "LangSmith",
        "Langfuse",
        "LLM-as-a-Judge",
        "AWS",
        "GCP",
        "Azure",
        "CI/CD",
        "Docker",
        "Kubernetes",
        "Prometheus",
        "Grafana",
        "Azure DevOps",
        "Apache Airflow."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Aisha_Patel_20251129_055725.pdf",
      "job_title": "Data Integration Engineer with verification",
      "job_company": "Boot Barn",
      "job_id": "4319462092",
      "skill_score": 0.16666666666666666,
      "semantic_score": 0.6430913805961609,
      "topic_score": 0.6430913805961609,
      "final_score": 0.42870025932788847,
      "resume_skills_count": 16,
      "job_skills_count": 19,
      "matching_skills_count": 5,
      "resume_text_length": 1912,
      "resume_skills": [
        "a/b testing",
        "agile",
        "azure",
        "ci/cd",
        "data analysis",
        "decision making",
        "git",
        "github",
        "machine learning",
        "power bi",
        "python",
        "r",
        "scrum",
        "snowflake",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "azure",
        "ci/cd",
        "communication",
        "data integration",
        "elt",
        "etl",
        "git",
        "mongodb",
        "problem solving",
        "python",
        "reporting",
        "scala",
        "spark",
        "sql",
        "stakeholder management",
        "teamwork",
        "time management",
        "user stories",
        "validation rules"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate's background focuses largely on data science and ML, lacking the specific ETL, data‑vault modeling, Spark, SSIS, Azure Data Factory, and cloud integration experience required for the Data Integration Engineer role.",
      "llm_recommendations": [
        "Highlight any ETL/ELT projects, explicitly naming tools like SSIS, Azure Data Factory, Azure Data Lake and describing batch/streaming pipeline design and deployment.",
        "Add concrete examples of using Spark/Scala for large‑scale data processing, CDC implementations, and schema evolution automation.",
        "Include experience with relational databases (MS SQL Server), NoSQL (MongoDB aggregation), and data‑vault 2.0 modeling—add any certifications or training.",
        "Demonstrate use of PowerShell/.NET utilities, Azure DevOps Pipelines, and monitoring (Grafana, Azure Monitor) for job orchestration and metrics.",
        "Update résumé to mention specific cloud-based ETL deployments (2+ years in Azure or AWS), and describe integration with third‑party APIs."
      ],
      "linkedin_keywords": [
        "data engineer",
        "ETL",
        "SSIS",
        "Azure Data Factory",
        "Azure DevOps",
        "Spark",
        "Scala",
        "Data Vault",
        "CDC",
        "PowerShell",
        "MongoDB",
        "MS SQL Server",
        "cloud ETL",
        "Azure Data Engineer Associate."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Aisha_Thompson_20251129_062821.pdf",
      "job_title": "Data Scientist, Watchlist",
      "job_company": "RemoteHunter",
      "job_id": "4319177929",
      "skill_score": 0.3684210526315789,
      "semantic_score": 0.596553159376415,
      "topic_score": 0.596553159376415,
      "final_score": 0.4938937113412387,
      "resume_skills_count": 15,
      "job_skills_count": 11,
      "matching_skills_count": 7,
      "resume_text_length": 1677,
      "resume_skills": [
        "airflow",
        "aws",
        "azure",
        "data visualization",
        "etl",
        "hadoop",
        "leadership",
        "machine learning",
        "pandas",
        "python",
        "r",
        "spark",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "adaptability",
        "aws",
        "databricks",
        "feature engineering",
        "hadoop",
        "machine learning",
        "python",
        "r",
        "spark",
        "sql",
        "xgboost"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate’s 5‑year data science background with expertise in Python, R, SQL, Hadoop/Spark, AWS SageMaker, and end‑to‑end model deployment aligns well with the role’s requirements.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Alexandra_Reyes_20251129_065105.pdf",
      "job_title": "DevSecOps",
      "job_company": "QualiZeal",
      "job_id": "4332406515",
      "skill_score": 0.34615384615384615,
      "semantic_score": 0.7498737573623657,
      "topic_score": 0.7498737573623657,
      "final_score": 0.5681997973185319,
      "resume_skills_count": 23,
      "job_skills_count": 12,
      "matching_skills_count": 9,
      "resume_text_length": 3046,
      "resume_skills": [
        "a/b testing",
        "airflow",
        "aws",
        "bash",
        "ci/cd",
        "data ingestion",
        "docker",
        "gcp",
        "git",
        "github",
        "gradient boosting",
        "jenkins",
        "kubernetes",
        "machine learning",
        "neural networks",
        "pandas",
        "power bi",
        "python",
        "random forest",
        "s3",
        "spark",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "aws",
        "azure",
        "bash",
        "bitbucket",
        "ci/cd",
        "docker",
        "git",
        "github",
        "gitlab",
        "jenkins",
        "kubernetes",
        "python"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate demonstrates strong DevOps capabilities—Jenkins, Actions, Docker, Kubernetes, Helm, Terraform, Ansible, CI/CD workflows, AWS, Git, scripting, and monitoring—aligning well with the core responsibilities of the DevSecOps role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Alkhatib-Khaled-Resume.pdf",
      "job_title": "Technical Lead - Java",
      "job_company": "Soho Square Solutions",
      "job_id": "4318099038",
      "skill_score": 0.26666666666666666,
      "semantic_score": 0.7024603244709632,
      "topic_score": 0.7024603244709632,
      "final_score": 0.5063531784590297,
      "resume_skills_count": 8,
      "job_skills_count": 11,
      "matching_skills_count": 4,
      "resume_text_length": 3976,
      "resume_skills": [
        "aws",
        "bitbucket",
        "git",
        "github",
        "go",
        "javascript",
        "postgresql",
        "sql"
      ],
      "job_skills": [
        "aws",
        "ci/cd",
        "collaboration",
        "docker",
        "git",
        "java",
        "javascript",
        "jenkins",
        "mongodb",
        "sql",
        "typescript"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks the extensive Java, Spring Boot, AWS, and leadership experience required for the Technical Lead role.",
      "llm_recommendations": [
        "Add or emphasize any Java/Spring Boot projects and experience, including backend orchestration and API development, to meet the 8+ year expectation.",
        "Highlight hands‑on experience with CI/CD tools (Jenkins, Git, Artifactory) and Docker/EKS on AWS, and demonstrate build automation and automated testing practices.",
        "Include specific data‑base work with both relational (SQL Server, PostgreSQL) and non‑relational (MongoDB) databases, and provide examples of architecture and performance tuning.",
        "Update the resume to state location (e.g., Toronto) and mention willingness to relocate or remote availability, or seek local positions if onsite in McLean, VA is required.",
        "Showcase any team‑leading or mentorship roles, project management responsibilities, and delivery of on‑time, high‑quality software to demonstrate leadership capabilities."
      ],
      "linkedin_keywords": [
        "Java",
        "Spring Boot",
        "Angular",
        "AWS",
        "CI/CD",
        "Jenkins",
        "Git",
        "Docker",
        "EKS",
        "PostgreSQL",
        "MongoDB",
        "Team Leadership",
        "Full Stack Developer",
        "DevOps",
        "Security Engineering",
        "LDAP",
        "Build Automation",
        "API Development",
        "Cloud Integration",
        "Automation Testing"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Amber-Gaston-Resume-4.25.pdf",
      "job_title": "Senior Backend Engineer with verification",
      "job_company": "eBay",
      "job_id": "4316722527",
      "skill_score": 0.037037037037037035,
      "semantic_score": 0.5831503272056621,
      "topic_score": 0.5831503272056621,
      "final_score": 0.33739934662978077,
      "resume_skills_count": 15,
      "job_skills_count": 13,
      "matching_skills_count": 1,
      "resume_text_length": 5675,
      "resume_skills": [
        "collaboration",
        "customer retention",
        "digital marketing",
        "email marketing",
        "google ads",
        "google analytics",
        "hubspot",
        "jira",
        "mailchimp",
        "procurement",
        "project management",
        "salesforce",
        "seo",
        "tableau",
        "trend analysis"
      ],
      "job_skills": [
        "agile",
        "aws",
        "azure",
        "communication",
        "github",
        "java",
        "javascript",
        "jenkins",
        "jira",
        "machine learning",
        "scala",
        "teamwork",
        "typescript"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate’s experience and education focus on marketing and e‑commerce rather than the Java/Kotlin/Scala backend development skills required for the role.",
      "llm_recommendations": [
        "Highlight any programming or backend development experience, even if informal or in side projects.",
        "Obtain or emphasize courses, certifications, or projects involving Java, Kotlin, Scala, and Spring Boot.",
        "Showcase experience with cloud services (AWS, Azure), CI/CD pipelines (Jenkins, Maven), and API technologies (REST, GraphQL).",
        "Add any work or contributions related to distributed systems, Kafka, or similar data streaming technologies.",
        "Include a section on core computer science fundamentals such as data structures, algorithms, and concurrent programming."
      ],
      "linkedin_keywords": [
        "Java",
        "Kotlin",
        "Scala",
        "Spring Boot",
        "backend development",
        "RESTful APIs",
        "GraphQL",
        "AWS",
        "Azure",
        "Jenkins",
        "distributed systems",
        "Kafka",
        "CI/CD",
        "Maven",
        "Jira",
        "software engineering",
        "cloud architecture."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Amina_Diop_20251129_140929.pdf",
      "job_title": "Sr. Analyst, Digital Initiatives",
      "job_company": "Toast",
      "job_id": "4331625169",
      "skill_score": 0.18518518518518517,
      "semantic_score": 0.6127252578735395,
      "topic_score": 0.6127252578735395,
      "final_score": 0.42033222516378005,
      "resume_skills_count": 20,
      "job_skills_count": 12,
      "matching_skills_count": 5,
      "resume_text_length": 2209,
      "resume_skills": [
        "agile",
        "airflow",
        "data analytics",
        "data ingestion",
        "etl",
        "git",
        "logistics",
        "looker",
        "machine learning",
        "numpy",
        "pandas",
        "python",
        "redshift",
        "reporting",
        "scrum",
        "snowflake",
        "spark",
        "sql",
        "tableau",
        "validation rules"
      ],
      "job_skills": [
        "communication",
        "data analysis",
        "data analytics",
        "data visualization",
        "power bi",
        "presentation skills",
        "python",
        "r",
        "reporting",
        "sql",
        "stakeholder management",
        "tableau"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate has 5 years of experience and lacks several key requirements such as 7+ years in analytics, Power BI skills, deep experience in pos/sales transactional data, and explicit demonstration of stakeholder management and business‑centric communication.",
      "llm_recommendations": [
        "Highlight any additional years of relevant experience or relevant certifications to bridge the 7‑year gap.",
        "Add specific Power BI or Excel projects and quantify insights produced from transactional sales data.",
        "Emphasize stakeholder management by detailing collaboration with marketing, operations, and third‑party delivery partners.",
        "Showcase English language proficiency and presentation experience, including any client‑facing meetings or dashboards shared with senior leadership.",
        "Tailor the summary to stress business‑driven insight generation and digital initiative support, including any kiosk, loyalty, or delivery channel work."
      ],
      "linkedin_keywords": [
        "Sr Analyst",
        "Digital Initiatives",
        "Business Intelligence",
        "SQL",
        "Tableau",
        "Power BI",
        "Stakeholder Management",
        "Data Visualization",
        "Python",
        "Operations Analytics",
        "Customer Experience Analytics",
        "Pos Data Analysis",
        "Transactional Data",
        "KPI Dashboard",
        "Delivery Optimization",
        "Loyalty Programs",
        "EMEA Data Insights",
        "Analytics Leadership",
        "Excel Advanced",
        "Digital Projects"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Amir_Boroumand.pdf",
      "job_title": "Sr. Full Stack Developer with verification",
      "job_company": "Motion Recruitment",
      "job_id": "4331373865",
      "skill_score": 0.52,
      "semantic_score": 0.7035752534866333,
      "topic_score": 0.7035752534866333,
      "final_score": 0.6209663894176484,
      "resume_skills_count": 18,
      "job_skills_count": 20,
      "matching_skills_count": 13,
      "resume_text_length": 8007,
      "resume_skills": [
        "agile",
        "aws",
        "c#",
        "ci/cd",
        "docker",
        "github",
        "java",
        "javascript",
        "jenkins",
        "kubernetes",
        "oracle",
        "postgresql",
        "python",
        "s3",
        "sap",
        "scrum",
        "sql",
        "typescript"
      ],
      "job_skills": [
        "agile",
        "aws",
        "c#",
        "c++",
        "communication",
        "docker",
        "ec2",
        "git",
        "github",
        "gitlab",
        "java",
        "javascript",
        "kubernetes",
        "mongodb",
        "nosql",
        "oracle",
        "python",
        "s3",
        "scrum",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume shows strong Java/Spring Boot and AWS experience but lacks key dev‑ops tools (CDK/Tekton, JBoss) and the specific test‑automation frameworks and travel‑industry context required by the posting.",
      "llm_recommendations": [
        "Add explicit experience (or certification) with AWS CDK and/or Tekton for CI/CD pipeline writing.",
        "Highlight any work with JBoss or similar Java EE application servers.",
        "Include UI automation examples using Selenium or Cypress and mention TDD/BDD practices with Gherkin syntax.",
        "Mention use of AWS CloudFormation or CodePipeline and any exposure to Red Hat OpenShift if applicable.",
        "If possible, add a brief project or volunteer experience related to travel‑industry systems or security compliance."
      ],
      "linkedin_keywords": [
        "Java",
        "Spring Boot",
        "Cloud Native",
        "AWS",
        "AWS CDK",
        "Tekton",
        "DevOps",
        "CI/CD",
        "JBoss",
        "Kubernetes",
        "Docker",
        "Terraform",
        "Kafka",
        "Serverless",
        "REST API",
        "Microservices",
        "Test Automation",
        "Selenium",
        "Cypress",
        "TDD",
        "BDD",
        "Gherkin",
        "Agile",
        "Frontend",
        "React",
        "Python",
        "AWS CodePipeline",
        "Red Hat OpenShift",
        "CloudFormation"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Amira_Khatri_20251129_061557.pdf",
      "job_title": "Desenvolvedor Front End Vue.js | UI",
      "job_company": "innolevels",
      "job_id": "4317267515",
      "skill_score": 0.1875,
      "semantic_score": 0.29290567594204453,
      "topic_score": 0.29290567594204453,
      "final_score": 0.2454731217681245,
      "resume_skills_count": 15,
      "job_skills_count": 4,
      "matching_skills_count": 3,
      "resume_text_length": 1787,
      "resume_skills": [
        "a/b testing",
        "aws",
        "azure",
        "bert",
        "collaboration",
        "data visualization",
        "docker",
        "feature engineering",
        "git",
        "kubernetes",
        "named entity recognition",
        "nlp",
        "python",
        "sentiment analysis",
        "text classification"
      ],
      "job_skills": [
        "aws",
        "git",
        "gitlab",
        "kubernetes"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume focuses on NLP and backend engineering, with no experience or skills related to Vue.js, front‑end UI development, or the required JavaScript ecosystem.",
      "llm_recommendations": [
        "Add front‑end development experience, including projects built with Vue.js, JavaScript, HTML, and CSS.",
        "Highlight UI/UX work or any experience using Vuetify, component libraries, or design systems.",
        "Include any relevant certifications or coursework in front‑end frameworks (Vue.js, React, Angular) and mention experience with Node.js or Spring Boot.",
        "Showcase cloud deployment skills specific to AWS or Kubernetes, and detail use of Git/GitLab and Linux environments.",
        "Emphasize strong communication skills and English fluency, perhaps with examples of client interaction or documentation."
      ],
      "linkedin_keywords": [
        "vue.js",
        "front-end developer",
        "UI developer",
        "javascript",
        "css",
        "html",
        "vuetify",
        "node.js",
        "angular",
        "springboot",
        "aws",
        "kubernetes",
        "git",
        "linux",
        "software engineer",
        "front-end engineer"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Amira_Tan_20251129_060511.pdf",
      "job_title": "AI/ML Software Engineer with verification",
      "job_company": "Guidehouse",
      "job_id": "4332300185",
      "skill_score": 0.17391304347826086,
      "semantic_score": 0.7573051678331331,
      "topic_score": 0.7573051678331331,
      "final_score": 0.4947787118734406,
      "resume_skills_count": 14,
      "job_skills_count": 13,
      "matching_skills_count": 4,
      "resume_text_length": 2706,
      "resume_skills": [
        "aws",
        "ci/cd",
        "data analysis",
        "data pipeline",
        "decision making",
        "docker",
        "feature engineering",
        "gcp",
        "kubernetes",
        "machine learning",
        "python",
        "reporting",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "aws",
        "communication",
        "deep learning",
        "javascript",
        "machine learning",
        "numpy",
        "oracle",
        "outreach",
        "pandas",
        "postgresql",
        "python",
        "sql",
        "typescript"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks key requirements such as public trust clearance, JavaScript/TypeScript expertise, and specific experience with RAG systems, chatbots, and advanced AI deployment pipelines.",
      "llm_recommendations": [
        "Highlight any security clearance or experience working with federal clients to address the public trust requirement.",
        "Add explicit JavaScript or TypeScript projects, indicating full-stack development or API integration.",
        "Detail experience building retrieval‑augmented generation (RAG) systems, conversational AI/chatbots, or agentic AI workflows.",
        "Include REST API development and deployment details, especially using AWS services and IaC (e.g., CloudFormation or Terraform).",
        "Mention use of search technologies (Elasticsearch or OpenSearch), relational databases (PostgreSQL, Oracle), and any GPU or CUDA optimization work."
      ],
      "linkedin_keywords": [
        "Python",
        "TensorFlow",
        "PyTorch",
        "scikit-learn",
        "SQL",
        "AWS",
        "Kubernetes",
        "Docker",
        "MLOps",
        "RAG",
        "chatbots",
        "REST APIs",
        "Elasticsearch",
        "JavaScript",
        "TypeScript",
        "public trust clearance",
        "federal AI solutions",
        "cloud infrastructure",
        "AI pipelines"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Amit_Sharma.pdf",
      "job_title": "Business Development Intern",
      "job_company": "PixelSoft",
      "job_id": "4319228004",
      "skill_score": 0.11764705882352941,
      "semantic_score": 0.542534579626422,
      "topic_score": 0.542534579626422,
      "final_score": 0.35133519526512036,
      "resume_skills_count": 16,
      "job_skills_count": 3,
      "matching_skills_count": 2,
      "resume_text_length": 12965,
      "resume_skills": [
        "account management",
        "communication",
        "competitive analysis",
        "data transformation",
        "digital marketing",
        "mailchimp",
        "market research",
        "process optimization",
        "procurement",
        "reporting",
        "salesforce",
        "sap",
        "seo",
        "sql",
        "strategic planning",
        "vendor management"
      ],
      "job_skills": [
        "communication",
        "lead generation",
        "market research"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate’s extensive experience in market research, lead generation, customer service, data analysis, communication, teamwork, digital technology solutions and relevant education make her an excellent fit for the Business Development Intern role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Ana_Martinez_20251129_064703.pdf",
      "job_title": "Data Analyst",
      "job_company": "Wyndham Hotels & Resorts, Inc.",
      "job_id": "981851940397954",
      "skill_score": 0.3333333333333333,
      "semantic_score": 0.7035224051333193,
      "topic_score": 0.7035224051333193,
      "final_score": 0.5369373228233256,
      "resume_skills_count": 11,
      "job_skills_count": 5,
      "matching_skills_count": 4,
      "resume_text_length": 1766,
      "resume_skills": [
        "a/b testing",
        "collaboration",
        "data visualization",
        "etl",
        "inventory management",
        "power bi",
        "python",
        "r",
        "reporting",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "data analysis",
        "power bi",
        "reporting",
        "sql",
        "tableau"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "This candidate’s 8 years of experience using Tableau, Power BI, SQL, and data visualization to create actionable dashboards and insights directly matches the job requirements.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Ari_Patel_20251129_063351.pdf",
      "job_title": "Data Engineer AWS - Hybrid in NY",
      "job_company": "TechTriad",
      "job_id": "4317962527",
      "skill_score": 0.18181818181818182,
      "semantic_score": 0.7512963562431243,
      "topic_score": 0.7512963562431243,
      "final_score": 0.49503117775190014,
      "resume_skills_count": 13,
      "job_skills_count": 13,
      "matching_skills_count": 4,
      "resume_text_length": 1827,
      "resume_skills": [
        "airflow",
        "aws",
        "data analysis",
        "data cleaning",
        "data ingestion",
        "data pipeline",
        "etl",
        "numpy",
        "pandas",
        "python",
        "snowflake",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "airflow",
        "aws",
        "ci/cd",
        "databricks",
        "java",
        "kubernetes",
        "lambda",
        "machine learning",
        "pyspark",
        "python",
        "s3",
        "scala",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate has 5 years of experience and lacks several critical skill areas (Databricks, enterprise feature store, advanced AWS services, and senior-level responsibilities) required for this senior data engineer role.",
      "llm_recommendations": [
        "Highlight any experience with Databricks, Delta Lake, and Spark that may have been omitted.",
        "Expand on AWS services used (S3, Glue, EMR, Lambda, Kinesis) and demonstrate scale and performance optimization.",
        "Include detail on any feature store implementation, MLflow, Unity Catalog, or Kubernetes usage.",
        "Emphasize senior or lead responsibilities, such as strategy, architecture design, or mentoring, to meet the 7‑10 year senior level expectation.",
        "Add relevant certifications or training (e.g., AWS Certified Data Analytics, Databricks Certification) to strengthen the cloud and data engineering credentials."
      ],
      "linkedin_keywords": [
        "data engineer",
        "AWS",
        "Databricks",
        "Spark",
        "Delta Lake",
        "MLflow",
        "feature store",
        "Airflow",
        "Kubernetes",
        "S3",
        "Glue",
        "EMR",
        "Kinesis",
        "Lambda",
        "data pipelines",
        "big data",
        "python",
        "sql"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Ariana_Patel_20251129_064307.pdf",
      "job_title": "Product Designer, ChatGPT",
      "job_company": "ExecutivePlacements.com",
      "job_id": "4332440377",
      "skill_score": 0.0,
      "semantic_score": 0.5240644245224514,
      "topic_score": 0.5240644245224514,
      "final_score": 0.28823543348734826,
      "resume_skills_count": 14,
      "job_skills_count": 3,
      "matching_skills_count": 0,
      "resume_text_length": 2806,
      "resume_skills": [
        "bert",
        "data pipeline",
        "deep learning",
        "docker",
        "git",
        "kubernetes",
        "leadership",
        "machine learning",
        "natural language processing",
        "nlp",
        "python",
        "sentiment analysis",
        "spark",
        "sql"
      ],
      "job_skills": [
        "communication",
        "product management",
        "user research"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume focuses on NLP engineering and deep learning, lacking the product design, UX/UI, and design‑system experience required for the Product Designer role.",
      "llm_recommendations": [
        "Highlight any experience working with product designers or contributing to design research, such as user interviews, personas, or usability tests.",
        "Include a portfolio section that showcases design projects, wireframes, prototypes, and visual design work, preferably with links or screenshots.",
        "Add familiarity with design tools (Figma, Sketch, Adobe XD) and mention any design system contributions or prototyping work.",
        "Emphasize communication and storytelling abilities, detailing collaboration with cross‑functional teams and presentation of design concepts to stakeholders.",
        "Provide concrete examples of shipping design‑focused products or features and the impact on user experience or business metrics."
      ],
      "linkedin_keywords": [
        "product design",
        "UX design",
        "interaction design",
        "design system",
        "Figma",
        "Sketch",
        "Adobe XD",
        "user research",
        "prototype",
        "design thinking",
        "visual design",
        "stakeholder collaboration"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Arianna_K_Patel_20251129_064943.pdf",
      "job_title": "Data and Production Support Analyst with verification",
      "job_company": "VanEck",
      "job_id": "4310751775",
      "skill_score": 0.23809523809523808,
      "semantic_score": 0.6207086813872278,
      "topic_score": 0.6207086813872278,
      "final_score": 0.4485326319058324,
      "resume_skills_count": 14,
      "job_skills_count": 12,
      "matching_skills_count": 5,
      "resume_text_length": 2392,
      "resume_skills": [
        "budgeting",
        "data analysis",
        "data cleaning",
        "data visualization",
        "leadership",
        "machine learning",
        "operational efficiency",
        "power bi",
        "python",
        "r",
        "reporting",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "business analysis",
        "communication",
        "critical thinking",
        "data analysis",
        "data visualization",
        "multitasking",
        "operational efficiency",
        "problem solving",
        "reporting",
        "sql",
        "teamwork",
        "time management"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks the required financial industry data operations experience, a bachelor’s degree, and domain expertise in investment data and market data management.",
      "llm_recommendations": [
        "Highlight any previous work with financial, investment, or market data sets, including security master data, pricing data, and corporate actions.",
        "Add any experience or coursework demonstrating knowledge of financial workflows, portfolio systems, and data quality governance in a finance context.",
        "Include a formal bachelor’s degree or equivalent coursework in Computer Science, Information Systems, or Finance to meet educational requirements.",
        "Detail specific ETL or data ingestion projects, especially those involving large datasets, troubleshooting pipelines, and ensuring data integrity across systems.",
        "Incorporate certifications or training in SQL Server, Power BI, or financial data platforms (e.g., Bloomberg, S&P Global) to showcase technical alignment with the role."
      ],
      "linkedin_keywords": [
        "data operations analyst",
        "financial data analyst",
        "investment data",
        "market data",
        "SQL",
        "Power BI",
        "data ingestion",
        "ETL",
        "data quality",
        "portfolio systems",
        "security master data",
        "corporate actions",
        "pricing data."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Arielle_Martinez_20251129_063312.pdf",
      "job_title": "Data Analyst with verification",
      "job_company": "Randstad Digital Americas",
      "job_id": "4317995592",
      "skill_score": 0.2777777777777778,
      "semantic_score": 0.6629772989804031,
      "topic_score": 0.6629772989804031,
      "final_score": 0.48963751443922177,
      "resume_skills_count": 18,
      "job_skills_count": 5,
      "matching_skills_count": 5,
      "resume_text_length": 2308,
      "resume_skills": [
        "a/b testing",
        "airflow",
        "data analysis",
        "data cleaning",
        "data visualization",
        "etl",
        "forecasting",
        "leadership",
        "machine learning",
        "numpy",
        "pandas",
        "power bi",
        "python",
        "r",
        "reporting",
        "snowflake",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "data analysis",
        "etl",
        "power bi",
        "sql",
        "tableau"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate’s experience with SQL scripting, ETL pipeline development, Power BI/Tableau dashboards, and data validation aligns strongly with the Data Analyst with Verification role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Arjun_Patel_20251129_063128.pdf",
      "job_title": "Senior Engineer Data Science and Engineering with verification",
      "job_company": "TBO.COM",
      "job_id": "4319336877",
      "skill_score": 0.125,
      "semantic_score": 0.44574460063842625,
      "topic_score": 0.44574460063842625,
      "final_score": 0.30140953035113444,
      "resume_skills_count": 14,
      "job_skills_count": 13,
      "matching_skills_count": 3,
      "resume_text_length": 2083,
      "resume_skills": [
        "a/b testing",
        "agile",
        "data analytics",
        "deep learning",
        "feature engineering",
        "hadoop",
        "machine learning",
        "pyspark",
        "python",
        "r",
        "spark",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "athena",
        "aws",
        "ci/cd",
        "data ingestion",
        "etl",
        "git",
        "lambda",
        "leadership",
        "pyspark",
        "s3",
        "scala",
        "spark",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks explicit evidence of hands‑on experience with the required data‑engineering stack (AWS Glue, Spark/Scala, HDFS, Hudi, S3, Athena, CI/CD, and version control) and detailed data‑pipeline design.",
      "llm_recommendations": [
        "Highlight any AWS-related work (Glue, S3, Athena, Lambda, IAM, EMR) and mention the specific services used.",
        "Demonstrate experience building and managing ETL workflows, data lake architectures, or data ingestion pipelines, including use of HDFS/Hudi.",
        "Add details on version control (Git) and CI/CD practices applied to data or ML pipelines.",
        "Include certifications or training in cloud platforms (AWS, Azure) and big‑data technologies.",
        "Quantify impact of data‑engineering projects (e.g., reduced processing time, improved data quality)."
      ],
      "linkedin_keywords": [
        "Data Engineer",
        "Senior Data Engineer",
        "Spark",
        "PySpark",
        "Apache Hudi",
        "AWS Glue",
        "AWS S3",
        "Athena",
        "Hadoop",
        "HDFS",
        "ETL",
        "SQL",
        "Data Modeling",
        "CI/CD",
        "Cloud Computing",
        "Big Data",
        "Python",
        "DevOps"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Arjun_Raghavan_20251129_063831.pdf",
      "job_title": "Senior DevOps Engineer with verification",
      "job_company": "Munich Re",
      "job_id": "4311393243",
      "skill_score": 0.36,
      "semantic_score": 0.7599889492982013,
      "topic_score": 0.7599889492982013,
      "final_score": 0.5799939221140107,
      "resume_skills_count": 21,
      "job_skills_count": 13,
      "matching_skills_count": 9,
      "resume_text_length": 2750,
      "resume_skills": [
        "airflow",
        "aws",
        "azure",
        "bash",
        "ci/cd",
        "data analysis",
        "data analytics",
        "docker",
        "feature engineering",
        "forecasting",
        "gcp",
        "github",
        "gitlab",
        "jenkins",
        "kubernetes",
        "machine learning",
        "python",
        "s3",
        "sales forecasting",
        "sql",
        "xgboost"
      ],
      "job_skills": [
        "aws",
        "azure",
        "ci/cd",
        "collaboration",
        "communication",
        "docker",
        "gcp",
        "github",
        "javascript",
        "kubernetes",
        "machine learning",
        "python",
        "typescript"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks the required bachelor’s degree and explicit experience in cybersecurity and networking fundamentals stated in the job posting.",
      "llm_recommendations": [
        "Obtain or highlight a bachelor’s degree or equivalent certifications (e.g., AWS Certified Advanced Networking, CompTIA Security+).",
        "Add certifications or coursework demonstrating knowledge of cybersecurity fundamentals (information security, identity, application, network).",
        "Include experience with cloud networking (VPC, subnets, load balancers) or networking tools to satisfy the networking fundamentals requirement.",
        "Highlight any incident response or security incident handling experience or roles that involved coordinating cross‑functional response teams.",
        "Mention experience with additional orchestration platforms such as OpenShift or crossplane to strengthen IaC and orchestration credentials."
      ],
      "linkedin_keywords": [
        "Senior DevOps Engineer",
        "IaC",
        "Terraform",
        "Kubernetes",
        "CI/CD",
        "Cloud Architecture",
        "AWS",
        "GCP",
        "Azure",
        "Python",
        "Monitoring",
        "Prometheus",
        "Grafana",
        "Incident Response",
        "Cloud Security",
        "Network Engineering",
        "OpenShift",
        "Crossplane",
        "DevSecOps"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Arun_Venkatesh_20251129_060257.pdf",
      "job_title": "Senior Lead Software Engineer - SRE/Databricks with verification",
      "job_company": "JPMorganChase",
      "job_id": "4331849522",
      "skill_score": 0.22580645161290322,
      "semantic_score": 0.6798599982011327,
      "topic_score": 0.6798599982011327,
      "final_score": 0.47553590223642944,
      "resume_skills_count": 25,
      "job_skills_count": 13,
      "matching_skills_count": 7,
      "resume_text_length": 2552,
      "resume_skills": [
        "agile",
        "airflow",
        "aws",
        "bigquery",
        "ci/cd",
        "data ingestion",
        "data visualization",
        "decision making",
        "docker",
        "etl",
        "gcp",
        "gitlab",
        "kubernetes",
        "looker",
        "power bi",
        "python",
        "redshift",
        "reporting",
        "s3",
        "scala",
        "scrum",
        "spark",
        "sql",
        "tableau",
        "trend analysis"
      ],
      "job_skills": [
        "agile",
        "aws",
        "ci/cd",
        "communication",
        "databricks",
        "docker",
        "java",
        "kubernetes",
        "machine learning",
        "mapreduce",
        "python",
        "root cause analysis",
        "spark"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks explicit experience with AWS Databricks platform administration, SRE principles, and incident management required for this senior lead role.",
      "llm_recommendations": [
        "Highlight any experience administering or developing on Databricks, including cluster management, job scheduling, and data lake integration.",
        "Detail SRE-related duties such as defining SLIs/SLOs, managing error budgets, and performing incident response or root‑cause analyses.",
        "Specify use of enterprise monitoring tools (Prometheus, Grafana, Datadog, etc.) and automation frameworks in production environments.",
        "If applicable, add experience with Java or other JVM languages used for core service development, and certifications in AWS, Databricks, or SRE practices.",
        "Emphasize leadership of larger teams or cross‑functional projects, and any quantifiable impact on reliability or uptime."
      ],
      "linkedin_keywords": [
        "senior software engineer",
        "lead engineer",
        "SRE",
        "Databricks",
        "AWS",
        "Terraform",
        "Apache Spark",
        "Java",
        "CI/CD",
        "incident management",
        "monitoring",
        "DevOps",
        "distributed systems",
        "large‑scale data processing",
        "automation",
        "data pipelines",
        "docker",
        "kubernetes"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Asha_Patel_20251129_063921.pdf",
      "job_title": "Data Engineer AWS - Hybrid in NY",
      "job_company": "TechTriad",
      "job_id": "4317962527",
      "skill_score": 0.28,
      "semantic_score": 0.7373004868864486,
      "topic_score": 0.7373004868864486,
      "final_score": 0.5315152677875468,
      "resume_skills_count": 19,
      "job_skills_count": 13,
      "matching_skills_count": 7,
      "resume_text_length": 2535,
      "resume_skills": [
        "airflow",
        "aws",
        "azure",
        "ci/cd",
        "customer segmentation",
        "deep learning",
        "docker",
        "feature engineering",
        "gcp",
        "git",
        "jenkins",
        "kubernetes",
        "machine learning",
        "pandas",
        "power bi",
        "python",
        "spark",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "airflow",
        "aws",
        "ci/cd",
        "databricks",
        "java",
        "kubernetes",
        "lambda",
        "machine learning",
        "pyspark",
        "python",
        "s3",
        "scala",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate’s experience is strong in MLOps and cloud CI/CD but lacks explicit Databricks, Delta Lake, Glue, EMR, Lambda, Kinesis, and feature‑store work required for the role.",
      "llm_recommendations": [
        "Highlight any past projects involving Databricks, Delta Lake, and SQL for data processing.",
        "Add specific experience with building and managing centralized feature stores or feature-flag systems.",
        "Detail usage of AWS services such as Glue, EMR, Lambda, and Kinesis in data pipelines.",
        "Include mention of distributed systems knowledge (e.g., Spark cluster architecture, tuning, and scaling).",
        "Showcase familiarity with Unity Catalog, Kubernetes, and observability tools in a data‑engineering context."
      ],
      "linkedin_keywords": [
        "data engineer",
        "AWS",
        "Databricks",
        "PySpark",
        "Delta Lake",
        "feature store",
        "CI/CD",
        "Airflow",
        "Kubernetes",
        "MLflow"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Aswin.pdf",
      "job_title": "Data Analyst Junior con foco en herramientas modernas (Python, Power BI...)",
      "job_company": "Evolve",
      "job_id": "4332450092",
      "skill_score": 0.13333333333333333,
      "semantic_score": 0.36971771717071533,
      "topic_score": 0.36971771717071533,
      "final_score": 0.26334474444389344,
      "resume_skills_count": 13,
      "job_skills_count": 4,
      "matching_skills_count": 2,
      "resume_text_length": 11180,
      "resume_skills": [
        "aws",
        "azure",
        "c#",
        "c++",
        "ci/cd",
        "docker",
        "gitlab",
        "java",
        "javascript",
        "kubernetes",
        "python",
        "r",
        "sql"
      ],
      "job_skills": [
        "pandas",
        "power bi",
        "python",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks recent, relevant data‑analysis experience and mentions of the modern tools (Python, Power BI, SQL, Pandas, Jupyter, scikit‑learn) that the junior role requires.",
      "llm_recommendations": [
        "Add a dedicated “Data Analyst / Data Science Projects” section highlighting hands‑on use of Python, Pandas, SQL, Jupyter, scikit‑learn, and Power BI.",
        "Include any recent courses, certifications, or MOOCs related to data analytics or business intelligence.",
        "Emphasize teamwork, communication, and learning agility with concrete examples from recent projects.",
        "Showcase data‑visualization and predictive‑modeling deliverables that align with “predicción y análisis exploratorio.”",
        "Downplay very senior roles or reframe experience at the level of a junior analyst (e.g., “Mentored teams on data‑analysis techniques”)."
      ],
      "linkedin_keywords": [
        "data analyst",
        "python",
        "power bi",
        "sql",
        "pandas",
        "data visualization",
        "jupyter",
        "scikit-learn",
        "data science",
        "machine learning."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Aswin_A_van_den_Berg.pdf",
      "job_title": "Data Analyst Junior con foco en herramientas modernas (Python, Power BI...)",
      "job_company": "Evolve",
      "job_id": "4332450092",
      "skill_score": 0.13333333333333333,
      "semantic_score": 0.36971771717071533,
      "topic_score": 0.36971771717071533,
      "final_score": 0.26334474444389344,
      "resume_skills_count": 13,
      "job_skills_count": 4,
      "matching_skills_count": 2,
      "resume_text_length": 11180,
      "resume_skills": [
        "aws",
        "azure",
        "c#",
        "c++",
        "ci/cd",
        "docker",
        "gitlab",
        "java",
        "javascript",
        "kubernetes",
        "python",
        "r",
        "sql"
      ],
      "job_skills": [
        "pandas",
        "power bi",
        "python",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate’s extensive senior software engineering background and lack of relevant data‑analysis, Power BI, and introductory data science experience make him a poor fit for the junior, training‑focused role.",
      "llm_recommendations": [
        "Add a dedicated “Data Science Projects” section listing hands‑on projects using Python, pandas, SQL, Jupyter, and Power BI, even if they are academic or personal.",
        "Highlight any coursework, certifications, or workshops in data analysis, machine learning (scikit‑learn), and business intelligence tools.",
        "Emphasize experience with exploratory data analysis, visualization, and predictive modeling, using concrete metrics or results.",
        "Re‑order the skills section to prioritize modern data‑analysis tools (Python, SQL, pandas, Power BI, Jupyter, scikit‑learn) over legacy or less relevant programming languages.",
        "Update the education section to reflect recent or ongoing studies in computer science, data science, or related fields, or clearly state “recent graduate/undergraduate” to align with the program’s entry requirements."
      ],
      "linkedin_keywords": [
        "Python",
        "Power BI",
        "Jupyter",
        "pandas",
        "SQL",
        "scikit-learn",
        "data visualization",
        "exploratory analysis",
        "data science",
        "machine learning",
        "Tableau",
        "business intelligence",
        "SQL Server",
        "AzureML",
        "data analytics",
        "ETL",
        "R",
        "Tableau Desktop",
        "Data Engineer",
        "Business Analyst"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Avery_Chen_20251129_062555.pdf",
      "job_title": "Data Analyst",
      "job_company": "Sports Research",
      "job_id": "4317975045",
      "skill_score": 0.21428571428571427,
      "semantic_score": 0.6728466158670144,
      "topic_score": 0.6728466158670144,
      "final_score": 0.46649421015542936,
      "resume_skills_count": 11,
      "job_skills_count": 6,
      "matching_skills_count": 3,
      "resume_text_length": 2206,
      "resume_skills": [
        "collaboration",
        "data visualization",
        "machine learning",
        "power bi",
        "python",
        "r",
        "reporting",
        "sql",
        "statistical analysis",
        "strategic planning",
        "tableau"
      ],
      "job_skills": [
        "communication",
        "data analysis",
        "data visualization",
        "leadership",
        "reporting",
        "sql"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate’s extensive experience with SQL, BI tools (Tableau, Power BI), dashboard creation, KPI tracking, cross‑functional collaboration, and data‑driven decision support aligns well with the job’s core responsibilities.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Avery_Ortiz_20251129_062431.pdf",
      "job_title": "Sr Data Analyst with verification",
      "job_company": "Horizontal Talent",
      "job_id": "4318091470",
      "skill_score": 0.23529411764705882,
      "semantic_score": 0.6760431729568072,
      "topic_score": 0.6760431729568072,
      "final_score": 0.47770609806742037,
      "resume_skills_count": 13,
      "job_skills_count": 8,
      "matching_skills_count": 4,
      "resume_text_length": 1913,
      "resume_skills": [
        "data cleaning",
        "data visualization",
        "etl",
        "git",
        "machine learning",
        "power bi",
        "python",
        "r",
        "reporting",
        "snowflake",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "azure",
        "ci/cd",
        "etl",
        "oracle",
        "python",
        "root cause analysis",
        "snowflake",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume does not demonstrate the required Azure Data Factory, Informatica/SSIS, Oracle, or cloud certification experience highlighted in the job description.",
      "llm_recommendations": [
        "Highlight any projects where you used Azure Data Factory, Informatica PowerCenter, or SSIS for data pipeline development.",
        "Include experience with batch scripting, PowerShell, or other automation tools used in ETL processes.",
        "Add details of working with Oracle, SQL Server, or other relational databases beyond Snowflake.",
        "Acquire or list relevant Azure certifications (e.g., Azure Data Engineer Associate) to meet the preferred skills.",
        "Mention any familiarity with HL7, FHIR, or other healthcare data standards if applicable."
      ],
      "linkedin_keywords": [
        "Azure Data Factory",
        "Informatica",
        "SSIS",
        "Snowflake",
        "Oracle",
        "SQL Server",
        "PowerShell",
        "Batch Scripting",
        "FHIR",
        "HL7",
        "Data Engineering",
        "ETL",
        "Data Warehouse",
        "Data Modeling",
        "Python",
        "SQL",
        "Azure Certification",
        "Cloud Data Pipeline"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Ayesha_Patel_20251129_060413.pdf",
      "job_title": "Senior DataBricks Developer (Healthcare) with verification",
      "job_company": "Lumenalta",
      "job_id": "4332077514",
      "skill_score": 0.2,
      "semantic_score": 0.7147084644610617,
      "topic_score": 0.7147084644610617,
      "final_score": 0.48308965545358395,
      "resume_skills_count": 15,
      "job_skills_count": 9,
      "matching_skills_count": 4,
      "resume_text_length": 2385,
      "resume_skills": [
        "airflow",
        "aws",
        "bigquery",
        "data pipeline",
        "etl",
        "kpi reporting",
        "machine learning",
        "power bi",
        "python",
        "reporting",
        "s3",
        "snowflake",
        "spark",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "aws",
        "ci/cd",
        "collaboration",
        "databricks",
        "elt",
        "etl",
        "pyspark",
        "python",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks demonstrated experience with Databricks in AWS, healthcare claims datasets, HIPAA compliance, and explicit CI/CD/devops practices required for the role.",
      "llm_recommendations": [
        "Highlight any projects where you used Databricks (or spark on Databricks clusters) in AWS environments.",
        "Add specific experience with healthcare datasets (claims, EHR, payer-provider) and mention HIPAA or other regulatory compliance work.",
        "Showcase CI/CD pipelines and DevOps best practices you’ve implemented (e.g., using git, CI tools, automated testing, deployment workflows).",
        "Detail familiarity with AWS Glue Data Catalog or Lake Formation if applicable, or other data governance tools.",
        "Emphasize your 5+ years of hands‑on data engineering at senior level, focusing on large, complex pipelines and performance tuning in the cloud."
      ],
      "linkedin_keywords": [
        "Databricks",
        "AWS",
        "HIPAA",
        "Healthcare Data Engineering",
        "CI/CD",
        "Data Governance",
        "ETL",
        "Spark",
        "Python",
        "SQL",
        "Data Lake Formation",
        "Glue",
        "Claims Analytics",
        "EHR",
        "Payer-Provider Data"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Ben-Fishbeins-Resume-Digital-Marketing-Specialist.pdf",
      "job_title": "Digital Growth & AEO Specialist (Contract-to-Hire) with verification",
      "job_company": "AdOmni",
      "job_id": "4316964338",
      "skill_score": 0.25,
      "semantic_score": 0.5564014911651651,
      "topic_score": 0.5564014911651651,
      "final_score": 0.4185208201408408,
      "resume_skills_count": 7,
      "job_skills_count": 3,
      "matching_skills_count": 2,
      "resume_text_length": 2708,
      "resume_skills": [
        "content marketing",
        "digital marketing",
        "google ads",
        "google analytics",
        "performance analysis",
        "reporting",
        "seo"
      ],
      "job_skills": [
        "client success",
        "digital marketing",
        "seo"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks the required depth in AEO, schema/structured data, GA4/GSC, and 7‑10 years of B2B digital marketing experience needed for this role.",
      "llm_recommendations": [
        "Pursue hands‑on AEO projects, including schema implementation and featured answer strategy, and document results.",
        "Gain proficiency in GA4, GSC, and CRM/automation platforms; add certifications or training if possible.",
        "Highlight any experiences with AI search engines (ChatGPT, Gemini, Claude) and demonstrate how insights were translated into marketing programs.",
        "Quantify achievements with specific metrics (e.g., % increase in qualified pipeline, improvement in featured‑answer share).",
        "Update the resume to explicitly state years of experience in each key area (SEO, paid media, demand generation) and frame roles as B2B."
      ],
      "linkedin_keywords": [
        "digital growth specialist",
        "AEO specialist",
        "answer engine optimization",
        "schema",
        "structured data",
        "featured snippets",
        "AI search",
        "paid media",
        "B2B marketing",
        "demand generation",
        "GA4",
        "Google Search Console",
        "SEO strategy."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Boni Vasius Rosen - Resume.pdf",
      "job_title": "Junior Machine Learning Engineer",
      "job_company": "GITAA",
      "job_id": "4331137647",
      "skill_score": 0.1935483870967742,
      "semantic_score": 0.6877746991976929,
      "topic_score": 0.6877746991976929,
      "final_score": 0.4653728587522794,
      "resume_skills_count": 22,
      "job_skills_count": 15,
      "matching_skills_count": 6,
      "resume_text_length": 5442,
      "resume_skills": [
        "agile",
        "azure",
        "collaboration",
        "critical thinking",
        "data analysis",
        "data analytics",
        "data visualization",
        "deep learning",
        "docker",
        "forecasting",
        "java",
        "jira",
        "machine learning",
        "natural language processing",
        "process improvement",
        "project management",
        "python",
        "r",
        "spark",
        "sql",
        "tableau",
        "text classification"
      ],
      "job_skills": [
        "aws",
        "azure",
        "ci/cd",
        "customer segmentation",
        "docker",
        "feature engineering",
        "forecasting",
        "gcp",
        "kubernetes",
        "machine learning",
        "nlp",
        "nosql",
        "operational efficiency",
        "python",
        "sql"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate demonstrates strong machine‑learning expertise, relevant experience in data pipelines and model deployment, cloud and containerization skills, and more than the required 1.5 years of industry work.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "BryceTsuyukiResume.pdf",
      "job_title": "Principal Software Engineer with verification",
      "job_company": "Microsoft",
      "job_id": "4332807035",
      "skill_score": 0.13333333333333333,
      "semantic_score": 0.48250207617841867,
      "topic_score": 0.48250207617841867,
      "final_score": 0.32537614189813024,
      "resume_skills_count": 10,
      "job_skills_count": 7,
      "matching_skills_count": 2,
      "resume_text_length": 4145,
      "resume_skills": [
        "aws",
        "bash",
        "docker",
        "git",
        "javascript",
        "jira",
        "kubernetes",
        "leadership",
        "notion",
        "postgresql"
      ],
      "job_skills": [
        "azure",
        "c#",
        "c++",
        "java",
        "javascript",
        "kubernetes",
        "python"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate’s experience and skill set lack the extensive IP networking, SDN, and Azure infrastructure expertise required for the role, falling short of the 6‑year and specialized networking technology experience needed.",
      "llm_recommendations": [
        "Highlight any networking projects or coursework involving IP routing, EVPN, VXLAN, or MPLS, and quantify your involvement.",
        "Obtain and document relevant certifications (e.g., CCNP, Azure Network Engineer Associate, or Microsoft Azure Fundamentals).",
        "Emphasize large‑scale distributed system experience, such as managing thousands of hosts, cluster scalability, or multi‑region deployments.",
        "Include any exposure to Azure services (AKS, Azure Networking, Azure Sentinel) or similar cloud environments.",
        "Expand on on‑call, incident response, and reliability metrics to demonstrate proven reliability engineering at scale."
      ],
      "linkedin_keywords": [
        "Azure",
        "Kubernetes",
        "SDN",
        "EVPN",
        "VXLAN",
        "MPLS",
        "Network Engineering",
        "Distributed Systems",
        "Site Reliability Engineering",
        "Cloud Infrastructure"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Cartographic_Analyst_andrew.pdf",
      "job_title": "Manufacturing Associate II - Night Shift, Site Based, Redmond, WA with verification",
      "job_company": "Evotec",
      "job_id": "4318482807",
      "skill_score": 0.16666666666666666,
      "semantic_score": 0.35647780765527304,
      "topic_score": 0.35647780765527304,
      "final_score": 0.27106279421040014,
      "resume_skills_count": 10,
      "job_skills_count": 4,
      "matching_skills_count": 2,
      "resume_text_length": 7772,
      "resume_skills": [
        "aws",
        "azure",
        "communication",
        "data analysis",
        "data visualization",
        "leadership",
        "presentation skills",
        "python",
        "sql",
        "time management"
      ],
      "job_skills": [
        "communication",
        "go",
        "spark",
        "time management"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate’s experience is focused on geospatial analysis and military roles, with no background in biopharmaceutical manufacturing, CGMP processes, or relevant engineering skills required for the Manufacturing Associate II role.",
      "llm_recommendations": [
        "Highlight any transferable skills such as quality assurance, process monitoring, and SOP development, or add them if you have experience in similar contexts.",
        "Acquire relevant experience through internships, certifications, or coursework in biopharma manufacturing, CGMP compliance, or laboratory operations.",
        "Emphasize mechanical aptitude, right‑first‑time focus, and teamwork abilities, aligning them with the job’s manufacturing expectations.",
        "Quantify accomplishments with metrics (e.g., number of products, processes improved) to demonstrate impact and readiness for a production environment.",
        "Tailor your résumé to include keywords from the job posting (CGMP, MES, EBR, chromatography, filtration) and consider a brief summary that explicitly references manufacturing and bioprocessing interests."
      ],
      "linkedin_keywords": [
        "biopharmaceutical manufacturing",
        "CGMP",
        "single‑use bioreactor",
        "chromatography",
        "filtration",
        "MES",
        "EBR",
        "quality assurance",
        "process monitoring",
        "GMP compliance"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Chris-Pepper-Resume.pdf",
      "job_title": "Lead Full Stack Developer with verification",
      "job_company": "Raymond James",
      "job_id": "4316703496",
      "skill_score": 0.15384615384615385,
      "semantic_score": 0.5462028980255127,
      "topic_score": 0.5462028980255127,
      "final_score": 0.3696423631448012,
      "resume_skills_count": 5,
      "job_skills_count": 10,
      "matching_skills_count": 2,
      "resume_text_length": 7643,
      "resume_skills": [
        "aws",
        "data analysis",
        "github",
        "machine learning",
        "oracle"
      ],
      "job_skills": [
        "aws",
        "ci/cd",
        "collaboration",
        "java",
        "kubernetes",
        "leadership",
        "mysql",
        "oracle",
        "python",
        "strategic thinking"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate has extensive systems‑administration background but lacks explicit experience with Java/Python full‑stack development, React/Angular front‑end, SpringBoot/Django back‑end, and cloud/container orchestration required for this Lead Full Stack Developer role.",
      "llm_recommendations": [
        "Highlight any Java, Python, or SpringBoot/Django projects, including code samples or open‑source contributions.",
        "Add work demonstrating React or Angular front‑end development, full‑stack CI/CD pipelines, and secure coding practices.",
        "Include experience with AWS services (EC2, ECS, EKS), Kubernetes, containerization, and OAuth integration.",
        "Provide concrete examples of team leadership in software development, specifying team size, project scope, and outcomes.",
        "Obtain and list relevant certifications (e.g., AWS Certified Solutions Architect, Certified Kubernetes Administrator, Java SE Programmer)."
      ],
      "linkedin_keywords": [
        "full stack developer",
        "Java",
        "Python",
        "React",
        "Angular",
        "Spring Boot",
        "Django",
        "Kubernetes",
        "AWS",
        "cloud engineering",
        "senior developer",
        "technical leadership",
        "CI/CD",
        "secure coding",
        "verification systems."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Connor-Scott-Business-Development-Manager.pdf",
      "job_title": "Customer Success Manager",
      "job_company": "Jump",
      "job_id": "4332384325",
      "skill_score": 0.2857142857142857,
      "semantic_score": 0.6590024828910828,
      "topic_score": 0.6590024828910828,
      "final_score": 0.49102279416152406,
      "resume_skills_count": 7,
      "job_skills_count": 2,
      "matching_skills_count": 2,
      "resume_text_length": 4145,
      "resume_skills": [
        "account management",
        "business analysis",
        "communication",
        "data analytics",
        "market research",
        "negotiation",
        "requirements gathering"
      ],
      "job_skills": [
        "account management",
        "communication"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The résumé emphasizes business development and general account management but lacks concrete experience in customer success, renewals, upsells, financial services (RIAs/BDS) and data‑driven retention metrics required for the role.",
      "llm_recommendations": [
        "Highlight any past roles or projects where you managed customer health scores, performed QBRs, and drove renewals or expansion revenue.",
        "Demonstrate familiarity with the wealthtech/fintech ecosystem—mention specific financial institutions, RIAs, or broker‑dealer engagements.",
        "Quantify outcomes: include specific adoption, retention, or expansion percentages and revenue figures to showcase impact.",
        "Add any coursework or certifications related to customer success, SaaS metrics, or financial services compliance.",
        "Tailor your résumé to include keywords from the job description (e.g., “customer success manager,” “enterprise accounts,” “financial advisors,” “AI SaaS”)."
      ],
      "linkedin_keywords": [
        "customer success manager",
        "account management",
        "wealthtech",
        "fintech",
        "financial services",
        "AI SaaS",
        "client retention",
        "upsell strategy",
        "renewal management",
        "enterprise customer success"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Corey_Reichle.pdf",
      "job_title": "HPE ProLiant Server Engineer – L2 (Compute + VMware/Linux) with verification",
      "job_company": "Talent Worx",
      "job_id": "4332413251",
      "skill_score": 0.2,
      "semantic_score": 0.6254829838008324,
      "topic_score": 0.6254829838008324,
      "final_score": 0.4340156410904578,
      "resume_skills_count": 4,
      "job_skills_count": 2,
      "matching_skills_count": 1,
      "resume_text_length": 5345,
      "resume_skills": [
        "decision making",
        "go",
        "jira",
        "oracle"
      ],
      "job_skills": [
        "oracle",
        "shell scripting"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks specific experience with VMware, HPE ProLiant servers, IBM MQ, and several required middleware tools, making the match weak for this role.",
      "llm_recommendations": [
        "Highlight any experience with HPE ProLiant hardware, VMware ESXi, or related virtualization platforms.",
        "Demonstrate proficiency with IBM MQ, WebSphere Application Server, Oracle WebLogic, Apache Tomcat, and other middleware in the résumé.",
        "Include certifications such as HPE Server Engineer, VMware Certified Professional, or relevant Linux/Unix credentials.",
        "Provide details on L3/L2 level support incidents, troubleshooting, and documentation handled.",
        "Emphasize scripting skills (Shell, Perl, Python) and automation projects for infrastructure management."
      ],
      "linkedin_keywords": [
        "HPE ProLiant",
        "VMware ESXi",
        "Linux Server Administration",
        "WebSphere Application Server",
        "IBM MQ",
        "Oracle WebLogic",
        "Middleware Engineer",
        "Server Support",
        "L2/L3 Support",
        "VMware Certified Professional"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Daniel_Cooper.pdf",
      "job_title": "Software Engineer with verification",
      "job_company": "CACI International Inc",
      "job_id": "4331329662",
      "skill_score": 0.17391304347826086,
      "semantic_score": 0.688518464565282,
      "topic_score": 0.688518464565282,
      "final_score": 0.4569460250761225,
      "resume_skills_count": 17,
      "job_skills_count": 10,
      "matching_skills_count": 4,
      "resume_text_length": 5398,
      "resume_skills": [
        "agile",
        "aws",
        "bigquery",
        "c#",
        "collaboration",
        "github",
        "java",
        "javascript",
        "jira",
        "leadership",
        "mongodb",
        "mysql",
        "postgresql",
        "python",
        "s3",
        "sql",
        "typescript"
      ],
      "job_skills": [
        "agile",
        "azure",
        "communication",
        "docker",
        "gitlab",
        "javascript",
        "oracle",
        "postgresql",
        "python",
        "reporting"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume demonstrates strong general software and security skills but lacks the specific DevSecOps, containerization, cyberstar, DISA STIG, and DOD application experience required for this role; it also does not address the preferred education or clearance prerequisites.",
      "llm_recommendations": [
        "Highlight any experience with containerization (Docker, Podman) and Azure DevOps or GitLab CI/CD pipelines.",
        "Include concrete examples of security verification work (e.g., DISA STIG compliance, vulnerability scanning, or penetration testing) and any interaction with DOD or defense-related applications.",
        "Emphasize DevSecOps practices, test automation, and familiarity with YAML, PowerShell, and cloud security tools.",
        "Add relevant certifications (e.g., CompTIA Security+, CISSP, or Azure Security Engineer Associate) and note intent to obtain or already having a security clearance.",
        "Update the education section to showcase any cybersecurity-related coursework or degree-equivalent projects that align with the required BS in cybersecurity."
      ],
      "linkedin_keywords": [
        "python",
        "linux",
        "postgresql",
        "html",
        "javascript",
        "devops",
        "docker",
        "podman",
        "azure devops",
        "gitlab",
        "devsecops",
        "security engineering",
        "cybersecurity analyst",
        "software engineer",
        "cloud security",
        "agile",
        "containerization",
        "DISA STIG",
        "DOD applications",
        "cybersecurity certification",
        "security clearance."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Danielle-Connolly-Resume.pdf",
      "job_title": "Category Analyst, Own Brands with verification",
      "job_company": "Daymon",
      "job_id": "4323959918",
      "skill_score": 0.0,
      "semantic_score": 0.5729662692218667,
      "topic_score": 0.5729662692218667,
      "final_score": 0.31513144807202664,
      "resume_skills_count": 9,
      "job_skills_count": 1,
      "matching_skills_count": 0,
      "resume_text_length": 5241,
      "resume_skills": [
        "a/b testing",
        "communication",
        "content marketing",
        "google analytics",
        "hubspot",
        "leadership",
        "ppc",
        "project management",
        "seo"
      ],
      "job_skills": [
        "decision making"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks direct Retail Link/Atlas database experience and specific vendor onboarding or category analysis background required for the Category Analyst role.",
      "llm_recommendations": [
        "Highlight any experience with retail analytics tools (e.g., Retail Link, SAP, Oracle Retail, or similar systems).",
        "Include examples of vendor management, product onboarding, or supplier collaboration activities.",
        "Detail any weekly sales reporting or ad‑hoc reporting performed, especially with Excel, PowerPoint, or data visualization tools.",
        "Mention proficiency in Microsoft Access and data synchronization processes.",
        "Obtain or list certifications related to retail analytics or category management (e.g., Retail Analytics, Category Management Specialist)."
      ],
      "linkedin_keywords": [
        "Category Analyst",
        "Retail Link",
        "Sales Analysis",
        "Vendor Management",
        "Data Analytics",
        "Excel",
        "PowerPoint",
        "Retail Analytics",
        "Amazon Marketplace",
        "Business Intelligence",
        "ATLAS Database",
        "Product Onboarding."
      ],
      "llm_error": null
    },
    {
      "resume_file": "David-Zhang-Resume.pdf",
      "job_title": "Sr Principal Software Engineer - Yahoo Search with verification",
      "job_company": "Yahoo",
      "job_id": "4319255318",
      "skill_score": 0.125,
      "semantic_score": 0.5543685785485237,
      "topic_score": 0.5543685785485237,
      "final_score": 0.36115271820168804,
      "resume_skills_count": 10,
      "job_skills_count": 8,
      "matching_skills_count": 2,
      "resume_text_length": 3222,
      "resume_skills": [
        "aws",
        "communication",
        "git",
        "mongodb",
        "mysql",
        "python",
        "s3",
        "seo",
        "sql",
        "typescript"
      ],
      "job_skills": [
        "analytical thinking",
        "collaboration",
        "communication",
        "go",
        "java",
        "leadership",
        "python",
        "spark"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate does not meet the extensive 7+ year backend, distributed systems, and leadership experience required for a Sr Principal Software Engineer role.",
      "llm_recommendations": [
        "Highlight any large‑scale or cloud‑native projects, including measurable performance or scalability metrics.",
        "Add explicit experience or coursework with Go, Java, or mature backend frameworks, and document any contributions to distributed architecture or data‑processing pipelines.",
        "Emphasize team leadership, mentoring, and code‑review activities, detailing team sizes and impact on quality.",
        "Mention remote collaboration skills and any experience working across multiple time zones or departments.",
        "Include certifications or training in AWS, GCP, or Azure that demonstrate cloud engineering competence."
      ],
      "linkedin_keywords": [
        "distributed systems",
        "cloud architecture",
        "Go",
        "Java",
        "Python",
        "backend engineering",
        "software architecture",
        "scalability",
        "performance optimization",
        "technical leadership",
        "full‑stack engineering",
        "senior software engineer",
        "data processing",
        "AWS",
        "GCP",
        "Azure."
      ],
      "llm_error": null
    },
    {
      "resume_file": "David_J_Frederickson.pdf",
      "job_title": "Procurement Executive",
      "job_company": "Rockhill Asia",
      "job_id": "4318667112",
      "skill_score": 0.25,
      "semantic_score": 0.7769247160509073,
      "topic_score": 0.7769247160509073,
      "final_score": 0.5398085938279991,
      "resume_skills_count": 11,
      "job_skills_count": 9,
      "matching_skills_count": 4,
      "resume_text_length": 4331,
      "resume_skills": [
        "communication",
        "forecasting",
        "leadership",
        "logistics",
        "operational efficiency",
        "oracle",
        "process improvement",
        "procurement",
        "project management",
        "reporting",
        "sales operations"
      ],
      "job_skills": [
        "collaboration",
        "communication",
        "cost analysis",
        "cross-functional collaboration",
        "logistics",
        "negotiation",
        "procurement",
        "reporting",
        "stakeholder management"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate’s procurement management experience, supplier negotiation skills, and data‑driven approach align closely with the role’s core responsibilities, making him a strong match for the Procurement Executive position.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Dikshit_Khandelwal.pdf",
      "job_title": "Software Engineer with verification",
      "job_company": "Calance",
      "job_id": "4318607942",
      "skill_score": 0.30434782608695654,
      "semantic_score": 0.5807317495346069,
      "topic_score": 0.5807317495346069,
      "final_score": 0.45635898398316427,
      "resume_skills_count": 15,
      "job_skills_count": 15,
      "matching_skills_count": 7,
      "resume_text_length": 5166,
      "resume_skills": [
        "aws",
        "azure",
        "c++",
        "ci/cd",
        "communication",
        "docker",
        "dynamodb",
        "ec2",
        "gcp",
        "go",
        "javascript",
        "kubernetes",
        "python",
        "s3",
        "scala"
      ],
      "job_skills": [
        "aws",
        "ci/cd",
        "communication",
        "docker",
        "ec2",
        "github",
        "gitlab",
        "kubernetes",
        "lambda",
        "leadership",
        "mongodb",
        "nosql",
        "root cause analysis",
        "s3",
        "teamwork"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks the extensive 10+ years of senior backend experience, MongoDB expertise, and the specific AWS, Kubernetes, Helm, CI/CD, and observability tool set required for the senior Golang AWS developer role.",
      "llm_recommendations": [
        "Highlight any large-scale Go projects, especially those involving AWS services, and quantify the impact or scale.",
        "Gain and document experience with MongoDB (nosql modeling, indexing, performance tuning).",
        "Build or contribute to CI/CD pipelines using GitLab and GitHub Actions, emphasizing automation and quality gates.",
        "Include observability work with Opentelemetry or Datadog, detailing metrics, tracing, and logging implementation.",
        "Emphasize leadership or mentorship roles, especially across distributed or offshore teams, and any management or architectural responsibilities."
      ],
      "linkedin_keywords": [
        "Go",
        "AWS",
        "Kubernetes",
        "Docker",
        "Helm",
        "MongoDB",
        "CI/CD",
        "GitLab",
        "GitHub Actions",
        "Opentelemetry",
        "Datadog",
        "backend architecture",
        "senior developer",
        "mentorship."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Dr_Maya_Patel_20251129_060920.pdf",
      "job_title": "Deep Learning Engineer",
      "job_company": "Shields Group Search",
      "job_id": "4319372032",
      "skill_score": 0.047619047619047616,
      "semantic_score": 0.6211822258721467,
      "topic_score": 0.6211822258721467,
      "final_score": 0.3630787956582521,
      "resume_skills_count": 18,
      "job_skills_count": 4,
      "matching_skills_count": 1,
      "resume_text_length": 2942,
      "resume_skills": [
        "aws",
        "azure",
        "ci/cd",
        "data analysis",
        "deep learning",
        "docker",
        "etl",
        "git",
        "kubernetes",
        "machine learning",
        "natural language processing",
        "nlp",
        "nosql",
        "python",
        "spark",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "collaboration",
        "deep learning",
        "go",
        "teamwork"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks explicit evidence of computer vision projects (object detection/semantic segmentation) and production‑level deployment/monitoring experience required for the role.",
      "llm_recommendations": [
        "Add detailed descriptions of CV projects (e.g., object detection, segmentation, layout analysis) with measurable results.",
        "Highlight any deployment or monitoring pipelines (ML Ops, A/B testing, performance dashboards) used in production.",
        "Include experience with active learning, RLHF, or similar cutting‑edge methodologies if applicable.",
        "Specify involvement in architecture/roadmap decisions and integration of models into product systems.",
        "State willingness and availability to work onsite in New York City."
      ],
      "linkedin_keywords": [
        "deep learning engineer",
        "computer vision",
        "object detection",
        "semantic segmentation",
        "PyTorch",
        "machine learning",
        "MLOps",
        "production ML",
        "reinforcement learning",
        "RLHF"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Elena_Moreno_20251129_064803.pdf",
      "job_title": "Technical Product Manager - AI Platforms - GenAI Developer Platform - CTO Office with verification",
      "job_company": "Bloomberg",
      "job_id": "4318085393",
      "skill_score": 0.19230769230769232,
      "semantic_score": 0.6328192949295044,
      "topic_score": 0.6328192949295044,
      "final_score": 0.43458907374968897,
      "resume_skills_count": 20,
      "job_skills_count": 11,
      "matching_skills_count": 5,
      "resume_text_length": 2622,
      "resume_skills": [
        "a/b testing",
        "data analysis",
        "data analytics",
        "data ingestion",
        "data visualization",
        "decision making",
        "docker",
        "feature engineering",
        "kubernetes",
        "leadership",
        "machine learning",
        "natural language processing",
        "nlp",
        "numpy",
        "pandas",
        "python",
        "r",
        "sentiment analysis",
        "sql",
        "statistical analysis"
      ],
      "job_skills": [
        "aws",
        "azure",
        "collaboration",
        "communication",
        "gcp",
        "kubernetes",
        "machine learning",
        "natural language processing",
        "nlp",
        "product management",
        "sentiment analysis"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume demonstrates strong NLP and engineering skills but lacks evidence of technical product‑management experience, GenAI platform familiarity, and the specific cloud/MLOps expertise required for this role.",
      "llm_recommendations": [
        "Highlight any product ownership, roadmap development, or stakeholder‑management responsibilities you have led, even if under a different title.",
        "Add experience with cloud platforms (AWS, GCP, Azure), MLOps tools, and GenAI frameworks such as LangChain, LlamaIndex, or OpenAI APIs.",
        "Include any open‑source contributions, community leadership, or public‑project involvements that showcase collaboration and technical influence.",
        "Emphasize communication, prioritization, and cross‑functional team leadership skills that align with product‑management duties.",
        "Add measurable product outcomes (e.g., adoption metrics, revenue impact, customer satisfaction) to demonstrate business‑focused impact."
      ],
      "linkedin_keywords": [
        "Technical Product Manager",
        "AI Product Manager",
        "GenAI",
        "MLOps",
        "LLM",
        "NLP",
        "Python",
        "Kubernetes",
        "Docker",
        "Cloud Computing",
        "AWS",
        "GCP",
        "Azure",
        "OpenAI",
        "LlamaIndex",
        "LangChain",
        "Product Roadmap",
        "Machine Learning",
        "Data Science",
        "Software Engineering",
        "Product Ownership",
        "User Experience",
        "AI Platforms",
        "AI Strategy"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Elena_Müller_20251129_064138.pdf",
      "job_title": "Sr Business Intelligence Engineer with verification",
      "job_company": "Humana",
      "job_id": "4318957629",
      "skill_score": 0.25,
      "semantic_score": 0.6532549651417127,
      "topic_score": 0.6532549651417127,
      "final_score": 0.47179023082794197,
      "resume_skills_count": 15,
      "job_skills_count": 15,
      "matching_skills_count": 6,
      "resume_text_length": 1922,
      "resume_skills": [
        "airflow",
        "data analysis",
        "data integration",
        "data visualization",
        "decision making",
        "etl",
        "leadership",
        "machine learning",
        "operational efficiency",
        "power bi",
        "python",
        "r",
        "reporting",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "agile",
        "aws",
        "communication",
        "data analysis",
        "data analytics",
        "forecasting",
        "leadership",
        "power bi",
        "process improvement",
        "qlik",
        "reporting",
        "root cause analysis",
        "sql",
        "tableau",
        "trend analysis"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks the specific healthcare data experience, advanced T‑SQL and Power Platform skills (Power Apps/Automate), cloud expertise, and domain knowledge required for the Sr Business Intelligence Engineer role at Humana.",
      "llm_recommendations": [
        "Highlight any healthcare or claims/billing data projects, even if limited, and quantify impacts.",
        "Acquire and showcase experience with T‑SQL, Power Apps, and Power Automate through certifications or sample projects.",
        "Gain basic cloud experience (AWS or Azure Databricks) and add it to the skillset and portfolio.",
        "Emphasize advanced Power BI and Tableau dashboards for senior leadership, including executive‑level presentations.",
        "Add Microsoft Office proficiency (Word, Excel, PowerPoint) and any consulting or stakeholder‑management experience."
      ],
      "linkedin_keywords": [
        "business intelligence",
        "power bi",
        "sql",
        "t-sql",
        "power apps",
        "power automate",
        "healthcare analytics",
        "claims analysis",
        "cloud data platforms",
        "tableau",
        "etl",
        "apache airflow",
        "data governance",
        "predictive modeling",
        "process improvement",
        "agile",
        "executive communication."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Elena_S_Marin_20251129_060041.pdf",
      "job_title": "Senior Research Scientist with verification",
      "job_company": "NVIDIA",
      "job_id": "4332735576",
      "skill_score": 0.18181818181818182,
      "semantic_score": 0.5851642067779367,
      "topic_score": 0.5851642067779367,
      "final_score": 0.403658495546047,
      "resume_skills_count": 8,
      "job_skills_count": 5,
      "matching_skills_count": 2,
      "resume_text_length": 1837,
      "resume_skills": [
        "docker",
        "embeddings",
        "feature engineering",
        "leadership",
        "natural language processing",
        "nlp",
        "sentiment analysis",
        "text classification"
      ],
      "job_skills": [
        "deep learning",
        "machine learning",
        "natural language processing",
        "nlp",
        "python"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks the advanced research credentials, speech/NLP focus, publications, and open‑source contributions required for this senior research scientist role.",
      "llm_recommendations": [
        "Pursue or highlight an advanced degree (PhD or equivalent) in computer science, electrical engineering, or a related field.",
        "Publish peer‑reviewed papers in top NLP or speech conferences (e.g., Interspeech, ACL, ICASSP) and include them on the resume.",
        "Contribute to or maintain open‑source projects (e.g., NeMo, Hugging Face) and list these contributions explicitly.",
        "Emphasize experience with speech recognition, synthesis, or translation models, including any GPU‑accelerated training or inference work.",
        "Quantify research impact (e.g., citations, model accuracy improvements, real‑world deployments) and specify any roles as conference reviewer or workshop speaker."
      ],
      "linkedin_keywords": [
        "speech recognition",
        "natural language processing",
        "deep learning",
        "transformer models",
        "PyTorch",
        "TensorFlow",
        "neural machine translation",
        "NeMo",
        "open source",
        "research publications",
        "GPU acceleration",
        "academic conferences",
        "PhD",
        "research scientist"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Garrick_Hilliard.pdf",
      "job_title": "Senior Software Engineer with verification",
      "job_company": "Intuit",
      "job_id": "4331348317",
      "skill_score": 0.25,
      "semantic_score": 0.6183385252952576,
      "topic_score": 0.6183385252952576,
      "final_score": 0.45258618891239166,
      "resume_skills_count": 11,
      "job_skills_count": 9,
      "matching_skills_count": 4,
      "resume_text_length": 1448,
      "resume_skills": [
        "aws",
        "c#",
        "communication",
        "critical thinking",
        "dynamodb",
        "java",
        "javascript",
        "lambda",
        "mysql",
        "salesforce",
        "sql"
      ],
      "job_skills": [
        "agile",
        "aws",
        "communication",
        "java",
        "javascript",
        "leadership",
        "nosql",
        "product management",
        "scrum"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume does not demonstrate sufficient experience with Java, Spring Boot, React, or large‑scale system design required for the Senior Software Engineer role.",
      "llm_recommendations": [
        "Highlight or add projects involving Java (Spring Boot/REST or GraphQL) to show relevant backend experience.",
        "Include React (or similar SPA framework) work, detailing use of Redux, React‑Native, or modern JavaScript patterns.",
        "Demonstrate experience with AWS services beyond ELB/DynamoDB/Lambda, such as EC2, RDS, S3, or cloud architecture designs.",
        "Showcase any experience with unit testing, TDD, and automated testing frameworks in a production environment.",
        "Add a formal education line (B.S./M.S. in Computer Science or equivalent) and any relevant certifications (AWS Solutions Architect, Java SE, etc.)."
      ],
      "linkedin_keywords": [
        "senior software engineer",
        "Java",
        "Spring Boot",
        "React",
        "AWS",
        "microservices",
        "unit testing",
        "TDD",
        "API development",
        "large scale systems"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Ginny_Kim_Resume.pdf",
      "job_title": "Full Stack Senior Software Engineer - Post Trade Systems with verification",
      "job_company": "KKR",
      "job_id": "4317227328",
      "skill_score": 0.08108108108108109,
      "semantic_score": 0.5955829379309986,
      "topic_score": 0.5955829379309986,
      "final_score": 0.3640571023485357,
      "resume_skills_count": 25,
      "job_skills_count": 15,
      "matching_skills_count": 3,
      "resume_text_length": 5356,
      "resume_skills": [
        "communication",
        "data analysis",
        "docker",
        "etl",
        "forecasting",
        "git",
        "github",
        "knn",
        "lda",
        "logistic regression",
        "market research",
        "metabase",
        "mongodb",
        "nosql",
        "operational efficiency",
        "pandas",
        "postgresql",
        "python",
        "r",
        "sap",
        "sap erp",
        "sql",
        "topic modeling",
        "trend analysis",
        "xgboost"
      ],
      "job_skills": [
        "analytical thinking",
        "aws",
        "change management",
        "communication",
        "critical thinking",
        "ec2",
        "fp&a",
        "lambda",
        "leadership",
        "python",
        "redshift",
        "reporting",
        "sql",
        "teamwork",
        "workflow automation"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks the depth of financial‑services software engineering experience, fixed‑income domain knowledge, and technical skills (Python, AWS, TDD, REST API, Aurora RDS) required for the post‑trade systems role.",
      "llm_recommendations": [
        "Highlight or add experience building end‑to‑end software solutions (e.g., RESTful APIs, microservices) with a strong emphasis on test‑driven development and code reviews.",
        "Incorporate specific financial‑services exposure (fixed‑income instruments, trade confirmations, settlement workflows, or investment‑operations projects) and any related domain knowledge.",
        "Detail any use of cloud services, especially AWS (Aurora RDS, EC2, Lambda, ALB, Secrets Manager), and mention any certifications or hands‑on projects.",
        "If applicable, add experience with Mendix or other low‑code platforms and note any relevant training or courses.",
        "Emphasize any large‑scale data or trading system projects, including performance tuning, security, and compliance requirements."
      ],
      "linkedin_keywords": [
        "full stack developer",
        "senior software engineer",
        "Python developer",
        "REST API",
        "TDD",
        "SQL",
        "AWS",
        "cloud computing",
        "fixed income",
        "trade settlement",
        "DTCC",
        "SWIFT",
        "Mendix",
        "investment operations",
        "post‑trade systems",
        "financial services engineering."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Ira_Patel_20251129_061250.pdf",
      "job_title": "Senior DataBricks Developer (Healthcare) with verification",
      "job_company": "Lumenalta",
      "job_id": "4332077514",
      "skill_score": 0.26666666666666666,
      "semantic_score": 0.6856054851051888,
      "topic_score": 0.6856054851051888,
      "final_score": 0.49708301680785383,
      "resume_skills_count": 10,
      "job_skills_count": 9,
      "matching_skills_count": 4,
      "resume_text_length": 1913,
      "resume_skills": [
        "airflow",
        "aws",
        "etl",
        "git",
        "python",
        "reporting",
        "snowflake",
        "spark",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "aws",
        "ci/cd",
        "collaboration",
        "databricks",
        "elt",
        "etl",
        "pyspark",
        "python",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate does not meet the required senior level (7+ years, 5+ with Databricks on AWS) or healthcare data/ HIPAA experience highlighted in the posting.",
      "llm_recommendations": [
        "Gain hands‑on experience building Databricks pipelines with pyspark on AWS and document these projects.",
        "Acquire or highlight experience with healthcare datasets (claims, EHR, payer‑provider) and mention HIPAA compliance or related data governance practices.",
        "Showcase senior‑level responsibilities that demonstrate leadership, architecture design, and CI/CD implementation for data pipelines.",
        "Emphasize large‑scale ETL/ELT work, including performance optimization and data quality mechanisms.",
        "Include certifications or training (e.g., Databricks Certified Associate, AWS Data Analytics Specialty, HIPAA compliance training)."
      ],
      "linkedin_keywords": [
        "Databricks",
        "pyspark",
        "healthcare data",
        "claims data",
        "HIPAA compliance",
        "data engineering",
        "ETL",
        "CI/CD",
        "AWS Glue",
        "data governance",
        "senior data engineer"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Ivy_Chen_20251129_062308.pdf",
      "job_title": "AI/Machine Learning (NLP) Engineer with verification",
      "job_company": "Aslase",
      "job_id": "4332070318",
      "skill_score": 0.19047619047619047,
      "semantic_score": 0.6912168514286867,
      "topic_score": 0.6912168514286867,
      "final_score": 0.46588355400006337,
      "resume_skills_count": 16,
      "job_skills_count": 9,
      "matching_skills_count": 4,
      "resume_text_length": 2076,
      "resume_skills": [
        "airflow",
        "aws",
        "azure",
        "data analysis",
        "data analytics",
        "docker",
        "feature engineering",
        "gcp",
        "machine learning",
        "natural language processing",
        "nlp",
        "python",
        "r",
        "reporting",
        "sql",
        "statistical analysis"
      ],
      "job_skills": [
        "ci/cd",
        "docker",
        "embeddings",
        "github",
        "kubernetes",
        "machine learning",
        "nlp",
        "python",
        "spark"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks key experience with LLM APIs, embeddings, langchain/llamaindex, vector databases, prompt engineering, and MLOps practices required for the role.",
      "llm_recommendations": [
        "Add specific projects or work where you integrated and fine‑tuned LLMs using APIs such as OpenAI or Anthropic, and deployed them with lightweight inference engines.",
        "Highlight use of embedding models (e.g., Hugging Face, OpenAI Ada) and vector databases (Pinecone, Weaviate, FAISS) for RAG systems, including prompt engineering and prompt tuning work.",
        "Emphasize MLOps workflow experience, detailing CI/CD pipelines, Docker/Kubernetes deployments, and model monitoring/versioning tools like MLflow.",
        "Include any no‑code prototyping work with tools such as n8n or Make, as well as contributions to open‑source AI projects or repositories.",
        "Mention experience with streaming data pipelines (Apache Kafka, Spark) and a basic understanding of AI ethics and bias mitigation."
      ],
      "linkedin_keywords": [
        "LLM integration",
        "OpenAI",
        "Anthropic",
        "LangChain",
        "LlamaIndex",
        "CrewAI",
        "Pinecone",
        "Weaviate",
        "FAISS",
        "prompt engineering",
        "RAG",
        "MLOps",
        "Docker",
        "Kubernetes",
        "MLflow",
        "n8n",
        "no-code automation",
        "Apache Kafka",
        "AI ethics",
        "bias mitigation"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Jacob_Lee_Resume_2024.pdf",
      "job_title": "[HN] Hiring Android Developer Level Middle - Senior - Signing Bonus - Upto 50M",
      "job_company": "JoySoft Go",
      "job_id": "4319167636",
      "skill_score": 0.0,
      "semantic_score": 0.26218907462423635,
      "topic_score": 0.26218907462423635,
      "final_score": 0.14420399104332998,
      "resume_skills_count": 1,
      "job_skills_count": 2,
      "matching_skills_count": 0,
      "resume_text_length": 5557,
      "resume_skills": [
        "r"
      ],
      "job_skills": [
        "git",
        "java"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks any Android, Kotlin/Java, or Jetpack experience required for the Android developer role.",
      "llm_recommendations": [
        "Add a section detailing Android projects, emphasizing Java/Kotlin usage and architecture patterns such as MVVM or clean architecture.",
        "Highlight any experience with Jetpack components (Navigation, Lifecycle, Coroutines, Room, WorkManager) and performance optimization techniques.",
        "Include certifications or coursework in Android development, Kotlin programming, and mobile UI/UX design.",
        "List concrete examples of using Firebase, AdMob, or in-app purchase implementations to demonstrate full-stack mobile skills.",
        "Showcase collaboration and project management experience in an Android or mobile product development team environment."
      ],
      "linkedin_keywords": [
        "Android Developer",
        "Kotlin",
        "Java",
        "MVVM",
        "Jetpack",
        "Android Architecture",
        "Clean Architecture",
        "Firebase",
        "AdMob",
        "In-App Purchase",
        "Performance Optimization",
        "Multi-Threading",
        "Android Studio",
        "Mobile UX",
        "Android SDK"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Jae_Kim_20251129_060339.pdf",
      "job_title": "Data Analyst",
      "job_company": "Sports Research",
      "job_id": "4317975045",
      "skill_score": 0.23076923076923078,
      "semantic_score": 0.7438049163702081,
      "topic_score": 0.7438049163702081,
      "final_score": 0.5129388578497683,
      "resume_skills_count": 10,
      "job_skills_count": 6,
      "matching_skills_count": 3,
      "resume_text_length": 1654,
      "resume_skills": [
        "data cleaning",
        "data visualization",
        "etl",
        "power bi",
        "python",
        "r",
        "reporting",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "communication",
        "data analysis",
        "data visualization",
        "leadership",
        "reporting",
        "sql"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate has 5 years of relevant data analytics experience, strong SQL, BI and dashboard skills, KPI tracking, hypothesis testing, and has worked in e-commerce and SaaS environments which aligns closely with the nutraceutical industry emphasis.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Jason_Stahl-Resume-1.pdf",
      "job_title": "Senior Cybersecurity Engineer/Architect with verification",
      "job_company": "Deloitte",
      "job_id": "4331391751",
      "skill_score": 0.0,
      "semantic_score": 0.6804448771741926,
      "topic_score": 0.6804448771741926,
      "final_score": 0.3742446824458059,
      "resume_skills_count": 1,
      "job_skills_count": 2,
      "matching_skills_count": 0,
      "resume_text_length": 5478,
      "resume_skills": [
        "bash"
      ],
      "job_skills": [
        "data analytics",
        "leadership"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks the extensive 10+ year architecture experience, required TS-Q clearance, CISSP/CISM certification, and local Las Vegas residency stipulated in the job posting.",
      "llm_recommendations": [
        "Obtain senior-level security architecture experience (e.g., lead design/implementation projects) and highlight it prominently.",
        "Earn a CISSP or CISM certification and add it to the resume; if possible, also pursue a TS (Top Secret) clearance or indicate current eligibility.",
        "Obtain or confirm a bachelor's degree and clearly list it to satisfy the education requirement.",
        "Update location information to reflect willingness to relocate to Las Vegas or note remote work possibilities if acceptable.",
        "Emphasize leadership roles, project delivery timelines, and measurable outcomes in security initiatives to demonstrate the depth of experience."
      ],
      "linkedin_keywords": [
        "cybersecurity architect",
        "security solutions architect",
        "CISSP",
        "CISM",
        "TS clearance",
        "threat intelligence",
        "incident response",
        "security operations center",
        "governance compliance",
        "enterprise security",
        "cloud security",
        "SIEM",
        "risk assessment",
        "senior security engineer",
        "architecture design",
        "network security",
        "security strategy."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Javier_Martinez_20251129_061437.pdf",
      "job_title": "Senior Software Manager / Director of Engineering with verification",
      "job_company": "ThreadBeast",
      "job_id": "4318647215",
      "skill_score": 0.1111111111111111,
      "semantic_score": 0.41052588820457747,
      "topic_score": 0.41052588820457747,
      "final_score": 0.2757892385125176,
      "resume_skills_count": 17,
      "job_skills_count": 13,
      "matching_skills_count": 3,
      "resume_text_length": 2446,
      "resume_skills": [
        "aws",
        "bert",
        "ci/cd",
        "data analytics",
        "docker",
        "machine learning",
        "nlp",
        "pandas",
        "power bi",
        "python",
        "r",
        "reporting",
        "sentiment analysis",
        "sql",
        "tableau",
        "text classification",
        "xgboost"
      ],
      "job_skills": [
        "ci/cd",
        "collaboration",
        "communication",
        "cross-functional collaboration",
        "forecasting",
        "gcp",
        "github",
        "inventory management",
        "javascript",
        "leadership",
        "machine learning",
        "operational efficiency",
        "python"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks the required leadership experience, full‑stack engineering background, and specific technology stack (React, Node.js, GCP) needed for a Senior Software Manager role.",
      "llm_recommendations": [
        "Highlight any engineering management or team lead positions, detailing team size, hiring, and mentorship responsibilities.",
        "Add experience with full‑stack or backend technologies such as Node.js, JavaScript, or PHP and illustrate contributions to customer‑facing web or mobile applications.",
        "Demonstrate cloud infrastructure proficiency by listing projects on GCP (or comparable platforms) and CI/CD pipelines integrated with cloud services.",
        "Include any work with subscription business models, e‑commerce personalization, or recommendation engines to align with the company's domain.",
        "Incorporate a brief section on strategic technical vision or architectural decisions made for scalable systems or ML/Ops deployments."
      ],
      "linkedin_keywords": [
        "software engineering manager",
        "full stack engineer",
        "cloud architecture",
        "GCP",
        "CI/CD",
        "Docker",
        "Kubernetes",
        "Python",
        "TensorFlow",
        "ML Ops",
        "e‑commerce",
        "subscription services",
        "team lead",
        "AI engineer",
        "JavaScript",
        "Node.js"
      ],
      "llm_error": null
    },
    {
      "resume_file": "JimDunneLinkedIn.pdf",
      "job_title": "Junior Dot Net Developer",
      "job_company": "Greysoft",
      "job_id": "4332434216",
      "skill_score": 0.3333333333333333,
      "semantic_score": 0.41994412004781145,
      "topic_score": 0.41994412004781145,
      "final_score": 0.3809692660262963,
      "resume_skills_count": 5,
      "job_skills_count": 3,
      "matching_skills_count": 2,
      "resume_text_length": 9726,
      "resume_skills": [
        "c#",
        "c++",
        "leadership",
        "oracle",
        "sql"
      ],
      "job_skills": [
        "c#",
        "javascript",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume shows extensive senior experience and leadership roles that exceed the junior level expectations for this ASP.NET/C# position, and it lacks specific banking domain experience mentioned in the posting.",
      "llm_recommendations": [
        "Highlight or add a concise section on any projects or work experience directly related to the banking or finance sector.",
        "Emphasize hands‑on experience with SQL Server, stored procedures, and writing complex SQL queries.",
        "Showcase familiarity with MVC, JavaScript, jQuery, and AngularJS through specific project examples.",
        "Reframe senior leadership details to focus on technical contributions relevant to the developer role.",
        "Include any certifications or training that cover modern .NET development practices (e.g., Microsoft Certified: Azure Developer Associate)."
      ],
      "linkedin_keywords": [
        "ASP.NET",
        "C#",
        "SQL Server",
        "MVC",
        "AngularJS",
        "jQuery",
        "JavaScript",
        ".NET Developer",
        "Banking Software",
        "Financial Applications",
        "Enterprise Applications"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Jin_Liu_20251129_060129.pdf",
      "job_title": "Machine Learning Engineer with verification",
      "job_company": "AmeriHealth Caritas",
      "job_id": "4332716222",
      "skill_score": 0.35714285714285715,
      "semantic_score": 0.6748905986417053,
      "topic_score": 0.6748905986417053,
      "final_score": 0.5319041149672237,
      "resume_skills_count": 12,
      "job_skills_count": 7,
      "matching_skills_count": 5,
      "resume_text_length": 2363,
      "resume_skills": [
        "a/b testing",
        "aws",
        "azure",
        "c++",
        "ci/cd",
        "etl",
        "gcp",
        "hive",
        "machine learning",
        "python",
        "spark",
        "sql"
      ],
      "job_skills": [
        "azure",
        "ci/cd",
        "databricks",
        "machine learning",
        "pyspark",
        "python",
        "sql"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate possesses strong Python, SQL, Azure, Spark, and ML pipeline experience, aligning closely with the job's requirements for data engineering, model deployment, and monitoring, even though specific Databricks and Azure DevOps mentions are absent.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Jin_Soo_Park_20251129_060711.pdf",
      "job_title": "Sr Data Analyst with verification",
      "job_company": "Horizontal Talent",
      "job_id": "4318091470",
      "skill_score": 0.15789473684210525,
      "semantic_score": 0.5977920115652211,
      "topic_score": 0.5977920115652211,
      "final_score": 0.39983823793981893,
      "resume_skills_count": 14,
      "job_skills_count": 8,
      "matching_skills_count": 3,
      "resume_text_length": 2290,
      "resume_skills": [
        "airflow",
        "cost analysis",
        "customer segmentation",
        "data cleaning",
        "data visualization",
        "etl",
        "logistics",
        "power bi",
        "project management",
        "python",
        "r",
        "reporting",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "azure",
        "ci/cd",
        "etl",
        "oracle",
        "python",
        "root cause analysis",
        "snowflake",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks the specific cloud, ETL, and data‑warehousing skills (Azure Data Factory, Informatica/SSIS, Snowflake/Oracle/SQL Server) required for the Sr. Data Analyst role.",
      "llm_recommendations": [
        "Include hands‑on experience with Azure Data Factory and other cloud ETL tools (e.g., Azure Data Lake, Azure Synapse).",
        "Highlight any use of Informatica PowerCenter, SSIS, or equivalent ETL platforms, and list any related projects or achievements.",
        "Detail experience with relational databases such as Oracle, Snowflake, and SQL Server, including schema design and query optimization.",
        "Add certifications (e.g., Microsoft Certified: Azure Data Engineer Associate) and any training in HL7/FHIR if applicable.",
        "Showcase batch scripting (PowerShell/Bash) or Python scripting used for orchestration, automation, or data transformation."
      ],
      "linkedin_keywords": [
        "Azure Data Factory",
        "Informatica",
        "SSIS",
        "Snowflake",
        "Oracle",
        "PowerShell",
        "Python ETL",
        "Data Warehousing",
        "ETL Development",
        "Azure Data Engineering"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Jonathan_Gin.pdf",
      "job_title": "Senior Software Engineer (Ruby)",
      "job_company": "Luminor Group",
      "job_id": "4311546134",
      "skill_score": 0.3125,
      "semantic_score": 0.6062001351237388,
      "topic_score": 0.6062001351237388,
      "final_score": 0.47403507431805636,
      "resume_skills_count": 10,
      "job_skills_count": 11,
      "matching_skills_count": 5,
      "resume_text_length": 4260,
      "resume_skills": [
        "docker",
        "git",
        "github",
        "go",
        "java",
        "javascript",
        "mysql",
        "python",
        "sql",
        "typescript"
      ],
      "job_skills": [
        "aws",
        "ci/cd",
        "collaboration",
        "communication",
        "docker",
        "git",
        "gitlab",
        "java",
        "kubernetes",
        "mysql",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume shows some Ruby on Rails experience but lacks the depth in AWS, Kubernetes, CI/CD, secure coding practices, and senior‑level responsibilities required for this senior role.",
      "llm_recommendations": [
        "Highlight any AWS or cloud platform experience and detail specific services used (e.g., ECS, RDS, S3).",
        "Add experiences that demonstrate senior‑level ownership: architecture decisions, mentoring, code reviews, or leading feature development.",
        "Explicitly mention Linux proficiency, Docker/Kubernetes usage, and CI/CD pipelines (GitLab CI, Jenkins, or similar).",
        "Include background job processing (e.g., Sidekiq, Resque) and API integration patterns (REST/JSON, SOAP).",
        "Emphasize security and performance best practices, such as secure coding reviews, test coverage, and load testing."
      ],
      "linkedin_keywords": [
        "Ruby on Rails",
        "senior software engineer",
        "backend engineer",
        "cloud engineering",
        "Kubernetes",
        "Docker",
        "CI/CD",
        "AWS",
        "secure coding",
        "API development"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Jordan_Thompson_20251129_064621.pdf",
      "job_title": "Data Engineer AWS - Hybrid in NY",
      "job_company": "TechTriad",
      "job_id": "4317962527",
      "skill_score": 0.14285714285714285,
      "semantic_score": 0.765633326953652,
      "topic_score": 0.765633326953652,
      "final_score": 0.48538404411022285,
      "resume_skills_count": 19,
      "job_skills_count": 13,
      "matching_skills_count": 4,
      "resume_text_length": 1906,
      "resume_skills": [
        "a/b testing",
        "aws",
        "azure",
        "data visualization",
        "deep learning",
        "etl",
        "feature engineering",
        "git",
        "hadoop",
        "leadership",
        "machine learning",
        "power bi",
        "python",
        "r",
        "reporting",
        "spark",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "airflow",
        "aws",
        "ci/cd",
        "databricks",
        "java",
        "kubernetes",
        "lambda",
        "machine learning",
        "pyspark",
        "python",
        "s3",
        "scala",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks explicit experience with the specific Databricks and AWS data‑engineering tools, feature store implementation, and CI/CD workflows required for this senior data engineer role.",
      "llm_recommendations": [
        "Add detailed descriptions of data‑engineering projects that used Databricks (PySpark, Delta Lake) and AWS services such as S3, Glue, EMR, Lambda, and Kinesis.",
        "Highlight any experience with feature store design, Unity Catalog, or MLflow, as well as version control and CI/CD pipelines for data products.",
        "Include proficiency in Scala or Java and list any related projects or certifications.",
        "Emphasize large‑scale, production‑grade ETL or ML pipeline development, mentioning performance tuning and observability tools (e.g., Grafana, DataDog).",
        "Attach or summarize relevant certifications (AWS Certified Data Analytics, Databricks Certified Engineer)."
      ],
      "linkedin_keywords": [
        "data engineer",
        "AWS",
        "Databricks",
        "PySpark",
        "Delta Lake",
        "feature store",
        "CI/CD",
        "MLflow",
        "Airflow",
        "Kubernetes"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Jun_Li_20251129_064843.pdf",
      "job_title": "Machine Learning Engineer with verification",
      "job_company": "AmeriHealth Caritas",
      "job_id": "4332716222",
      "skill_score": 0.14285714285714285,
      "semantic_score": 0.7048678852420878,
      "topic_score": 0.7048678852420878,
      "final_score": 0.45196305116886254,
      "resume_skills_count": 25,
      "job_skills_count": 7,
      "matching_skills_count": 4,
      "resume_text_length": 2576,
      "resume_skills": [
        "a/b testing",
        "aws",
        "azure",
        "customer segmentation",
        "data analysis",
        "data pipeline",
        "decision making",
        "deep learning",
        "etl",
        "forecasting",
        "gradient boosting",
        "hadoop",
        "leadership",
        "logistic regression",
        "machine learning",
        "nlp",
        "product analytics",
        "python",
        "r",
        "reporting",
        "sales forecasting",
        "scala",
        "spark",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "azure",
        "ci/cd",
        "databricks",
        "machine learning",
        "pyspark",
        "python",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks specific experience with Azure Databricks, PySpark, MLflow, Feature Store, Azure DevOps, and model drift monitoring required for the role.",
      "llm_recommendations": [
        "Highlight any projects or work with Azure Databricks, including hands‑on PySpark coding and Delta Lake usage.",
        "Add experience deploying ML models with MLflow, managing model registry and feature store, and implementing CI/CD pipelines in Azure DevOps.",
        "Describe building and maintaining model and data drift monitoring solutions in production environments.",
        "Include relevant certifications such as Microsoft Certified: Azure AI Engineer Associate or Databricks Certified Data Scientist.",
        "Emphasize any end‑to‑end ML pipeline development and deployment on the Azure platform, detailing scalability and reliability aspects."
      ],
      "linkedin_keywords": [
        "Azure Databricks",
        "PySpark",
        "MLflow",
        "Azure DevOps",
        "Feature Store",
        "model monitoring",
        "data drift",
        "CI/CD",
        "Azure AI Engineer Associate",
        "data engineering",
        "ML Ops",
        "big data",
        "data science",
        "python",
        "sql",
        "machine learning",
        "cloud computing"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Justin-Tidrow-Resume-2025.pdf",
      "job_title": "Print Buyer with verification",
      "job_company": "HALO Branded Solutions",
      "job_id": "4332316113",
      "skill_score": 0.1,
      "semantic_score": 0.6785765098676998,
      "topic_score": 0.6785765098676998,
      "final_score": 0.4182170804272348,
      "resume_skills_count": 4,
      "job_skills_count": 7,
      "matching_skills_count": 1,
      "resume_text_length": 2016,
      "resume_skills": [
        "email marketing",
        "project management",
        "salesforce",
        "seo"
      ],
      "job_skills": [
        "communication",
        "negotiation",
        "organization skills",
        "procurement",
        "project management",
        "reporting",
        "spark"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks the procurement, vendor negotiation, budget, inventory management and print production experience required for the Print Buyer role.",
      "llm_recommendations": [
        "Highlight any procurement or vendor management experience, including negotiation outcomes and cost savings.",
        "Add experience managing print production projects or ordering materials, even if in a support capacity.",
        "Quantify budgeting or inventory responsibilities (e.g., $X budget size, inventory turnover, on‑time delivery rates).",
        "Emphasize Microsoft Excel proficiency, detailing use of formulas, data analysis, and reporting.",
        "Include any relevant certifications or training in supply chain, procurement, or inventory management."
      ],
      "linkedin_keywords": [
        "print procurement",
        "vendor management",
        "procurement specialist",
        "print production",
        "inventory management",
        "supply chain",
        "budget forecasting",
        "negotiation skills",
        "project management",
        "Excel."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Kas Kiatsukasem Resume.pdf",
      "job_title": "Power BI Analyst",
      "job_company": "Infomatics Corp",
      "job_id": "4319560521",
      "skill_score": 0.06666666666666667,
      "semantic_score": 0.6318751134723605,
      "topic_score": 0.6318751134723605,
      "final_score": 0.3775313124097982,
      "resume_skills_count": 12,
      "job_skills_count": 4,
      "matching_skills_count": 1,
      "resume_text_length": 4786,
      "resume_skills": [
        "fp&a",
        "go",
        "go-to-market",
        "gong",
        "hubspot",
        "nlp",
        "outreach",
        "revops",
        "salesforce",
        "sql",
        "tableau",
        "zoominfo"
      ],
      "job_skills": [
        "dashboard development",
        "forecasting",
        "power bi",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks specific experience with Power BI, revenue cycle management, and healthcare analytics that are essential for the role.",
      "llm_recommendations": [
        "Add concrete Power BI projects, dashboards, or certifications to the résumé.",
        "Highlight any roles involving revenue cycle management or billing analytics in a healthcare setting.",
        "Include examples of forecasting or predictive modeling work, especially using SQL or BI tools.",
        "Emphasize stakeholder‑management experience and the ability to translate business problems into data solutions.",
        "Tailor the résumé to showcase remote work capabilities and familiarity with industry‑specific compliance or standards."
      ],
      "linkedin_keywords": [
        "Power BI",
        "Revenue Cycle Management",
        "Healthcare Analytics",
        "SQL",
        "Data Visualization",
        "Forecasting",
        "Business Intelligence",
        "Remote Analyst",
        "Power BI Dashboard",
        "Health-Tech Data Analysis"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Keita_Tanaka_20251129_065145.pdf",
      "job_title": "DevSecOps",
      "job_company": "QualiZeal",
      "job_id": "4332406515",
      "skill_score": 0.47058823529411764,
      "semantic_score": 0.7549492266804454,
      "topic_score": 0.7549492266804454,
      "final_score": 0.6269867805565978,
      "resume_skills_count": 13,
      "job_skills_count": 12,
      "matching_skills_count": 8,
      "resume_text_length": 1755,
      "resume_skills": [
        "aws",
        "azure",
        "ci/cd",
        "customer segmentation",
        "data ingestion",
        "docker",
        "etl",
        "gcp",
        "git",
        "github",
        "kubernetes",
        "python",
        "sql"
      ],
      "job_skills": [
        "aws",
        "azure",
        "bash",
        "bitbucket",
        "ci/cd",
        "docker",
        "git",
        "github",
        "gitlab",
        "jenkins",
        "kubernetes",
        "python"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks several core DevSecOps skills required by the posting, such as Jenkins, Terraform, Ansible, and security scanning tools, and it does not fully cover testing integration or cloud infrastructure automation beyond basic cloud experience.",
      "llm_recommendations": [
        "Add explicit experience with Jenkins, GitLab CI, and Bitbucket pipelines.",
        "Gain hands‑on or documented projects using Terraform and Ansible for infrastructure‑as‑code.",
        "Include security tool usage (Trivy, Snyk, OWASP ZAP) and explain how you incorporated them into CI/CD workflows.",
        "Highlight experience with ELK/EFK stacks and any performance monitoring tasks beyond Prometheus/Grafana.",
        "Showcase collaboration with QA/testing teams using JUnit, PyTest, Cypress, or JMeter, and detail integration points."
      ],
      "linkedin_keywords": [
        "devops engineer",
        "devsecops",
        "CI/CD",
        "Jenkins",
        "GitLab CI",
        "Docker",
        "Kubernetes",
        "Helm",
        "Terraform",
        "Ansible",
        "Prometheus",
        "Grafana",
        "ELK",
        "AWS",
        "Azure",
        "security scanning",
        "Trivy",
        "Snyk",
        "OWASP ZAP",
        "Python",
        "Bash"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Keon_Lee_20251129_065330.pdf",
      "job_title": "Software Engineer- AI/ML, AWS Neuron Distributed Training with verification",
      "job_company": "Amazon Web Services (AWS)",
      "job_id": "4332896283",
      "skill_score": 0.09523809523809523,
      "semantic_score": 0.5819930683378629,
      "topic_score": 0.5819930683378629,
      "final_score": 0.36295333044296746,
      "resume_skills_count": 18,
      "job_skills_count": 5,
      "matching_skills_count": 2,
      "resume_text_length": 2463,
      "resume_skills": [
        "a/b testing",
        "agile",
        "c++",
        "communication",
        "deep learning",
        "docker",
        "etl",
        "git",
        "java",
        "kubernetes",
        "machine learning",
        "named entity recognition",
        "natural language processing",
        "nlp",
        "python",
        "scrum",
        "sentiment analysis",
        "sql"
      ],
      "job_skills": [
        "aws",
        "ec2",
        "leadership",
        "machine learning",
        "python"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks direct experience with AWS Neuron, large‑scale distributed training libraries (e.g., FSDP, DeepSpeed), and large language model deployment, which are key requirements for this role.",
      "llm_recommendations": [
        "Add any AWS-related projects or certifications, especially involving Inferentia or Trainium accelerators.",
        "Highlight experience with distributed training frameworks (FSDP, DeepSpeed, or similar) and mention scaling large transformer models.",
        "Emphasize work on fault‑tolerant, highly scalable systems and provide concrete performance tuning examples.",
        "Include leadership or architecture design responsibilities on large‑scale ML or software projects.",
        "Showcase any involvement with cloud ML services (e.g., SageMaker, Kubeflow) and integration with XLA or neuron compiler/runtime stacks."
      ],
      "linkedin_keywords": [
        "AWS Neuron",
        "DeepSpeed",
        "FSDP",
        "distributed training",
        "large language models",
        "GPT",
        "Transformer",
        "PyTorch",
        "TensorFlow",
        "AWS Inferentia",
        "AWS Trainium",
        "machine learning engineering",
        "cloud ML",
        "scalable systems",
        "model deployment",
        "ML ops",
        "performance tuning."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Kharee_Smith.pdf",
      "job_title": "Software Engineer - AI with verification",
      "job_company": "Newfold Digital",
      "job_id": "4331327644",
      "skill_score": 0.1111111111111111,
      "semantic_score": 0.6745429039001465,
      "topic_score": 0.6745429039001465,
      "final_score": 0.4209985971450806,
      "resume_skills_count": 9,
      "job_skills_count": 11,
      "matching_skills_count": 2,
      "resume_text_length": 3363,
      "resume_skills": [
        "azure",
        "collaboration",
        "github",
        "inventory management",
        "operational efficiency",
        "problem solving",
        "project management",
        "r",
        "salesforce"
      ],
      "job_skills": [
        "azure",
        "bitbucket",
        "c#",
        "ci/cd",
        "communication",
        "docker",
        "github",
        "jenkins",
        "leadership",
        "python",
        "roadmap planning"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume does not demonstrate the required backend engineering depth, LLM orchestration experience, or technical leadership needed for this role.",
      "llm_recommendations": [
        "Include concrete Python or .NET backend projects (e.g., FastAPI or ASP.NET APIs), detailing performance tuning, dependency injection, and REST design.",
        "Add experience with PostgreSQL, SQLAlchemy/SQLModel, and migration tools like Alembic, showing scalability or production‑grade database work.",
        "Highlight any AI/ML integration beyond chatbots—such as building RAG pipelines, vector stores (e.g., Azure AI Search, PGVector, Chroma) or managing multi‑agent orchestration.",
        "Showcase CI/CD workflow implementation, containerization (Docker), and basic IaC, with evidence of deployment to test/production environments.",
        "Provide evidence of engineering leadership: code reviews, mentoring, roadmap planning, or cross‑team coordination, even if in smaller or informal settings."
      ],
      "linkedin_keywords": [
        "Python",
        "FastAPI",
        ".NET",
        "C#",
        "Azure AI",
        "LLM",
        "Semantic Kernel",
        "RAG",
        "PostgreSQL",
        "Docker."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Kibaek Resume - Nov 2025 2P.pdf",
      "job_title": "AI Platform Engineer with verification",
      "job_company": "XpertDirect",
      "job_id": "4318466539",
      "skill_score": 0.19047619047619047,
      "semantic_score": 0.7402925800928378,
      "topic_score": 0.7402925800928378,
      "final_score": 0.4928752047653465,
      "resume_skills_count": 18,
      "job_skills_count": 7,
      "matching_skills_count": 4,
      "resume_text_length": 3886,
      "resume_skills": [
        "aws",
        "c#",
        "ci/cd",
        "docker",
        "git",
        "github",
        "hubspot",
        "java",
        "javascript",
        "jenkins",
        "leadership",
        "nlp",
        "pca",
        "postgresql",
        "python",
        "redshift",
        "typescript",
        "xgboost"
      ],
      "job_skills": [
        "airflow",
        "aws",
        "ci/cd",
        "docker",
        "gcp",
        "kubernetes",
        "python"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate’s resume lacks key experience with Kubernetes, Terraform, AWS EKS, Kubeflow, MLflow, and Airflow—core requirements for architecting and scaling an AI platform at the scale described.",
      "llm_recommendations": [
        "Add hands‑on experience with Kubernetes, Terraform, and AWS EKS (or equivalent) in the résumé.",
        "Highlight projects that used Kubeflow, MLflow, or Airflow to automate training, deployment, and rollback at scale.",
        "Showcase GPU or inference workload optimization across multi‑region clusters, including latency and cost metrics.",
        "Quantify MLOps impact (models deployed, inference throughput, production uptime).",
        "Include relevant certifications or training in container orchestration, cloud‑native MLOps tools, and CI/CD with Kubernetes."
      ],
      "linkedin_keywords": [
        "Kubernetes",
        "Terraform",
        "AWS EKS",
        "Kubeflow",
        "MLflow",
        "Airflow",
        "MLOps",
        "AI Platform Engineer",
        "Cloud Infrastructure",
        "AI/ML",
        "Scale",
        "AWS",
        "GCP",
        "Docker",
        "CI/CD",
        "GPU Optimization"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Kushagra_s Resume.pdf",
      "job_title": "Software Architect with verification",
      "job_company": "Allwyn Lottery Solutions",
      "job_id": "4332707295",
      "skill_score": 0.21739130434782608,
      "semantic_score": 0.3967685103416443,
      "topic_score": 0.3967685103416443,
      "final_score": 0.3160487676444261,
      "resume_skills_count": 18,
      "job_skills_count": 10,
      "matching_skills_count": 5,
      "resume_text_length": 5651,
      "resume_skills": [
        "agile",
        "aws",
        "ci/cd",
        "docker",
        "ec2",
        "git",
        "github",
        "java",
        "jenkins",
        "jira",
        "kubernetes",
        "lambda",
        "mysql",
        "postgresql",
        "project management",
        "s3",
        "sap",
        "sql"
      ],
      "job_skills": [
        "account management",
        "aws",
        "azure",
        "ci / cd",
        "communication",
        "docker",
        "java",
        "kubernetes",
        "postgresql",
        "scala"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate has relevant Java and cloud skills but lacks the 5+ years senior‑level architecture experience, domain knowledge in iGaming/lottery, and explicit focus on secure, scalable distributed systems required for the role.",
      "llm_recommendations": [
        "Highlight any architecture‑level projects, including design documents, diagrams, and decision logs to demonstrate senior‑level ownership.",
        "Add concrete experience with enterprise‑scale distributed systems, concurrency patterns, and security compliance (e.g., OWASP, ISO 27001).",
        "Include any domain or transferable experience in regulated or gaming-related platforms, or related areas such as financial services, to address the lottery/igaming requirement.",
        "Emphasize PostgreSQL expertise, cloud provider breadth (AWS, Azure, GCE), and CI/CD practices (Jenkins, GitHub Actions, SonarQube).",
        "Mention documentation and mentorship roles explicitly, and consider a brief Architecture or Security certification to strengthen the profile."
      ],
      "linkedin_keywords": [
        "software architecture",
        "enterprise architecture",
        "Java",
        "Spring Boot",
        "microservices",
        "Kubernetes",
        "Docker",
        "AWS",
        "PostgreSQL",
        "CI/CD",
        "distributed systems",
        "security architecture",
        "REST API",
        "iGaming",
        "lottery solutions",
        "AWS EC2",
        "Azure",
        "GCP",
        "Java EE",
        "Spring MVC",
        "JPA."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Kyson-Xu-Senior-Marketing-Manager-Resume-2025-.pdf",
      "job_title": "Marketing Manager",
      "job_company": "Hangry",
      "job_id": "4319148231",
      "skill_score": 0.2,
      "semantic_score": 0.6295280268929022,
      "topic_score": 0.6295280268929022,
      "final_score": 0.4362404147910962,
      "resume_skills_count": 11,
      "job_skills_count": 7,
      "matching_skills_count": 3,
      "resume_text_length": 9672,
      "resume_skills": [
        "collaboration",
        "communication",
        "go",
        "go-to-market",
        "hubspot",
        "leadership",
        "mailchimp",
        "salesforce",
        "sem",
        "seo",
        "sql"
      ],
      "job_skills": [
        "agile",
        "collaboration",
        "communication",
        "cross-functional collaboration",
        "leadership",
        "stakeholder management",
        "teamwork"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate’s 10+ years of senior marketing experience, proven cross‑functional leadership, data‑driven KPI optimization, budget management, multi‑brand initiatives, and strong communication and stakeholder skills align well with Hangry’s Marketing Manager role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Lee_lab_associate.pdf",
      "job_title": "Software Engineer, Data Integration",
      "job_company": "Chan Zuckerberg Biohub Network",
      "job_id": "4311154172",
      "skill_score": 0.0,
      "semantic_score": 0.29689151607650766,
      "topic_score": 0.29689151607650766,
      "final_score": 0.1632903338420792,
      "resume_skills_count": 0,
      "job_skills_count": 5,
      "matching_skills_count": 0,
      "resume_text_length": 2828,
      "resume_skills": [],
      "job_skills": [
        "communication",
        "data integration",
        "javascript",
        "problem solving",
        "python"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks the required software development, database, and imaging skills needed for the software engineer, data integration role.",
      "llm_recommendations": [
        "Highlight any programming experience (Python, JavaScript, SQL) and add software projects, even personal or academic, to demonstrate coding proficiency.",
        "Obtain a formal CS or related degree, or complete certificates (e.g., computer science fundamentals, data structures, database design).",
        "Include specific experience with front‑end frameworks (React, Next.js) and web technologies (HTML, CSS, REST APIs).",
        "Showcase data‑intensive work with imaging formats (TIFF, Zarr) or any image‑processing/analysis tools to align with the imaging focus.",
        "Emphasize collaboration with scientists and any cross‑functional projects that required data integration or workflow automation."
      ],
      "linkedin_keywords": [
        "software engineer",
        "data integration",
        "Python",
        "JavaScript",
        "React",
        "Next.js",
        "database design",
        "high-performance computing",
        "image processing",
        "bioinformatics",
        "scientific computing",
        "web development",
        "open-source",
        "collaboration",
        "laboratory data."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Leena_Patel_20251129_055926.pdf",
      "job_title": "Sr Data Analyst with verification",
      "job_company": "Horizontal Talent",
      "job_id": "4318091470",
      "skill_score": 0.23529411764705882,
      "semantic_score": 0.6808016515733792,
      "topic_score": 0.6808016515733792,
      "final_score": 0.480323261306535,
      "resume_skills_count": 13,
      "job_skills_count": 8,
      "matching_skills_count": 4,
      "resume_text_length": 1770,
      "resume_skills": [
        "a/b testing",
        "bigquery",
        "data visualization",
        "etl",
        "git",
        "leadership",
        "power bi",
        "python",
        "reporting",
        "snowflake",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "azure",
        "ci/cd",
        "etl",
        "oracle",
        "python",
        "root cause analysis",
        "snowflake",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The résumé lacks key Azure Data Factory experience, Informatica/SSIS usage, Oracle/SQL Server familiarity, and the specific scripting and HL7/FHIR knowledge required for the Sr. Data Analyst role.",
      "llm_recommendations": [
        "Highlight any Azure Data Factory or related cloud data pipeline projects; if none, pursue relevant training or certifications.",
        "Include experience or coursework with Informatica PowerCenter, SSIS, or analogous ETL tools.",
        "Emphasize work with relational databases such as Oracle, SQL Server, or similar systems.",
        "Detail batch or PowerShell scripting projects, as well as Python scripting contributions.",
        "Add any exposure to HL7/FHIR standards or health data compliance frameworks, and mention Azure certifications (e.g., AZ-900, DP-203) if achieved."
      ],
      "linkedin_keywords": [
        "Azure Data Factory",
        "Snowflake",
        "ETL",
        "Informatica PowerCenter",
        "SSIS",
        "Python scripting",
        "PowerShell scripting",
        "Oracle",
        "SQL Server",
        "HL7",
        "FHIR",
        "data modeling",
        "data pipeline",
        "cloud data integration",
        "data engineering",
        "analytics engineer."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Leila_Nguyen_20251129_062022.pdf",
      "job_title": "Senior Encore Developer-OMBP",
      "job_company": "Navy Federal Credit Union",
      "job_id": "4323911447",
      "skill_score": 0.1111111111111111,
      "semantic_score": 0.46049201488494873,
      "topic_score": 0.46049201488494873,
      "final_score": 0.3032706081867218,
      "resume_skills_count": 17,
      "job_skills_count": 3,
      "matching_skills_count": 2,
      "resume_text_length": 2759,
      "resume_skills": [
        "agile",
        "data visualization",
        "decision making",
        "etl",
        "forecasting",
        "machine learning",
        "outreach",
        "power bi",
        "project management",
        "python",
        "r",
        "reporting",
        "sales forecasting",
        "snowflake",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "change management",
        "reporting",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate’s background is centered on data analysis and business intelligence, lacking the required VB.NET/VB6 development, SQL Server, TFS, and banking application support experience for the Senior Encore Developer role.",
      "llm_recommendations": [
        "Highlight any past coding or software development experience, especially in VB.NET or VBA, and quantify contributions (e.g., “developed a VB.NET module for data ingestion”).",
        "Add concrete examples of client/server application support or maintenance, including specifics such as “supported Oracle/SQL Server‑based retail POS systems in a distributed environment.”",
        "Include experience with version control systems (TFS, Git, SVN) and detail any code‑review or release‑management activities.",
        "Showcase any work with financial or teller/banking systems or exposure to regulated banking environments.",
        "Obtain or list any certifications in .NET development, SQL Server, or banking software platforms."
      ],
      "linkedin_keywords": [
        "VB.NET developer",
        "VB6",
        "SQL Server",
        "TFS",
        ".NET developer",
        "application support",
        "banking software",
        "client/server",
        "legacy migration",
        "financial systems analyst",
        ".NET application",
        "data integration",
        "system maintenance",
        "enterprise software",
        "banking IT support"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Lena_Müller_20251129_063235.pdf",
      "job_title": "ML/AI Engineer with verification",
      "job_company": "Munich Re",
      "job_id": "4310483254",
      "skill_score": 0.2222222222222222,
      "semantic_score": 0.6766263851487412,
      "topic_score": 0.6766263851487412,
      "final_score": 0.4721445118318076,
      "resume_skills_count": 17,
      "job_skills_count": 16,
      "matching_skills_count": 6,
      "resume_text_length": 1714,
      "resume_skills": [
        "airflow",
        "aws",
        "data analytics",
        "data visualization",
        "docker",
        "feature engineering",
        "forecasting",
        "git",
        "github",
        "machine learning",
        "python",
        "r",
        "reporting",
        "sales forecasting",
        "spark",
        "sql",
        "statistical analysis"
      ],
      "job_skills": [
        "azure",
        "ci/cd",
        "collaboration",
        "data analysis",
        "databricks",
        "docker",
        "elt",
        "etl",
        "feature engineering",
        "git",
        "leadership",
        "machine learning",
        "numpy",
        "pandas",
        "python",
        "spark"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks key experience with Databricks, Azure data services, advanced cloud and CI/CD tooling, and does not meet the advanced degree or leadership level required for this senior role.",
      "llm_recommendations": [
        "Highlight any work with Azure Databricks, Azure Data Lake Store, or Azure AI services, or obtain related certifications.",
        "Include experience building Databricks ETL/ELT pipelines, feature stores, and using libraries such as mlflow, dvc, and dbt.",
        "Detail experience with CI/CD pipelines in Azure Pipelines or equivalent, and demonstrate containerization (Docker) plus orchestration for model inference endpoints.",
        "Emphasize leadership responsibilities, such as managing a team of data engineers or ML engineers, and provide quantitative impact metrics.",
        "Pursue or mention a higher degree (Master’s or PhD) in computer science, engineering, or related field to match the preferred qualifications."
      ],
      "linkedin_keywords": [
        "Databricks",
        "Azure Data Lake",
        "Azure AI",
        "MLflow",
        "DVC",
        "dbt",
        "Spark",
        "Data Engineering",
        "CI/CD",
        "Azure Pipelines",
        "Docker",
        "Python",
        "TensorFlow",
        "PyTorch",
        "Feature Engineering",
        "Big Data",
        "Leadership",
        "Machine Learning Engineer",
        "Senior ML Engineer"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Liam_Chen_20251129_062105.pdf",
      "job_title": "AI Software Engineer with verification",
      "job_company": "XpertDirect",
      "job_id": "4318464252",
      "skill_score": 0.2857142857142857,
      "semantic_score": 0.7463685051689027,
      "topic_score": 0.7463685051689027,
      "final_score": 0.5390741064143251,
      "resume_skills_count": 19,
      "job_skills_count": 8,
      "matching_skills_count": 6,
      "resume_text_length": 1981,
      "resume_skills": [
        "a/b testing",
        "aws",
        "ci/cd",
        "data visualization",
        "decision trees",
        "docker",
        "feature engineering",
        "hadoop",
        "kubernetes",
        "linear regression",
        "machine learning",
        "numpy",
        "pandas",
        "pyspark",
        "python",
        "r",
        "spark",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "aws",
        "docker",
        "forecasting",
        "lambda",
        "machine learning",
        "pandas",
        "python",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate’s experience does not cover the key domain (energy, IoT, SCADA) and specific tools (Kafka, FastAPI, AWS Lambda, time‑series data) highlighted in the job posting.",
      "llm_recommendations": [
        "Highlight any energy or utilities industry projects, including work with grid or sensor data.",
        "Add experience with time‑series analysis or SCADA systems, and describe relevant datasets.",
        "Include hands‑on use of Kafka, FastAPI, and AWS Lambda in model deployment or data pipelines.",
        "Emphasize familiarity with IoT sensor streams (e.g., turbines, smart meters) and related data ingestion.",
        "Tailor the summary to mention renewable energy analytics and predictive modeling for power demand or asset health."
      ],
      "linkedin_keywords": [
        "AI Software Engineer",
        "Machine Learning Engineer",
        "Renewable Energy Analytics",
        "Energy Optimization",
        "Time‑Series Analytics",
        "SCADA",
        "IoT Data",
        "AWS Sagemaker",
        "Kafka",
        "FastAPI",
        "AWS Lambda",
        "ML Ops",
        "Python",
        "TensorFlow",
        "PyTorch",
        "Docker"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Lin_Wei_Chen_20251129_065230.pdf",
      "job_title": "Data Scientist, Watchlist",
      "job_company": "RemoteHunter",
      "job_id": "4319177929",
      "skill_score": 0.2857142857142857,
      "semantic_score": 0.6451112454997054,
      "topic_score": 0.6451112454997054,
      "final_score": 0.48338261359626655,
      "resume_skills_count": 16,
      "job_skills_count": 11,
      "matching_skills_count": 6,
      "resume_text_length": 2523,
      "resume_skills": [
        "a/b testing",
        "data analysis",
        "data visualization",
        "deep learning",
        "docker",
        "etl",
        "hadoop",
        "kubernetes",
        "leadership",
        "machine learning",
        "python",
        "r",
        "spark",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "adaptability",
        "aws",
        "databricks",
        "feature engineering",
        "hadoop",
        "machine learning",
        "python",
        "r",
        "spark",
        "sql",
        "xgboost"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate’s 10‑year data science background, strong Python/R skills, experience with Spark, Hadoop, TensorFlow, Scikit‑learn, and large‑scale data pipelines, along with demonstrated work on financial risk scoring and cross‑functional stakeholder communication, aligns well with the fraud‑risk product focus and technical requirements of the role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Lin_Zhou_20251129_063736.pdf",
      "job_title": "AI/ML Software Engineer with verification",
      "job_company": "Guidehouse",
      "job_id": "4332300185",
      "skill_score": 0.21739130434782608,
      "semantic_score": 0.6601328062852851,
      "topic_score": 0.6601328062852851,
      "final_score": 0.46089913041342856,
      "resume_skills_count": 15,
      "job_skills_count": 13,
      "matching_skills_count": 5,
      "resume_text_length": 1864,
      "resume_skills": [
        "agile",
        "aws",
        "data analysis",
        "data cleaning",
        "data visualization",
        "docker",
        "feature engineering",
        "git",
        "javascript",
        "machine learning",
        "python",
        "reporting",
        "scrum",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "aws",
        "communication",
        "deep learning",
        "javascript",
        "machine learning",
        "numpy",
        "oracle",
        "outreach",
        "pandas",
        "postgresql",
        "python",
        "sql",
        "typescript"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The résumé demonstrates solid 5‑year experience in machine Learning and Python, but it lacks specific experience with RAG systems, LangChain/Haystack frameworks, conversational AI, AWS SDK/Infrastructure‑as‑Code, JavaScript/TypeScript, and the security clearance requirement.",
      "llm_recommendations": [
        "Highlight any work with Retrieval‑Augmented Generation, LangChain, Haystack, or similar frameworks and discuss achieved metrics.",
        "Add specific projects that build or maintain chatbots or conversational agents, mentioning libraries like Rasa or OpenAI API integration.",
        "Include experience with AWS SDKs (e.g., boto3), infrastructure‑as‑Code tools (Terraform or CloudFormation), and deployment of models to production environments.",
        "Show proficiency in JavaScript/TypeScript (including any TypeScript codebases or libraries used) and relational database work with PostgreSQL or similar.",
        "Mention eligibility for or acquisition of a Public Trust or other federal security clearance and any related compliance experience."
      ],
      "linkedin_keywords": [
        "AI engineer",
        "Retrieval‑Augmented Generation",
        "LangChain",
        "Haystack",
        "Conversational AI",
        "Chatbot development",
        "AWS SageMaker",
        "AWS SDK",
        "Terraform",
        "Pytorch",
        "TensorFlow",
        "Python",
        "JavaScript",
        "TypeScript",
        "PostgreSQL",
        "SQL",
        "Docker",
        "Cloud Deployment",
        "Machine Learning Engineer",
        "Data Pipelines"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Linh_Nguyen_20251129_061818.pdf",
      "job_title": "Machine Learning Engineer",
      "job_company": "Sepal AI",
      "job_id": "4319216770",
      "skill_score": 0.19047619047619047,
      "semantic_score": 0.6449410895950602,
      "topic_score": 0.6449410895950602,
      "final_score": 0.4404318849915688,
      "resume_skills_count": 19,
      "job_skills_count": 6,
      "matching_skills_count": 4,
      "resume_text_length": 2221,
      "resume_skills": [
        "aws",
        "ci/cd",
        "data analysis",
        "data visualization",
        "deep learning",
        "docker",
        "feature engineering",
        "forecasting",
        "gcp",
        "hadoop",
        "machine learning",
        "numpy",
        "pandas",
        "power bi",
        "python",
        "r",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "data ingestion",
        "forecasting",
        "machine learning",
        "pandas",
        "python",
        "xgboost"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate has strong 6-year experience as a machine learning engineer, has built and deployed predictive models (including time-series forecasting and classification), and is fluent in Python with scikit-learn, pandas, and other core ML libraries.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Luca_Moretti_20251129_064336.pdf",
      "job_title": "Data Analyst",
      "job_company": "Helic & Co.",
      "job_id": "4332495253",
      "skill_score": 0.3333333333333333,
      "semantic_score": 0.6300830465505702,
      "topic_score": 0.6300830465505702,
      "final_score": 0.4965456756028136,
      "resume_skills_count": 10,
      "job_skills_count": 10,
      "matching_skills_count": 5,
      "resume_text_length": 1746,
      "resume_skills": [
        "data visualization",
        "etl",
        "forecasting",
        "power bi",
        "python",
        "r",
        "reporting",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "communication",
        "critical thinking",
        "data analysis",
        "data analytics",
        "data visualization",
        "looker",
        "power bi",
        "python",
        "sql",
        "tableau"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate’s six‑year data analyst experience, strong SQL/Python/TABLEAU skills, large‑scale ETL work, and proven statistical analysis align well with the job’s requirements for data collection, cleaning, visualization, and stakeholder communication.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Lukas_Weber_20251129_060558.pdf",
      "job_title": "Sr. Data Engineer",
      "job_company": "RemoteHunter",
      "job_id": "4319158781",
      "skill_score": 0.14814814814814814,
      "semantic_score": 0.7180125377193189,
      "topic_score": 0.7180125377193189,
      "final_score": 0.4615735624122921,
      "resume_skills_count": 20,
      "job_skills_count": 11,
      "matching_skills_count": 4,
      "resume_text_length": 2594,
      "resume_skills": [
        "agile",
        "airflow",
        "aws",
        "data visualization",
        "decision making",
        "docker",
        "etl",
        "gcp",
        "git",
        "hadoop",
        "leadership",
        "machine learning",
        "power bi",
        "python",
        "r",
        "reporting",
        "spark",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "airflow",
        "athena",
        "ci/cd",
        "communication",
        "dynamodb",
        "elt",
        "etl",
        "java",
        "python",
        "sql",
        "teamwork"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate has strong data engineering and ML experience but lacks explicit expertise with key technologies (ClickHouse, PostgreSQL, Athena, Kafka) and real‑time, low‑latency pipeline design required for this Sr. Data Engineer role.",
      "llm_recommendations": [
        "Highlight any experience with ClickHouse, PostgreSQL, Athena, or similar analytical databases.",
        "Provide concrete examples of batch and streaming pipeline design, including use of Kafka, Kinesis, or Spark Structured Streaming.",
        "Detail real‑time data processing projects that demonstrate low‑latency, high-throughput performance and compliance/security controls.",
        "Add certifications or coursework in distributed data systems, cloud architecture (AWS/GCP/Azure), and data security/privacy frameworks.",
        "Emphasize workflow orchestration (Airflow, Step Functions) projects that include CI/CD, monitoring, and automated testing."
      ],
      "linkedin_keywords": [
        "data engineer",
        "distributed data systems",
        "ClickHouse",
        "PostgreSQL",
        "Athena",
        "Apache Kafka",
        "real‑time analytics",
        "Airflow",
        "data pipeline architecture",
        "cloud data engineering",
        "AWS",
        "GCP",
        "data modeling",
        "query optimization",
        "low‑latency data processing"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Léa_Dubois_20251129_062337.pdf",
      "job_title": "Senior Analyst, Data Science with verification",
      "job_company": "Liberty Mutual Insurance",
      "job_id": "4332742602",
      "skill_score": 0.07692307692307693,
      "semantic_score": 0.6041135427884232,
      "topic_score": 0.6041135427884232,
      "final_score": 0.36687783314901734,
      "resume_skills_count": 10,
      "job_skills_count": 4,
      "matching_skills_count": 1,
      "resume_text_length": 1668,
      "resume_skills": [
        "collaboration",
        "data analysis",
        "data visualization",
        "docker",
        "feature engineering",
        "kubernetes",
        "machine learning",
        "python",
        "r",
        "sql"
      ],
      "job_skills": [
        "customer segmentation",
        "data ingestion",
        "feature engineering",
        "logistic regression"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate has the required 5‑year experience, bachelor’s degree in mathematics, strong Python/R/SQL skills, and extensive experience in building, deploying, and explaining predictive models, matching the core responsibilities of the Senior Analyst, Data Science role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "MARC_CHEN.pdf",
      "job_title": "Digital Growth & AEO Specialist (Contract-to-Hire) with verification",
      "job_company": "AdOmni",
      "job_id": "4316964338",
      "skill_score": 0.2,
      "semantic_score": 0.588405514454514,
      "topic_score": 0.588405514454514,
      "final_score": 0.41362303294998265,
      "resume_skills_count": 9,
      "job_skills_count": 3,
      "matching_skills_count": 2,
      "resume_text_length": 2383,
      "resume_skills": [
        "a/b testing",
        "digital marketing",
        "google ads",
        "google analytics",
        "hubspot",
        "mailchimp",
        "power bi",
        "ppc",
        "seo"
      ],
      "job_skills": [
        "client success",
        "digital marketing",
        "seo"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate meets the years of experience requirement but lacks key AEO, schema/structured‑data, GA4/GSC, and paid‑media/CRM analytics expertise required for the role.",
      "llm_recommendations": [
        "Add any AEO, answer‑engine optimization, or AI‑search related projects, highlighting schema, structured data, and featured‑answer strategy.",
        "Include a section on GA4 and GSC proficiency, with metrics on traffic and visibility improvements from those tools.",
        "Detail paid‑search and paid‑media campaigns, showing attribution models, CPM/CPA, and how they fed into the sales pipeline.",
        "Describe CRM and marketing‑automation tools used (e.g., HubSpot, Marketo) and how you built dashboards for real‑time pipeline tracking.",
        "Quantify growth impact (e.g., “increased qualified leads by 30% YOY” or “boosted structured‑data coverage to 95% of key pages”) to demonstrate data‑driven success."
      ],
      "linkedin_keywords": [
        "digital growth",
        "AEO",
        "answer engine optimization",
        "SEO structured data",
        "paid search",
        "GA4",
        "GSC",
        "CRM automation",
        "B2B marketing",
        "demand generation",
        "AI search",
        "analytics dashboards",
        "video advertising",
        "funnel optimization",
        "data‑driven marketing"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Mara_Nganga_20251129_141031.pdf",
      "job_title": "Frontend Developer with verification",
      "job_company": "IoThink Solutions",
      "job_id": "4278788470",
      "skill_score": 0.16666666666666666,
      "semantic_score": 0.4267128745783695,
      "topic_score": 0.4267128745783695,
      "final_score": 0.3096920810181032,
      "resume_skills_count": 13,
      "job_skills_count": 8,
      "matching_skills_count": 3,
      "resume_text_length": 2136,
      "resume_skills": [
        "aws",
        "bert",
        "ci/cd",
        "data ingestion",
        "data pipeline",
        "docker",
        "embeddings",
        "git",
        "kubernetes",
        "natural language processing",
        "nlp",
        "python",
        "spark"
      ],
      "job_skills": [
        "c#",
        "ci/cd",
        "collaboration",
        "communication",
        "docker",
        "javascript",
        "kubernetes",
        "typescript"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks the required frontend development experience, relevant frameworks (Angular, Blazor, JavaScript/TypeScript), and formal computer science education needed for the role.",
      "llm_recommendations": [
        "Include specific frontend projects demonstrating use of Angular or .NET Blazor, JavaScript, and TypeScript.",
        "Add any experience building user interfaces, mockups, or cross‑browser compatible web applications.",
        "Highlight academic background or coursework in computer science, software engineering, or a related field.",
        "Emphasize IoT-related work that involved front‑end dashboards or real‑time data visualization.",
        "Showcase soft skills and collaboration experiences, especially in DevOps, CI/CD pipelines, and API integration."
      ],
      "linkedin_keywords": [
        "frontend developer",
        "Angular",
        ".NET Blazor",
        "JavaScript",
        "TypeScript",
        "React",
        "UI/UX",
        "IoT",
        "Docker",
        "Kubernetes",
        "CI/CD",
        "API development",
        "cloud computing."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Mara_OConnor_20251129_141129.pdf",
      "job_title": "Senior DevOps Engineer with verification",
      "job_company": "Munich Re",
      "job_id": "4311393243",
      "skill_score": 0.28,
      "semantic_score": 0.7735930911777348,
      "topic_score": 0.7735930911777348,
      "final_score": 0.5514762001477542,
      "resume_skills_count": 19,
      "job_skills_count": 13,
      "matching_skills_count": 7,
      "resume_text_length": 2237,
      "resume_skills": [
        "agile",
        "airflow",
        "aws",
        "azure",
        "ci/cd",
        "data analytics",
        "docker",
        "gcp",
        "gitlab",
        "gradient boosting",
        "jenkins",
        "kubernetes",
        "power bi",
        "python",
        "r",
        "reporting",
        "scrum",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "aws",
        "azure",
        "ci/cd",
        "collaboration",
        "communication",
        "docker",
        "gcp",
        "github",
        "javascript",
        "kubernetes",
        "machine learning",
        "python",
        "typescript"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate’s extensive experience with Kubernetes, Docker, Terraform, major cloud platforms, CI/CD pipelines, Python, and monitoring tools directly matches the core technical requirements for the Senior DevOps Engineer role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Marcus_Chen_20251129_062636.pdf",
      "job_title": "Senior Software Engineer",
      "job_company": "Delmar Nord",
      "job_id": "4261066094",
      "skill_score": 0.13636363636363635,
      "semantic_score": 0.7113125748224614,
      "topic_score": 0.7113125748224614,
      "final_score": 0.4525855525159901,
      "resume_skills_count": 19,
      "job_skills_count": 6,
      "matching_skills_count": 3,
      "resume_text_length": 2316,
      "resume_skills": [
        "agile",
        "airflow",
        "aws",
        "bigquery",
        "data analysis",
        "data visualization",
        "decision making",
        "etl",
        "gcp",
        "git",
        "java",
        "machine learning",
        "product analytics",
        "python",
        "redshift",
        "reporting",
        "spark",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "aws",
        "ci/cd",
        "communication",
        "data analytics",
        "git",
        "python"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume shows strong Python and AWS experience but lacks the specific containerized microservices, EKS, Terraform, CI/CD, and cloud security expertise required for this senior software engineer role at a multi‑strategy hedge fund.",
      "llm_recommendations": [
        "Highlight any containerization/Kubernetes (EKS) projects, including Docker images and deployment workflows.",
        "Include experience with IaC tools such as Terraform or CloudFormation and describe how you built or maintained AWS infrastructure.",
        "Detail CI/CD pipeline implementations (e.g., GitHub Actions, Jenkins, CodePipeline) that enforce security and automated testing.",
        "Add any real‑time data processing or low‑latency microservice work (e.g., streaming, event‑driven architecture).",
        "Emphasize communication and stakeholder collaboration, especially cross‑functional teams in a finance setting."
      ],
      "linkedin_keywords": [
        "Python",
        "AWS",
        "Terraform",
        "Kubernetes",
        "EKS",
        "Docker",
        "CI/CD",
        "Cloud Architecture",
        "Microservices",
        "Cloud Security",
        "Data Engineering",
        "ETL",
        "Machine Learning",
        "Financial Technology",
        "Real‑Time Systems",
        "Cloud Infrastructure",
        "Hedge Fund",
        "Cloud Native",
        "GCP"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Marcus_Thompson_20251129_061516.pdf",
      "job_title": "Data Analytics Software Engineer (Trading)",
      "job_company": "Fintal Partners",
      "job_id": "4318639448",
      "skill_score": 0.25925925925925924,
      "semantic_score": 0.7139577269554341,
      "topic_score": 0.7139577269554341,
      "final_score": 0.5093434164921554,
      "resume_skills_count": 21,
      "job_skills_count": 13,
      "matching_skills_count": 7,
      "resume_text_length": 2341,
      "resume_skills": [
        "a/b testing",
        "aws",
        "data analysis",
        "data analytics",
        "data cleaning",
        "data visualization",
        "decision making",
        "etl",
        "feature engineering",
        "leadership",
        "machine learning",
        "numpy",
        "pandas",
        "python",
        "r",
        "redshift",
        "reporting",
        "risk analysis",
        "s3",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "data analysis",
        "data analytics",
        "databricks",
        "docker",
        "kubernetes",
        "machine learning",
        "numpy",
        "pandas",
        "python",
        "reporting",
        "scipy",
        "snowflake",
        "spark"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks direct experience with high‑frequency trading, real‑time data pipelines, distributed analytics frameworks, and HFT/prop trading environments required by the role.",
      "llm_recommendations": [
        "Explicitly detail any experience building low‑latency, real‑time data pipelines or working with streaming data (e.g., Kafka, Spark Structured Streaming).",
        "Highlight exposure to distributed or big‑data engines such as Spark, Dask, Trino, or Dremio, and data lake technologies like Snowflake, Databricks, or parquet/iceberg.",
        "Include knowledge of Linux, containerization (Docker/Kubernetes), and JupyterHub/JupyterLab usage in production settings.",
        "Add any past roles or projects in quantitative finance, HFT, or prop trading, and describe collaboration with quants and back‑testing systems.",
        "Consider pursuing (or adding) certifications or coursework in data engineering, HFT, or financial analytics, and showcase any relevant LLM or ML model deployment in low‑latency contexts."
      ],
      "linkedin_keywords": [
        "data engineering",
        "real‑time analytics",
        "high‑frequency trading",
        "quantitative finance",
        "Spark",
        "SQL",
        "Pandas",
        "NumPy",
        "distributed computing",
        "Docker",
        "Kubernetes",
        "JupyterLab",
        "Snowflake",
        "Dask",
        "machine learning",
        "Python",
        "backtesting",
        "data pipelines",
        "Linux",
        "LLM tools."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Marina_Alvarez_20251129_064217.pdf",
      "job_title": "AI/Machine Learning (NLP) Engineer with verification",
      "job_company": "Aslase",
      "job_id": "4332070318",
      "skill_score": 0.2,
      "semantic_score": 0.7048593197637216,
      "topic_score": 0.7048593197637216,
      "final_score": 0.4776726258700469,
      "resume_skills_count": 15,
      "job_skills_count": 9,
      "matching_skills_count": 4,
      "resume_text_length": 1675,
      "resume_skills": [
        "aws",
        "bert",
        "data analysis",
        "docker",
        "feature engineering",
        "machine learning",
        "nlp",
        "numpy",
        "pandas",
        "python",
        "reporting",
        "sentiment analysis",
        "sql",
        "tableau",
        "text classification"
      ],
      "job_skills": [
        "ci/cd",
        "docker",
        "embeddings",
        "github",
        "kubernetes",
        "machine learning",
        "nlp",
        "python",
        "spark"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks key LLM API integration, LangChain/LLamaIndex usage, vector database experience, and no‑code prototyping tools that the role specifically requires.",
      "llm_recommendations": [
        "Highlight experience with OpenAI, Cohere, or Gemini APIs and show how you fine‑tuned and deployed LLMs.",
        "Include projects that used LangChain, LlamaIndex, or CrewAI for building agent‑based or pipeline systems.",
        "Demonstrate vector‑database integration (Pinecone, Weaviate, or Faiss) and RAG‑style retrieval pipelines.",
        "Add details of CI/CD, MLOps tools (MLflow, Kubernetes), and no‑code platforms (n8n, Make) used in model deployment.",
        "Mention any AI ethics, bias‑mitigation work, open‑source contributions, or streaming data pipeline experience."
      ],
      "linkedin_keywords": [
        "AI Engineer",
        "NLP Engineer",
        "LLM",
        "OpenAI",
        "LangChain",
        "Pinecone",
        "Vector DB",
        "Prompt Engineering",
        "MLOps",
        "No-Code Automation",
        "RAG",
        "Streaming Data",
        "AI Ethics",
        "CI/CD",
        "AWS Sagemaker",
        "Docker",
        "MLflow",
        "Azure",
        "TensorFlow",
        "PyTorch"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Marta_Kovács_20251129_061159.pdf",
      "job_title": "Data Analytics Software Engineer (Trading)",
      "job_company": "Fintal Partners",
      "job_id": "4318639448",
      "skill_score": 0.25,
      "semantic_score": 0.6757634878158761,
      "topic_score": 0.6757634878158761,
      "final_score": 0.4841699182987319,
      "resume_skills_count": 12,
      "job_skills_count": 13,
      "matching_skills_count": 5,
      "resume_text_length": 2601,
      "resume_skills": [
        "airflow",
        "data analysis",
        "elt",
        "etl",
        "power bi",
        "python",
        "reporting",
        "snowflake",
        "spark",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "data analysis",
        "data analytics",
        "databricks",
        "docker",
        "kubernetes",
        "machine learning",
        "numpy",
        "pandas",
        "python",
        "reporting",
        "scipy",
        "snowflake",
        "spark"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The résumé lacks direct high‑frequency trading, low‑latency pipeline, and specific Python library (pandas, numpy, scikit‑learn) experience required for the role.",
      "llm_recommendations": [
        "Highlight any trading, financial, or quantitative research projects, even if on a small scale, to demonstrate domain relevance.",
        "Detail hands‑on use of Python data libraries (pandas, polars, numpy, scipy, scikit‑learn) and illustrate performance optimisations on large datasets.",
        "Include experience with JupyterHub/JupyterLab, showing ability to support multi‑user environments and advanced analytics notebooks.",
        "Provide examples of real‑time, low‑latency data pipelines, such as streaming ingestion, in‑memory processing, or dashboarding that meets strict latency requirements.",
        "Mention familiarity with Linux, Docker/Kubernetes, and LLM productivity tools (e.g., prompt‑engineering, code generation) to align with preferred qualifications."
      ],
      "linkedin_keywords": [
        "high‑frequency trading",
        "quantitative research",
        "real‑time data pipelines",
        "low‑latency analytics",
        "Python data engineering",
        "Spark",
        "Snowflake",
        "Jupyter",
        "pandas",
        "Linux",
        "Docker",
        "Kubernetes",
        "LLM tools",
        "machine learning",
        "data lake."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Matt.pdf",
      "job_title": "Front Office Interest Rates Quantitative Analyst - New York with verification",
      "job_company": "Santander Bank, N.A.",
      "job_id": "4308348996",
      "skill_score": 0.25,
      "semantic_score": 0.46326334883978104,
      "topic_score": 0.46326334883978104,
      "final_score": 0.36729484186187955,
      "resume_skills_count": 3,
      "job_skills_count": 2,
      "matching_skills_count": 1,
      "resume_text_length": 4201,
      "resume_skills": [
        "java",
        "leadership",
        "python"
      ],
      "job_skills": [
        "c++",
        "python"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks the required quantitative finance experience, fixed‑income knowledge, programming skill set (Python, C++, Rust), and industry background for a Front Office Interest Rates Quant role.",
      "llm_recommendations": [
        "Seek internships or project roles focused on interest rate models, fixed‑income pricing, or risk analytics to build relevant experience.",
        "Add coursework or certifications in financial engineering, quantitative finance, or derivatives (e.g., CFA Level II, FRM, or online courses in fixed‑income analytics).",
        "Highlight and deepen proficiency in Python, C++, and Rust through personal or academic projects, and document any modeling or algorithm development work.",
        "Incorporate concrete examples of probability, statistics, and mathematical modeling (e.g., option pricing theory, stochastic calculus) in the resume and cover letter.",
        "Tailor the summary and skills section to emphasize quantitative analysis, risk framework, and API development for pricing libraries."
      ],
      "linkedin_keywords": [
        "quantitative analyst",
        "interest rate quant",
        "fixed income",
        "pricing models",
        "risk management",
        "Python",
        "C++",
        "Rust",
        "probability",
        "statistics",
        "financial engineering",
        "capital markets",
        "derivatives",
        "quant developer",
        "Bloomberg",
        "MATLAB",
        "QuantLib."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Maya_Hernandez_20251129_062719.pdf",
      "job_title": "Data Analytics Software Engineer (Trading)",
      "job_company": "Fintal Partners",
      "job_id": "4318639448",
      "skill_score": 0.2608695652173913,
      "semantic_score": 0.6992204189300736,
      "topic_score": 0.6992204189300736,
      "final_score": 0.5019625347593666,
      "resume_skills_count": 16,
      "job_skills_count": 13,
      "matching_skills_count": 6,
      "resume_text_length": 2216,
      "resume_skills": [
        "airflow",
        "data analysis",
        "data analytics",
        "data visualization",
        "forecasting",
        "inventory management",
        "leadership",
        "machine learning",
        "power bi",
        "python",
        "r",
        "reporting",
        "snowflake",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "data analysis",
        "data analytics",
        "databricks",
        "docker",
        "kubernetes",
        "machine learning",
        "numpy",
        "pandas",
        "python",
        "reporting",
        "scipy",
        "snowflake",
        "spark"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks required experience in high‑frequency trading, real‑time data pipelines, distributed analytics engines, and containerized environments specified in the posting.",
      "llm_recommendations": [
        "Highlight any projects or coursework involving Spark, Dask, Trino, or Hadoop to demonstrate distributed data processing skills.",
        "Add experience with real‑time streaming platforms (Kafka, Flink, or Kinesis) and low‑latency data pipelines to align with HFT needs.",
        "Include containerization (Docker/Kubernetes) and Linux system administration experience, or note any related certifications or projects.",
        "Showcase usage of advanced Python libraries (NumPy, SciPy, Pandas, Polars) and your proficiency in both OOP and functional programming styles.",
        "Mention any exposure to LLM tools or machine learning models applied to market or financial data, even at a basic level."
      ],
      "linkedin_keywords": [
        "Python",
        "SQL",
        "Snowflake",
        "Spark",
        "Dask",
        "Data Engineering",
        "Airflow",
        "Docker",
        "Kubernetes",
        "High Frequency Trading",
        "Quantitative Analysis",
        "Real-time Analytics",
        "Pandas",
        "Scikit-learn"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Maya_Patel_20251129_062934.pdf",
      "job_title": "Data Analytics Software Engineer (Trading)",
      "job_company": "Fintal Partners",
      "job_id": "4318639448",
      "skill_score": 0.11538461538461539,
      "semantic_score": 0.6522311749733941,
      "topic_score": 0.6522311749733941,
      "final_score": 0.4106502231584437,
      "resume_skills_count": 16,
      "job_skills_count": 13,
      "matching_skills_count": 3,
      "resume_text_length": 2892,
      "resume_skills": [
        "a/b testing",
        "aws",
        "azure",
        "data analytics",
        "data visualization",
        "decision making",
        "feature engineering",
        "forecasting",
        "leadership",
        "machine learning",
        "natural language processing",
        "python",
        "r",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "data analysis",
        "data analytics",
        "databricks",
        "docker",
        "kubernetes",
        "machine learning",
        "numpy",
        "pandas",
        "python",
        "reporting",
        "scipy",
        "snowflake",
        "spark"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks specific experience with high‑frequency trading data pipelines, distributed data engines (Spark, Dask), trading‑specific analytics libraries, and containerized Linux environments required for the role.",
      "llm_recommendations": [
        "Highlight any projects involving real‑time trading data ingestion and low‑latency analytics (e.g., Python, numpy, pandas optimizations).",
        "Add experience or coursework with distributed processing tools such as Apache Spark, Dask, or Trino, and data lake technologies like Snowflake, Databricks, or Iceberg.",
        "Showcase familiarity with containerization (Docker) and orchestration (Kubernetes) on Linux, and any exposure to HFT/quantitative finance projects.",
        "Include usage of JupyterLab or JupyterHub and any experience with LLM‑assisted code generation or analysis tools.",
        "Add a concise section on trading‑specific performance monitoring, anomaly detection, and backtesting frameworks you have used or developed."
      ],
      "linkedin_keywords": [
        "high‑frequency trading",
        "data engineering",
        "Spark",
        "Dask",
        "Snowflake",
        "Databricks",
        "Linux",
        "Docker",
        "Kubernetes",
        "JupyterLab",
        "real‑time analytics",
        "financial markets",
        "large‑scale data pipelines",
        "Python",
        "pandas",
        "numpy",
        "scikit‑learn",
        "trading analytics",
        "machine learning",
        "low‑latency."
      ],
      "llm_error": null
    },
    {
      "resume_file": "MeenalGupta (10).pdf",
      "job_title": "Senior iOS Developer with verification",
      "job_company": "Kajabi",
      "job_id": "4299254728",
      "skill_score": 0.0625,
      "semantic_score": 0.547380805015564,
      "topic_score": 0.547380805015564,
      "final_score": 0.3291844427585602,
      "resume_skills_count": 7,
      "job_skills_count": 10,
      "matching_skills_count": 1,
      "resume_text_length": 3772,
      "resume_skills": [
        "a/b testing",
        "c++",
        "git",
        "javascript",
        "matlab",
        "mongodb",
        "salesforce"
      ],
      "job_skills": [
        "agile",
        "ci/cd",
        "collaboration",
        "communication",
        "confluence",
        "git",
        "github",
        "jira",
        "leadership",
        "reporting"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate’s résumé lacks the required iOS, Swift, and Apple ecosystem experience, as well as the advanced architectural and AI-assisted coding skills specified for the Senior iOS Developer role.",
      "llm_recommendations": [
        "Build and showcase iOS projects using Swift, SwiftUI, and UIKit, demonstrating MVVM + clean architecture and dependency injection patterns.",
        "Obtain hands‑on experience with iOS concurrency (async/await, actors), networking (URLSession, GraphQL/Apollo), and media frameworks (AVFoundation, AVPlayer).",
        "Record code samples that include unit testing with XCTest, CI/CD pipelines (Fastlane, CircleCI), and App Store deployment workflows; mention integration with observability tools like Datadog.",
        "Highlight any use of AI-assisted coding tools (Cursor, Claude, Alex Codes) or agentic coding workflows, and explicitly state experience with multi‑tenant or white‑label architecture if applicable."
      ],
      "linkedin_keywords": [
        "iOS developer",
        "Swift",
        "SwiftUI",
        "UIKit",
        "MVVM",
        "clean architecture",
        "dependency injection",
        "iOS concurrency",
        "Apple ecosystem",
        "fastlane",
        "CI/CD",
        "XCTest."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Mia_Rodriguez_20251129_063642.pdf",
      "job_title": "Senior Data Engineer",
      "job_company": "Envision Employment Solutions",
      "job_id": "4332319156",
      "skill_score": 0.37037037037037035,
      "semantic_score": 0.7116574473176878,
      "topic_score": 0.7116574473176878,
      "final_score": 0.5580782626913949,
      "resume_skills_count": 17,
      "job_skills_count": 20,
      "matching_skills_count": 10,
      "resume_text_length": 2410,
      "resume_skills": [
        "agile",
        "airflow",
        "aws",
        "data analysis",
        "elt",
        "etl",
        "git",
        "machine learning",
        "performance analysis",
        "power bi",
        "python",
        "redshift",
        "reporting",
        "snowflake",
        "spark",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "airflow",
        "aws",
        "bigquery",
        "ci/cd",
        "collaboration",
        "communication",
        "databricks",
        "docker",
        "elt",
        "etl",
        "gcp",
        "java",
        "kubernetes",
        "python",
        "redshift",
        "reporting",
        "scala",
        "snowflake",
        "spark",
        "sql"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate’s 8+ years of data engineering experience, strong SQL, Python, Apache Spark, Airflow, dbt, Snowflake, Redshift, and proven ability to design ETL pipelines and optimize performance align closely with the senior data engineer role requirements.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Michael_Xu.pdf",
      "job_title": "Full Stack Engineer",
      "job_company": "IOSSERVICES",
      "job_id": "4318448366",
      "skill_score": 0.35294117647058826,
      "semantic_score": 0.6541839448103949,
      "topic_score": 0.6541839448103949,
      "final_score": 0.5186246990574819,
      "resume_skills_count": 8,
      "job_skills_count": 15,
      "matching_skills_count": 6,
      "resume_text_length": 2377,
      "resume_skills": [
        "aws",
        "git",
        "go",
        "java",
        "javascript",
        "postgresql",
        "python",
        "typescript"
      ],
      "job_skills": [
        "aws",
        "ci/cd",
        "communication",
        "docker",
        "git",
        "github",
        "go",
        "javascript",
        "jenkins",
        "jira",
        "kubernetes",
        "mongodb",
        "nosql",
        "postgresql",
        "python"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate’s extensive full‑stack experience with JavaScript/Node.js, React, Go, PostgreSQL, MongoDB, AWS, and Docker, along with proven leadership and strong ownership of delivery, aligns closely with the full‑stack engineer role requirements.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Mina_Rahman_20251129_140708.pdf",
      "job_title": "AI Software Engineer with verification",
      "job_company": "XpertDirect",
      "job_id": "4318464252",
      "skill_score": 0.2857142857142857,
      "semantic_score": 0.6958184242248585,
      "topic_score": 0.6958184242248585,
      "final_score": 0.5112715618951007,
      "resume_skills_count": 19,
      "job_skills_count": 8,
      "matching_skills_count": 6,
      "resume_text_length": 2014,
      "resume_skills": [
        "aws",
        "ci/cd",
        "collaboration",
        "data cleaning",
        "data ingestion",
        "data visualization",
        "deep learning",
        "docker",
        "feature engineering",
        "forecasting",
        "kubernetes",
        "machine learning",
        "python",
        "random forest",
        "reporting",
        "sql",
        "statistical analysis",
        "tableau",
        "xgboost"
      ],
      "job_skills": [
        "aws",
        "docker",
        "forecasting",
        "lambda",
        "machine learning",
        "pandas",
        "python",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks domain experience in energy utilities and specific stack components such as AWS Lambda, Kafka, and FastAPI required for the role.",
      "llm_recommendations": [
        "Highlight any projects or coursework involving SCADA systems, IoT sensor data, or time-series forecasting for utilities or infrastructure.",
        "Add experience or familiarity with Kafka, FastAPI, and AWS Lambda, or state intent to learn these quickly.",
        "Tailor the resume to emphasize energy or grid‑related use cases, such as power demand forecasting or asset degradation detection, even if from simulations or academic projects.",
        "Indicate willingness to relocate to Houston, TX, or availability for remote work.",
        "Include any relevant certifications or training in renewable energy analytics or cloud‑based MLOps tools."
      ],
      "linkedin_keywords": [
        "energy analytics",
        "grid performance",
        "SCADA",
        "IoT",
        "power forecasting",
        "AWS SageMaker",
        "time-series forecasting",
        "MLOps",
        "FastAPI",
        "Docker",
        "Python",
        "TensorFlow",
        "PyTorch",
        "AWS Lambda",
        "predictive modeling",
        "renewable energy",
        "utility AI",
        "energy market simulation"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Ming_Chen_20251129_064421.pdf",
      "job_title": "IL0202 – Data Scientist with verification",
      "job_company": "RR Donnelley",
      "job_id": "4331336561",
      "skill_score": 0.16666666666666666,
      "semantic_score": 0.6664709250015584,
      "topic_score": 0.6664709250015584,
      "final_score": 0.4415590087508571,
      "resume_skills_count": 18,
      "job_skills_count": 24,
      "matching_skills_count": 6,
      "resume_text_length": 1771,
      "resume_skills": [
        "customer segmentation",
        "data analysis",
        "data visualization",
        "decision making",
        "etl",
        "feature engineering",
        "forecasting",
        "hadoop",
        "machine learning",
        "operational efficiency",
        "python",
        "r",
        "reporting",
        "sales forecasting",
        "sql",
        "statistical analysis",
        "tableau",
        "xgboost"
      ],
      "job_skills": [
        "agile",
        "athena",
        "aws",
        "data analytics",
        "data visualization",
        "feature engineering",
        "git",
        "hadoop",
        "java",
        "javascript",
        "jenkins",
        "lambda",
        "machine learning",
        "matplotlib",
        "mysql",
        "postgresql",
        "pyspark",
        "python",
        "redshift",
        "s3",
        "seaborn",
        "shell scripting",
        "snowflake",
        "tableau"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate meets basic data science roles but lacks key qualifications such as a master’s degree, cloud (AWS) experience, Java/PySpark usage, and relevant certifications required for the position.",
      "llm_recommendations": [
        "Add or highlight experience with AWS services (S3, RDS, SageMaker, Lambda, Step Functions, Athena) or a comparable cloud platform.",
        "Include proficiency in Java, shell scripting, PySpark, or SAS SQL to match programming requirements.",
        "Demonstrate use of relational databases like PostgreSQL or MySQL through real projects or certifications.",
        "Showcase Git version control and Agile methodology experience on projects or in coursework.",
        "Obtain or display a master’s degree credential, or relevant certifications (e.g., AWS Certified Data Analytics, Big Data certifications)."
      ],
      "linkedin_keywords": [
        "data scientist",
        "machine learning engineer",
        "python",
        "SQL",
        "Hadoop",
        "Tableau",
        "AWS",
        "PySpark",
        "Git",
        "Agile",
        "data engineering",
        "feature engineering"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Minho_Kim_20251129_061917.pdf",
      "job_title": "Data Engineer AWS - Hybrid in NY",
      "job_company": "TechTriad",
      "job_id": "4317962527",
      "skill_score": 0.4,
      "semantic_score": 0.7441980700138695,
      "topic_score": 0.7441980700138695,
      "final_score": 0.5893089385076282,
      "resume_skills_count": 15,
      "job_skills_count": 13,
      "matching_skills_count": 8,
      "resume_text_length": 1708,
      "resume_skills": [
        "airflow",
        "aws",
        "ci/cd",
        "data analysis",
        "data analytics",
        "databricks",
        "docker",
        "etl",
        "gcp",
        "github",
        "kubernetes",
        "machine learning",
        "python",
        "spark",
        "sql"
      ],
      "job_skills": [
        "airflow",
        "aws",
        "ci/cd",
        "databricks",
        "java",
        "kubernetes",
        "lambda",
        "machine learning",
        "pyspark",
        "python",
        "s3",
        "scala",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks explicit experience with AWS data services, a feature store, and Unity Catalog, which are central to the listed Data Engineer role.",
      "llm_recommendations": [
        "Highlight any experience building or maintaining feature stores (e.g., AWS SageMaker Feature Store, Feast).",
        "Detail specific AWS services used (S3, Glue, EMR, Lambda, Kinesis) and provide project examples.",
        "Include experience or projects involving Delta Lake, PySpark, and Unity Catalog if applicable.",
        "Add mentions of Scala/Java usage or indicate familiarity with these languages.",
        "Obtain or list certifications such as AWS Certified Solutions Architect, AWS Certified Data Analytics, or Databricks Certified Data Engineer."
      ],
      "linkedin_keywords": [
        "data engineer",
        "AWS",
        "Databricks",
        "Feature Store",
        "PySpark",
        "Delta Lake",
        "Scala",
        "Java",
        "CI/CD",
        "Machine Learning Pipelines",
        "Unity Catalog",
        "Kinesis",
        "Glue",
        "EMR",
        "Data Quality."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Mira_Kim_20251129_140816.pdf",
      "job_title": "AI/ML Software Engineer with verification",
      "job_company": "Guidehouse",
      "job_id": "4332300185",
      "skill_score": 0.125,
      "semantic_score": 0.6276045850633234,
      "topic_score": 0.6276045850633234,
      "final_score": 0.4014325217848279,
      "resume_skills_count": 14,
      "job_skills_count": 13,
      "matching_skills_count": 3,
      "resume_text_length": 2978,
      "resume_skills": [
        "aws",
        "azure",
        "bert",
        "c++",
        "ci/cd",
        "deep learning",
        "docker",
        "gcp",
        "java",
        "kubernetes",
        "named entity recognition",
        "natural language processing",
        "nlp",
        "python"
      ],
      "job_skills": [
        "aws",
        "communication",
        "deep learning",
        "javascript",
        "machine learning",
        "numpy",
        "oracle",
        "outreach",
        "pandas",
        "postgresql",
        "python",
        "sql",
        "typescript"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks critical requirements such as JavaScript/TypeScript expertise, retrieval‑augmented generation or LangChain experience, relational database skills, and a bachelor’s degree (plus federal clearance).",
      "llm_recommendations": [
        "Highlight any JavaScript/TypeScript projects or learn to build a small web demo to demonstrate front‑end skills.",
        "Add specifics about RAG, LangChain or Haystack projects, including re‑ranking strategies and agentic workflows.",
        "Include experience with relational databases (PostgreSQL, Oracle) and SQL data modeling, plus any Elasticsearch/OpenSearch usage.",
        "Describe cloud deployment details with AWS SDKs (e.g., boto3) and infrastructure‑as‑code, and emphasize CI/CD, Docker, Kubernetes practices.",
        "If possible, obtain or disclose a valid federal public trust or other security clearance and mention it in the résumé."
      ],
      "linkedin_keywords": [
        "AI engineer",
        "NLP engineer",
        "machine learning engineer",
        "data engineer",
        "Python",
        "PyTorch",
        "TensorFlow",
        "AWS",
        "LangChain",
        "Elasticsearch"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Mira_Patel_20251129_061342.pdf",
      "job_title": "AI/ML Software Engineer with verification",
      "job_company": "Guidehouse",
      "job_id": "4332300185",
      "skill_score": 0.15789473684210525,
      "semantic_score": 0.6291219813154958,
      "topic_score": 0.6291219813154958,
      "final_score": 0.41706972130247,
      "resume_skills_count": 9,
      "job_skills_count": 13,
      "matching_skills_count": 3,
      "resume_text_length": 2216,
      "resume_skills": [
        "a/b testing",
        "aws",
        "ci/cd",
        "docker",
        "lambda",
        "leadership",
        "machine learning",
        "nlp",
        "python"
      ],
      "job_skills": [
        "aws",
        "communication",
        "deep learning",
        "javascript",
        "machine learning",
        "numpy",
        "oracle",
        "outreach",
        "pandas",
        "postgresql",
        "python",
        "sql",
        "typescript"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks several key technical requirements such as JavaScript/TypeScript proficiency, RAG system experience, and search/database expertise needed for this role.",
      "llm_recommendations": [
        "Add or highlight projects involving JavaScript/TypeScript and Node.js for web/API development.",
        "Include experience building retrieval‑augmented generation (RAG) pipelines or document retrieval components.",
        "Provide details on creating and maintaining REST APIs for model inference and integration.",
        "Demonstrate usage of search technologies like Elasticsearch/OpenSearch and relational databases (e.g., PostgreSQL).",
        "Show deployments using IaC tools (Terraform, CDK) and CI/CD pipelines on AWS."
      ],
      "linkedin_keywords": [
        "AI engineer",
        "NLP engineer",
        "chatbot developer",
        "retrieval‑augmented generation",
        "RAG",
        "data pipelines",
        "AWS",
        "Python",
        "PyTorch",
        "TensorFlow",
        "JavaScript"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Nadia_Mahmoud_20251129_140854.pdf",
      "job_title": "Sr. Analyst, Digital Initiatives",
      "job_company": "Toast",
      "job_id": "4331625169",
      "skill_score": 0.47058823529411764,
      "semantic_score": 0.7468849118810788,
      "topic_score": 0.7468849118810788,
      "final_score": 0.6225514074169463,
      "resume_skills_count": 13,
      "job_skills_count": 12,
      "matching_skills_count": 8,
      "resume_text_length": 1766,
      "resume_skills": [
        "aws",
        "bash",
        "data analysis",
        "data visualization",
        "etl",
        "machine learning",
        "power bi",
        "python",
        "r",
        "reporting",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "communication",
        "data analysis",
        "data analytics",
        "data visualization",
        "power bi",
        "presentation skills",
        "python",
        "r",
        "reporting",
        "sql",
        "stakeholder management",
        "tableau"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate’s 9‑year data analytics background, strong SQL, Power BI/Tableau skills, Python/R experience, and proven stakeholder communication align well with the role’s core requirements.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Nick_Pomes_Resume.pdf",
      "job_title": "Senior Manager, Web Analytics, Content and User Insights with verification",
      "job_company": "Pfizer",
      "job_id": "4332094854",
      "skill_score": 0.2857142857142857,
      "semantic_score": 0.5149680836470223,
      "topic_score": 0.5149680836470223,
      "final_score": 0.41180387457729084,
      "resume_skills_count": 9,
      "job_skills_count": 9,
      "matching_skills_count": 4,
      "resume_text_length": 3625,
      "resume_skills": [
        "client communication",
        "collaboration",
        "communication",
        "digital marketing",
        "leadership",
        "reporting",
        "seo",
        "social media analytics",
        "strategic planning"
      ],
      "job_skills": [
        "a/b testing",
        "collaboration",
        "communication",
        "decision making",
        "google analytics",
        "leadership",
        "market research",
        "project management",
        "reporting"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks the required web analytics expertise, industry experience, and specific tool proficiencies outlined in the Pfizer job posting.",
      "llm_recommendations": [
        "Explicitly add experience with Adobe Analytics, Google Analytics, Adobe Tag Manager, and other web analytics platforms.",
        "Highlight any web‑site or digital content performance analysis, multi‑touch attribution, or user journey studies.",
        "Include any healthcare or pharmaceutical industry work, or at least metrics that demonstrate understanding of claims data or pharma marketing.",
        "Quantify web‑analytics results with metrics (traffic, conversion, attribution lift) and note any A/B testing or dynamic creative optimization.",
        "Obtain and list relevant certifications (e.g., Adobe Analytics Expert, Google Analytics IQ) and describe cross‑functional leadership and stakeholder collaboration."
      ],
      "linkedin_keywords": [
        "web analytics",
        "Adobe Analytics",
        "Google Analytics",
        "data-driven decision making",
        "cross-functional leadership",
        "content intelligence",
        "user journey analysis",
        "multi-touch attribution",
        "pharmaceutical marketing",
        "digital strategy."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Nikhil_Shah_20251129_065017.pdf",
      "job_title": "Sr./Staff Data Scientist",
      "job_company": "RemoteHunter",
      "job_id": "4319347697",
      "skill_score": 0.21052631578947367,
      "semantic_score": 0.5934733859864934,
      "topic_score": 0.5934733859864934,
      "final_score": 0.4211472043978345,
      "resume_skills_count": 12,
      "job_skills_count": 11,
      "matching_skills_count": 4,
      "resume_text_length": 1928,
      "resume_skills": [
        "a/b testing",
        "customer segmentation",
        "data visualization",
        "feature engineering",
        "gradient boosting",
        "hadoop",
        "machine learning",
        "python",
        "snowflake",
        "spark",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "aws",
        "azure",
        "communication",
        "data analysis",
        "data cleaning",
        "data transformation",
        "feature engineering",
        "gcp",
        "machine learning",
        "python",
        "spark"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate has 5 years experience as a data scientist, built a real‑time fraud detection system with Spark MLlib, strong Python skills, feature engineering pipelines, and relevant financial domain exposure that align well with the job’s core requirements.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Nora_Schaefer_20251129_061101.pdf",
      "job_title": "Senior DataBricks Developer (Healthcare) with verification",
      "job_company": "Lumenalta",
      "job_id": "4332077514",
      "skill_score": 0.2857142857142857,
      "semantic_score": 0.7244572207655192,
      "topic_score": 0.7244572207655192,
      "final_score": 0.5270228999924641,
      "resume_skills_count": 18,
      "job_skills_count": 9,
      "matching_skills_count": 6,
      "resume_text_length": 2493,
      "resume_skills": [
        "agile",
        "airflow",
        "aws",
        "ci/cd",
        "data analysis",
        "data pipeline",
        "decision making",
        "docker",
        "etl",
        "git",
        "java",
        "power bi",
        "pyspark",
        "python",
        "snowflake",
        "spark",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "aws",
        "ci/cd",
        "collaboration",
        "databricks",
        "elt",
        "etl",
        "pyspark",
        "python",
        "sql"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate meets general data engineering experience and skills but lacks specific Databricks, healthcare (claims/HIPAA) and AWS Glue/Lake Formation expertise required for the role.",
      "llm_recommendations": [
        "Highlight any project or role where you used Databricks on AWS, including notebook development, job scheduling, and cluster tuning.",
        "Add experience working with healthcare datasets (claims, EHR, payer‑provider) or mention knowledge of HIPAA regulations and data security practices.",
        "Include AWS data services such as Glue Data Catalog, Lake Formation, or Redshift Spectrum to show familiarity with data governance tools.",
        "Provide examples of CI/CD pipelines for data workflows, emphasizing tools like Airflow, GitHub Actions, or CircleCI that support data quality and lineage.",
        "Incorporate quantitative impacts (e.g., “reduced ETL runtime by 40%”, “handled 5TB daily data”) to demonstrate scalability and performance."
      ],
      "linkedin_keywords": [
        "Databricks",
        "AWS Glue",
        "Amazon Lake Formation",
        "HIPAA",
        "Healthcare data",
        "ETL pipelines",
        "Pyspark",
        "Python",
        "SQL",
        "Data engineering",
        "Data governance",
        "Compliance",
        "Large‑scale data processing",
        "Snowflake",
        "Apache Airflow"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Raymond Cao.pdf",
      "job_title": "Sr. Product Engineer (front-end) - Itinerary team with verification",
      "job_company": "WeTravel",
      "job_id": "4332466649",
      "skill_score": 0.0,
      "semantic_score": 0.34023162722587585,
      "topic_score": 0.34023162722587585,
      "final_score": 0.18712739497423173,
      "resume_skills_count": 4,
      "job_skills_count": 9,
      "matching_skills_count": 0,
      "resume_text_length": 3522,
      "resume_skills": [
        "data analytics",
        "forecasting",
        "leadership",
        "outreach"
      ],
      "job_skills": [
        "collaboration",
        "communication",
        "github",
        "kubernetes",
        "mongodb",
        "mysql",
        "python",
        "snowflake",
        "typescript"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks the technical front‑end, full‑stack, and product engineering experience required for the senior product engineer role.",
      "llm_recommendations": [
        "Highlight any software development or web development projects, especially involving JavaScript, TypeScript, or React.",
        "Include experience with back‑end technologies such as Ruby on Rails, Node.js, or Python microservices.",
        "Demonstrate familiarity with cloud environments (Kubernetes, Docker) and distributed or event‑driven architectures.",
        "Showcase any leadership or mentorship roles in engineering teams or product ownership.",
        "Add relevant coursework, certifications, or side projects that cover front‑end frameworks, monitoring, and AI tooling (e.g., Copilot, Claude, or Cursor)."
      ],
      "linkedin_keywords": [
        "frontend engineer",
        "react developer",
        "typescript",
        "ruby on rails",
        "full stack engineer",
        "microservices",
        "distributed systems",
        "AI assistant",
        "product ownership",
        "Kubernetes",
        "cloud computing",
        "devops."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Resume Example 1.pdf",
      "job_title": "Procurement Executive",
      "job_company": "Rockhill Asia",
      "job_id": "4318667112",
      "skill_score": 0.25,
      "semantic_score": 0.7769247160509073,
      "topic_score": 0.7769247160509073,
      "final_score": 0.5398085938279991,
      "resume_skills_count": 11,
      "job_skills_count": 9,
      "matching_skills_count": 4,
      "resume_text_length": 4331,
      "resume_skills": [
        "communication",
        "forecasting",
        "leadership",
        "logistics",
        "operational efficiency",
        "oracle",
        "process improvement",
        "procurement",
        "project management",
        "reporting",
        "sales operations"
      ],
      "job_skills": [
        "collaboration",
        "communication",
        "cost analysis",
        "cross-functional collaboration",
        "logistics",
        "negotiation",
        "procurement",
        "reporting",
        "stakeholder management"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate’s procurement manager role at Apple, coupled with extensive supplier management, cost negotiation, PO handling, and Excel‑driven analysis, closely aligns with the Procurement Executive requirements.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Resume Example 2.pdf",
      "job_title": "Sr. Product Engineer (front-end) - Itinerary team with verification",
      "job_company": "WeTravel",
      "job_id": "4332466649",
      "skill_score": 0.0,
      "semantic_score": 0.34023162722587585,
      "topic_score": 0.34023162722587585,
      "final_score": 0.18712739497423173,
      "resume_skills_count": 4,
      "job_skills_count": 9,
      "matching_skills_count": 0,
      "resume_text_length": 3522,
      "resume_skills": [
        "data analytics",
        "forecasting",
        "leadership",
        "outreach"
      ],
      "job_skills": [
        "collaboration",
        "communication",
        "github",
        "kubernetes",
        "mongodb",
        "mysql",
        "python",
        "snowflake",
        "typescript"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks any software engineering, front‑end development, or full‑stack experience required for the Sr. Product Engineer role.",
      "llm_recommendations": [
        "Add any programming experience, projects, or coursework in JavaScript/TypeScript and React that showcase front‑end skills.",
        "Include backend experience (e.g., Ruby on Rails, Node.js) or microservice exposure and emphasize full‑stack or system design capabilities.",
        "Highlight product ownership, user‑centric design, and mentorship roles within a technical context, even if these were part of a business venture.",
        "Include any cloud, database, or CI/CD tools knowledge (AWS, Kubernetes, GitHub Actions) to demonstrate production‑grade engineering competence.",
        "Obtain and list relevant certifications or online courses (e.g., React Certified, AWS Certified Developer) to bridge the skill gap."
      ],
      "linkedin_keywords": [
        "front-end developer",
        "react developer",
        "typescript",
        "ruby on rails",
        "full-stack engineer",
        "microservices",
        "product owner",
        "cloud computing",
        "Kubernetes",
        "AI assistants",
        "software engineering",
        "product engineering",
        "on-call",
        "monitoring",
        "UX design",
        "component library",
        "design system",
        "distributed systems",
        "event‑driven architecture",
        "mentoring engineer"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Resume Jeffrey Nord Systems Administrator.pdf",
      "job_title": "Microsoft Entra ID",
      "job_company": "Cubical Operations LLP",
      "job_id": "4332402375",
      "skill_score": 0.0,
      "semantic_score": 0.3410060608986658,
      "topic_score": 0.3410060608986658,
      "final_score": 0.1875533334942662,
      "resume_skills_count": 2,
      "job_skills_count": 1,
      "matching_skills_count": 0,
      "resume_text_length": 5934,
      "resume_skills": [
        "project management",
        "user research"
      ],
      "job_skills": [
        "azure"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume lacks specific experience and certifications in Azure AD/Entra ID, Intune, Purview, and related Microsoft security and identity management technologies required by the role.",
      "llm_recommendations": [
        "Gain hands‑on experience with Azure AD/Entra ID, focusing on conditional access, MFA, RBAC, Privileged Identity Management, and hybrid identity scenarios.",
        "Complete Microsoft certifications such as SC‑300 (Identity & Access Administrator) or SC‑400 (Information Protection Administrator) and consider AZ‑500 (Azure Security Engineer Associate).",
        "Add projects or brief case studies that illustrate deployment and management of Microsoft Intune (MDM/MAM) and device compliance policies.",
        "Highlight any involvement with Microsoft Purview or data classification/governance initiatives in the resume.",
        "Update the education and certifications section to prominently feature relevant Microsoft technical credentials and coursework."
      ],
      "linkedin_keywords": [
        "Azure AD",
        "Microsoft Intune",
        "Microsoft Entra ID",
        "Conditional Access",
        "MFA",
        "Hybrid identity",
        "Office 365 administration",
        "Active Directory",
        "Privileged Identity Management",
        "Microsoft Purview"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Rina_Tanaka_20251129_064106.pdf",
      "job_title": "Data and Production Support Analyst with verification",
      "job_company": "VanEck",
      "job_id": "4310751775",
      "skill_score": 0.18181818181818182,
      "semantic_score": 0.6792125701904297,
      "topic_score": 0.6792125701904297,
      "final_score": 0.4553850954229181,
      "resume_skills_count": 14,
      "job_skills_count": 12,
      "matching_skills_count": 4,
      "resume_text_length": 2057,
      "resume_skills": [
        "customer retention",
        "data analysis",
        "data cleaning",
        "data visualization",
        "hadoop",
        "machine learning",
        "oracle",
        "power bi",
        "python",
        "r",
        "reporting",
        "spark",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "business analysis",
        "communication",
        "critical thinking",
        "data analysis",
        "data visualization",
        "multitasking",
        "operational efficiency",
        "problem solving",
        "reporting",
        "sql",
        "teamwork",
        "time management"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate’s strong SQL and Power BI background is offset by a lack of direct financial data operations experience and specific expertise in managing investment data pipelines required for the role.",
      "llm_recommendations": [
        "Explicitly mention any experience working with financial datasets (e.g., securities master, pricing data, corporate actions, holdings).",
        "Detail concrete data ingestion, ETL, and data validation activities, including tools, processes, and outcomes.",
        "Highlight collaboration with IT or data engineering teams to troubleshoot data issues via ticketing systems and root‑cause analysis.",
        "Include certifications or training related to financial data management, data quality, or data operations best practices.",
        "Incorporate industry‑specific keywords (e.g., “investment data,” “market data quality,” “data pipeline,” “ETL,” “SQL-based data validation”) throughout the resume."
      ],
      "linkedin_keywords": [
        "Data Operations Analyst",
        "Financial Data",
        "Data Quality",
        "ETL",
        "SQL",
        "Power BI",
        "Investment Data",
        "Securities Data",
        "Data Validation",
        "Data Pipeline."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Robbie-Shawn-Resume-2025.pdf",
      "job_title": "Senior Digital Marketing Manager with verification",
      "job_company": "Argano",
      "job_id": "4318098498",
      "skill_score": 0.05555555555555555,
      "semantic_score": 0.6558196153748276,
      "topic_score": 0.6558196153748276,
      "final_score": 0.3857007884561552,
      "resume_skills_count": 6,
      "job_skills_count": 13,
      "matching_skills_count": 1,
      "resume_text_length": 10819,
      "resume_skills": [
        "aws",
        "communication",
        "logistics",
        "mailchimp",
        "r",
        "seo"
      ],
      "job_skills": [
        "collaboration",
        "communication",
        "content marketing",
        "digital marketing",
        "email marketing",
        "google analytics",
        "hubspot",
        "lead generation",
        "marketing automation",
        "oracle",
        "project management",
        "salesforce",
        "sem"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume focuses mainly on e‑commerce operations and retail channels, lacking the B2B technology marketing, Oracle ecosystem, and HubSpot/Salesforce automation experience required for the senior digital marketing manager role.",
      "llm_recommendations": [
        "Highlight any B2B demand‑generation campaigns and results (pipeline growth, lead metrics).",
        "Add experience with marketing automation platforms such as HubSpot, Salesforce Marketing Cloud, or Pardot.",
        "Include specific achievements tied to Oracle products or other enterprise software integrations.",
        "Showcase content marketing, ABM initiatives, and analytics use with tools like Google Analytics, HubSpot Reports, or Salesforce dashboards.",
        "Mention familiarity with AI content tools (Jasper, Clearscope, Gamma) and use of data‑driven optimization in campaigns."
      ],
      "linkedin_keywords": [
        "digital marketing",
        "B2B marketing",
        "demand generation",
        "HubSpot",
        "Salesforce",
        "Oracle",
        "ABM",
        "content marketing",
        "marketing automation",
        "analytics",
        "AI content tools",
        "programmatic advertising",
        "SEO",
        "PPC",
        "marketing strategy",
        "enterprise software",
        "channel marketing",
        "marketing metrics."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Robert_Reeves.pdf",
      "job_title": "Senior Security Specialist with verification",
      "job_company": "Tata Consultancy Services",
      "job_id": "4319158866",
      "skill_score": 0.3,
      "semantic_score": 0.6758466967207223,
      "topic_score": 0.6758466967207223,
      "final_score": 0.5067156831963973,
      "resume_skills_count": 8,
      "job_skills_count": 5,
      "matching_skills_count": 3,
      "resume_text_length": 4320,
      "resume_skills": [
        "aws",
        "bash",
        "communication",
        "gcp",
        "git",
        "github",
        "python",
        "s3"
      ],
      "job_skills": [
        "aws",
        "azure",
        "communication",
        "gcp",
        "leadership"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume shows strong systems administration and some SIEM experience, but lacks the deep Azure security, penetration testing, compliance, and senior‑level cybersecurity engineering experience required for the role.",
      "llm_recommendations": [
        "Include concrete Azure security experience (Sentinel, Defender, Azure Security Center) and detail any related projects or scripts built.",
        "Highlight penetration testing or vulnerability assessment work, naming tools (e.g., Nessus, Metasploit, Burp Suite) and any findings or remediations.",
        "Add compliance experience with ISO 27001, NIST, and GDPR, outlining audits, policy development, or control implementations.",
        "Quantify your senior‑level impact (e.g., “Reduced incident response time by 30 % through automation”) and clarify that you have 5+ years in cybersecurity‑focused roles.",
        "Obtain and list relevant certifications such as CISSP, CISM, or Microsoft Certified: Azure Security Engineer Associate."
      ],
      "linkedin_keywords": [
        "Azure Sentinel",
        "Azure Defender",
        "cloud security",
        "penetration testing",
        "ISO 27001",
        "NIST",
        "GDPR",
        "incident response",
        "security architecture",
        "CISSP."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Roselin_Burgos.pdf",
      "job_title": "Quality Analyst with verification",
      "job_company": "Esferasoft Solutions Pvt Ltd.",
      "job_id": "4318439314",
      "skill_score": 0.13333333333333333,
      "semantic_score": 0.6643317735689614,
      "topic_score": 0.6643317735689614,
      "final_score": 0.42538247546292873,
      "resume_skills_count": 13,
      "job_skills_count": 4,
      "matching_skills_count": 2,
      "resume_text_length": 4014,
      "resume_skills": [
        "git",
        "github",
        "java",
        "javascript",
        "jenkins",
        "jira",
        "mongodb",
        "mysql",
        "nosql",
        "python",
        "scrum",
        "sql",
        "sqlite"
      ],
      "job_skills": [
        "agile",
        "jira",
        "reporting",
        "scrum"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate demonstrates strong manual testing, test planning, bug tracking via JIRA, API and database validation, and Scrum/Agile experience, aligning well with the core responsibilities of the Quality Analyst role.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "Sina_Li_20251129_062203.pdf",
      "job_title": "AI/Machine Learning (NLP) Engineer with verification",
      "job_company": "Aslase",
      "job_id": "4332070318",
      "skill_score": 0.25925925925925924,
      "semantic_score": 0.7301564273378068,
      "topic_score": 0.7301564273378068,
      "final_score": 0.5182527017024604,
      "resume_skills_count": 25,
      "job_skills_count": 9,
      "matching_skills_count": 7,
      "resume_text_length": 2798,
      "resume_skills": [
        "a/b testing",
        "aws",
        "bert",
        "ci/cd",
        "customer segmentation",
        "data analysis",
        "data cleaning",
        "decision trees",
        "deep learning",
        "docker",
        "feature engineering",
        "gcp",
        "jenkins",
        "kubernetes",
        "logistic regression",
        "machine learning",
        "natural language processing",
        "nlp",
        "python",
        "r",
        "reporting",
        "sentiment analysis",
        "spark",
        "sql",
        "statistical analysis"
      ],
      "job_skills": [
        "ci/cd",
        "docker",
        "embeddings",
        "github",
        "kubernetes",
        "machine learning",
        "nlp",
        "python",
        "spark"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume demonstrates strong NLP and ML expertise but lacks specific experience with LLM APIs, LangChain/LlamaIndex/Crewai, vector databases, no-code prototyping tools, and prompt engineering which are key requirements of the posting.",
      "llm_recommendations": [
        "Highlight any hands‑on work with OpenAI, Cohere, or Google Gemini APIs, and mention fine‑tuning and deployment of LLMs.",
        "Add experience using orchestration frameworks such as LangChain, LlamaIndex, or CrewAI, and any RAG or prompt‑engineering projects.",
        "Include use of vector databases (Pinecone, Weaviate, FAISS) and a brief example of embedding pipelines.",
        "Mention familiarity with no‑code tools like n8n or Make and describe a rapid AI prototype you built.",
        "Briefly state your understanding of AI ethics/bias mitigation and any open‑source contributions or streaming data pipeline work (e.g., Kafka, Spark)."
      ],
      "linkedin_keywords": [
        "OpenAI",
        "LangChain",
        "Pinecone",
        "LLM engineer",
        "prompt engineering",
        "no-code AI",
        "n8n",
        "AI ethics",
        "MLOps",
        "Docker",
        "Kubernetes",
        "CI/CD ML",
        "vector database",
        "GPT",
        "RAG",
        "Hugging Face",
        "AWS",
        "GCP",
        "data pipelines"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Sofia_Nikolić_20251129_062513.pdf",
      "job_title": "Sr Business Intelligence Engineer with verification",
      "job_company": "Humana",
      "job_id": "4318957629",
      "skill_score": 0.23076923076923078,
      "semantic_score": 0.6848892945183213,
      "topic_score": 0.6848892945183213,
      "final_score": 0.4805352658312305,
      "resume_skills_count": 17,
      "job_skills_count": 15,
      "matching_skills_count": 6,
      "resume_text_length": 2180,
      "resume_skills": [
        "a/b testing",
        "customer segmentation",
        "data visualization",
        "decision making",
        "digital marketing",
        "etl",
        "feature prioritization",
        "forecasting",
        "leadership",
        "operational efficiency",
        "power bi",
        "python",
        "r",
        "reporting",
        "sales forecasting",
        "sql",
        "tableau"
      ],
      "job_skills": [
        "agile",
        "aws",
        "communication",
        "data analysis",
        "data analytics",
        "forecasting",
        "leadership",
        "power bi",
        "process improvement",
        "qlik",
        "reporting",
        "root cause analysis",
        "sql",
        "tableau",
        "trend analysis"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks required healthcare analytics experience, advanced T‑SQL, Power Apps/Automate, and cloud platform skills needed for the Sr Business Intelligence Engineer role.",
      "llm_recommendations": [
        "Highlight or acquire experience with healthcare data such as claims, billing, and reconciliation, and explicitly mention it in the resume.",
        "Emphasize advanced T‑SQL capabilities and provide examples of complex query development, possibly adding a certifications section (e.g., Microsoft Certified: SQL Server).",
        "Include experience or training with cloud services (AWS, Azure, Databricks) and tools like Power Apps / Power Automate.",
        "Showcase proficiency with Microsoft Office suite (Word, PowerPoint, Access) and demonstrate communication of technical insights to senior leadership.",
        "Add any process improvement, agile methodology, or root‑cause analysis experience to align with preferred qualifications."
      ],
      "linkedin_keywords": [
        "Sr Business Intelligence",
        "Power BI",
        "Power Apps",
        "Power Automate",
        "T‑SQL",
        "SQL Server",
        "Healthcare Analytics",
        "Claims Billing",
        "SSIS",
        "Databricks",
        "Cloud Analytics",
        "Process Improvement",
        "Agile",
        "Microsoft Excel",
        "Forecasting"
      ],
      "llm_error": null
    },
    {
      "resume_file": "Steven_J_Vik_Incident_Response_Resume.pdf",
      "job_title": "Cyber Defense Engineer",
      "job_company": "Confidential",
      "job_id": "4319304178",
      "skill_score": 0.1,
      "semantic_score": 0.6781907283720594,
      "topic_score": 0.6781907283720594,
      "final_score": 0.4180049006046327,
      "resume_skills_count": 10,
      "job_skills_count": 1,
      "matching_skills_count": 1,
      "resume_text_length": 5237,
      "resume_skills": [
        "agile",
        "bash",
        "collaboration",
        "communication",
        "go",
        "problem solving",
        "process optimization",
        "python",
        "r",
        "risk analysis"
      ],
      "job_skills": [
        "python"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate’s experience focuses on Splunk and LogRhythm SIEMs and lacks demonstrated expertise with Microsoft security tooling, Azure Sentinel, KQL, and API-based security integrations required for this role.",
      "llm_recommendations": [
        "Highlight or acquire projects involving Azure Sentinel, Microsoft Defender, and KQL query development to demonstrate proficiency with the Microsoft security stack.",
        "Add experience with API development or integration of security tools (e.g., SOAR connectors or custom webhook integrations) to show ability to extend SIEM functionality.",
        "Include any work in multi‑tenant or MSP-style environments or explicitly state experience supporting multiple clients or virtualized security deployments.",
        "Update certifications to reflect Microsoft security offerings such as Azure Security Engineer Associate or Microsoft Sentinel certification.",
        "Emphasize automation and scripting projects that utilize PowerShell, Python, and KQL, underscoring your role in building detection rules, playbooks, and response workflow orchestration."
      ],
      "linkedin_keywords": [
        "Azure Sentinel",
        "Microsoft Defender",
        "KQL",
        "PowerShell",
        "SIEM",
        "SOC",
        "incident response",
        "security automation",
        "API integration",
        "multi‑tenant",
        "MSP",
        "threat detection",
        "Microsoft security stack",
        "Azure security."
      ],
      "llm_error": null
    },
    {
      "resume_file": "Yin_Zhang_20251129_061731.pdf",
      "job_title": "Data Analyst",
      "job_company": "Helic & Co.",
      "job_id": "4332495253",
      "skill_score": 0.3157894736842105,
      "semantic_score": 0.6836525767470771,
      "topic_score": 0.6836525767470771,
      "final_score": 0.5181141803687872,
      "resume_skills_count": 15,
      "job_skills_count": 10,
      "matching_skills_count": 6,
      "resume_text_length": 2356,
      "resume_skills": [
        "collaboration",
        "data analytics",
        "data cleaning",
        "data visualization",
        "forecasting",
        "hadoop",
        "machine learning",
        "matlab",
        "power bi",
        "python",
        "r",
        "spark",
        "sql",
        "statistical analysis",
        "tableau"
      ],
      "job_skills": [
        "communication",
        "critical thinking",
        "data analysis",
        "data analytics",
        "data visualization",
        "looker",
        "power bi",
        "python",
        "sql",
        "tableau"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate’s extensive experience, advanced analytics skills, and proficiency with SQL, Python, R, Tableau, and Power BI align closely with the Data Analyst role’s requirements and responsibilities.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "campfield-resume.pdf",
      "job_title": "DevSecOps",
      "job_company": "QualiZeal",
      "job_id": "4332406515",
      "skill_score": 0.5294117647058824,
      "semantic_score": 0.6433027770103691,
      "topic_score": 0.6433027770103691,
      "final_score": 0.5920518214733501,
      "resume_skills_count": 14,
      "job_skills_count": 12,
      "matching_skills_count": 9,
      "resume_text_length": 8389,
      "resume_skills": [
        "aws",
        "azure",
        "bash",
        "ci/cd",
        "communication",
        "docker",
        "experiment design",
        "gitlab",
        "jenkins",
        "kubernetes",
        "leadership",
        "postgresql",
        "project management",
        "python"
      ],
      "job_skills": [
        "aws",
        "azure",
        "bash",
        "bitbucket",
        "ci/cd",
        "docker",
        "git",
        "github",
        "gitlab",
        "jenkins",
        "kubernetes",
        "python"
      ],
      "error": null,
      "llm_match": true,
      "llm_reasoning": "The candidate’s extensive DevOps background with Jenkins, GitLab, Docker, Kubernetes, Helm, Terraform, AWS/Azure, and Bash/Python scripting aligns closely with the DevSecOps role requirements.",
      "llm_recommendations": null,
      "linkedin_keywords": null,
      "llm_error": null
    },
    {
      "resume_file": "head_of_ai_emmanuel.pdf",
      "job_title": "Senior Machine Learning Engineer with verification",
      "job_company": "Adobe",
      "job_id": "4319167654",
      "skill_score": 0.2,
      "semantic_score": 0.645181855846534,
      "topic_score": 0.645181855846534,
      "final_score": 0.44485002071559365,
      "resume_skills_count": 8,
      "job_skills_count": 4,
      "matching_skills_count": 2,
      "resume_text_length": 5755,
      "resume_skills": [
        "azure",
        "c#",
        "docker",
        "gcp",
        "github",
        "nlp",
        "python",
        "sql"
      ],
      "job_skills": [
        "java",
        "machine learning",
        "nlp",
        "python"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The candidate lacks key experience in recommendation/search systems, bandits, NLP, LLM fine-tuning, full‑stack deployment, Java, and has less than the required 5 years of deep learning engineering experience.",
      "llm_recommendations": [
        "Highlight any project that demonstrates recommendation or search capabilities, including use of bandits, ranking, or retrieval.",
        "Include concrete examples of NLP or content intelligence models, especially for recommendation or search contexts.",
        "Add details on fine‑tuning large language models (LLMs) for domain‑specific tasks and discuss evaluation or ethical AI practices.",
        "Gain or explicitly state proficiency in Java and experience deploying end‑to‑end ML features in production (e.g., via REST APIs or microservices).",
        "Consider adding a certificate or course in advanced recommender systems, search engine architecture, or ethical AI to bridge the qualification gap."
      ],
      "linkedin_keywords": [
        "recommendation systems",
        "search engine",
        "bandit algorithms",
        "natural language processing",
        "LLM fine-tuning",
        "content intelligence",
        "ethical AI",
        "machine learning engineering",
        "full‑stack AI deployment",
        "Python",
        "Java",
        "cloud AI",
        "generative AI."
      ],
      "llm_error": null
    },
    {
      "resume_file": "musa_iftikhar_resume.pdf",
      "job_title": "Senior Design Engineer with verification",
      "job_company": "Moog Inc.",
      "job_id": "4332061419",
      "skill_score": 0.0,
      "semantic_score": 0.505858838558197,
      "topic_score": 0.505858838558197,
      "final_score": 0.2782223612070084,
      "resume_skills_count": 1,
      "job_skills_count": 4,
      "matching_skills_count": 0,
      "resume_text_length": 8883,
      "resume_skills": [
        "r"
      ],
      "job_skills": [
        "communication",
        "leadership",
        "matlab",
        "reporting"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The résumé lacks the required mechanical engineering background, flight‑control experience, and specialized design analysis skills specified in the job posting.",
      "llm_recommendations": [
        "Add or emphasize any mechanical engineering education (BS/MS in Mechanical Engineering) and related coursework.",
        "Highlight mechanical design projects, including CAD (NX, SolidWorks) and finite element analysis (ANSYS, Abaqus) experiences.",
        "Detail experience in stress analysis, kinematics, vibration, thermal, and hydraulic/electro‑mechanical systems relevant to space or defense applications.",
        "Include any leadership or mentorship roles that demonstrate the ability to guide junior engineers.",
        "Provide certifications or training in mechanical design, GDT, or aerospace engineering, and mention any ability to obtain a U.S. DoD security clearance."
      ],
      "linkedin_keywords": [
        "mechanical engineer",
        "aerospace design",
        "flight control systems",
        "stress analysis",
        "ANSYS",
        "NX 3D CAD",
        "vibration analysis",
        "thermal modeling",
        "GDT",
        "DoD security clearance"
      ],
      "llm_error": null
    },
    {
      "resume_file": "sofia.pdf",
      "job_title": "Software Engineer III, Full Stack, Google Cloud Platforms with verification",
      "job_company": "Google",
      "job_id": "4317952345",
      "skill_score": 0.35714285714285715,
      "semantic_score": 0.6725157108921672,
      "topic_score": 0.6725157108921672,
      "final_score": 0.5305979267049776,
      "resume_skills_count": 11,
      "job_skills_count": 8,
      "matching_skills_count": 5,
      "resume_text_length": 4416,
      "resume_skills": [
        "agile",
        "c++",
        "collaboration",
        "git",
        "java",
        "javascript",
        "leadership",
        "mongodb",
        "mysql",
        "python",
        "sql"
      ],
      "job_skills": [
        "c++",
        "data analysis",
        "java",
        "javascript",
        "leadership",
        "natural language processing",
        "python",
        "typescript"
      ],
      "error": null,
      "llm_match": false,
      "llm_reasoning": "The resume shows early internship experience and basic full‑stack skills but falls short of the 2+ years of professional programming, large‑scale system, and advanced testing experience required for a Software Engineer III role at Google.",
      "llm_recommendations": [
        "Quantify and highlight any large‑scale or production‑level projects (e.g., number of users, traffic handled) to demonstrate experience with performance and scalability.",
        "Add formal testing experience (unit/integration, test‑driven development) and any CI/CD pipelines used.",
        "Emphasize any leadership or mentorship roles, even within projects or study groups, to show leadership qualities expected at level III.",
        "Include any certifications or coursework in Google Cloud Platform, Kubernetes, or distributed systems to better match the cloud focus.",
        "If possible, gain or describe any experience with accessibility standards (WCAG) or building accessible interfaces."
      ],
      "linkedin_keywords": [
        "Google Cloud Platform",
        "Full Stack Engineer",
        "Python",
        "Node.js",
        "JavaScript",
        "TypeScript",
        "Java",
        "C++",
        "Cloud Development",
        "Software Testing",
        "CI/CD",
        "Performance Engineering",
        "Distributed Systems",
        "Accessibility",
        "Machine Learning",
        "AI",
        "Big Data",
        "Docker",
        "Kubernetes"
      ],
      "llm_error": null
    }
  ]
}